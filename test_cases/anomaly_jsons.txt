{
    "start_time": "1697296768",
    "end_time": "1697296839",
    "start_timestamp": "2023-10-14 23:19:28",
    "end_timestamp": "2023-10-14 23:20:39",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 75\n    \n    # Size of each column (in characters)\n    column_size = 46\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, where 53 users simultaneously insert a large amount of data into a database table containing 18 columns and 75 rows, each column being 46 characters long, a database exception may occur due to the high workload.\n"
}
{
    "start_time": "1697296899",
    "end_time": "1697296970",
    "start_timestamp": "2023-10-14 23:21:39",
    "end_timestamp": "2023-10-14 23:22:50",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 25\n    \n    # Number of rows to insert\n    num_rows = 69\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a scientific research project, data generated by 53 sensors need to be inserted into a database table simultaneously. The database table contains 25 columns and 69 rows, with each column having a size of 70 characters. This action simulates the database exception caused by the simultaneous insertion of large amounts of data.\n"
}
{
    "start_time": "1697297030",
    "end_time": "1697297090",
    "start_timestamp": "2023-10-14 23:23:50",
    "end_timestamp": "2023-10-14 23:24:50",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 191\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 78\n    \n    # Number of rows to insert\n    num_rows = 305\n    \n    # Size of each column (in characters)\n    column_size = 90\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of a large online store, 191 users simultaneously compete to perform frequent update operations on a database table containing 78 columns and 305 rows of product records, where each column has a size of 90 characters. This process causes contention for locking the database table and can result in a database exception.\n"
}
{
    "start_time": "1697297150",
    "end_time": "1697297328",
    "start_timestamp": "2023-10-14 23:25:50",
    "end_timestamp": "2023-10-14 23:28:48",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 176\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 3167807\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online marketplace, 176 users perform simultaneous searches on a database table containing 18 columns and 3,167,807 rows of product records. Each column has a size of 70 characters. The searches are conducted after a large-scale data cleaning operation, which causes a database exception due to increased workload and decreased performance.\n"
}
{
    "start_time": "1697297388",
    "end_time": "1697297503",
    "start_timestamp": "2023-10-14 23:29:48",
    "end_timestamp": "2023-10-14 23:31:43",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 57\n    \n    # Number of rows to insert\n    num_rows = 799006\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a digital marketing company, the database contains 57 columns and 799,006 rows of customer information. Each column has a size of 56 characters. Initially, a large number of indexes are created for customer attributes such as name, age, and gender. Then, nine users simultaneously perform queries on the customer data, and the indexes are deleted after the queries are completed. This simulates the additional storage and performance impact caused by redundant indexes.\n"
}
{
    "start_time": "1697297563",
    "end_time": "1697297653",
    "start_timestamp": "2023-10-14 23:32:43",
    "end_timestamp": "2023-10-14 23:34:13",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are uploading, downloading, or editing files at the same time, the system is experiencing contention for input/output (I/O) resources. This results in a slowdown in file transfers.\n"
}
{
    "start_time": "1697297714",
    "end_time": "1697297774",
    "start_timestamp": "2023-10-14 23:35:14",
    "end_timestamp": "2023-10-14 23:36:14",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a large-scale data analysis scenario, multiple users simultaneously perform a join operation on a database table containing a large number of columns and rows. This join operation puts a high load on the CPU, causing contention and resulting in poor performance.\n"
}
{
    "start_time": "1697297834",
    "end_time": "1697297983",
    "start_timestamp": "2023-10-14 23:37:14",
    "end_timestamp": "2023-10-14 23:39:43",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online store's database, when fetching a large amount of data from the inventory, the script executes subqueries that have a correlation with the main query. If these correlated subqueries are not optimized, the performance of the query may degrade.\n"
}
{
    "start_time": "1697298043",
    "end_time": "1697298115",
    "start_timestamp": "2023-10-14 23:40:43",
    "end_timestamp": "2023-10-14 23:41:55",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 117\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 11\n    \n    # Number of rows to insert\n    num_rows = 55\n    \n    # Size of each column (in characters)\n    column_size = 33\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a scientific research facility, 117 experiments are being conducted simultaneously. Each experiment generates a large amount of data with 11 parameters, each consisting of 33 characters. The data from these experiments needs to be inserted into the database, which may result in a database exception due to the high volume of data being inserted concurrently.\n"
}
{
    "start_time": "1697298175",
    "end_time": "1697298246",
    "start_timestamp": "2023-10-14 23:42:55",
    "end_timestamp": "2023-10-14 23:44:06",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 117\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 26\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, 117 sensors generate a large amount of data that needs to be inserted into the database simultaneously. This process involves inserting data into a table containing 26 columns and 68 rows, with each column having a size of 78 characters. The purpose is to simulate the database exception that can occur due to the insertion of such a large volume of data.\n"
}
{
    "start_time": "1697298306",
    "end_time": "1697298367",
    "start_timestamp": "2023-10-14 23:45:06",
    "end_timestamp": "2023-10-14 23:46:07",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 55\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 97\n    \n    # Number of rows to insert\n    num_rows = 255\n    \n    # Size of each column (in characters)\n    column_size = 60\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a real-time online ticket booking system, 55 customers are trying to book tickets simultaneously. This operation triggers a lock contention, where multiple users compete to lock the database table and perform the update operation. The database table contains 97 columns, 255 rows, and each column has a size of 60 characters.\n"
}
{
    "start_time": "1697298427",
    "end_time": "1697298498",
    "start_timestamp": "2023-10-14 23:47:07",
    "end_timestamp": "2023-10-14 23:48:18",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 177\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 11\n    \n    # Number of rows to insert\n    num_rows = 3025995\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 177 users are simultaneously searching a database table that contains 11 columns, 3,025,995 rows, and each column size is 78 characters. The search operation is followed by a vacuum operation on the database table, which aims to optimize the table's performance and storage space utilization. This scenario simulates the potential exception that could occur during this process.\n"
}
{
    "start_time": "1697298558",
    "end_time": "1697298672",
    "start_timestamp": "2023-10-14 23:49:18",
    "end_timestamp": "2023-10-14 23:51:12",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 87\n    \n    # Number of rows to insert\n    num_rows = 831757\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial database with 87 columns and 831,757 rows, each with a column size of 56 characters, a large number of unnecessary indexes are created at the beginning of the query operation for various financial information such as transaction date, account holder, and transaction amount. This can result in additional storage consumption and performance overhead. The query is performed by 5 users simultaneously.\n"
}
{
    "start_time": "1697298732",
    "end_time": "1697298823",
    "start_timestamp": "2023-10-14 23:52:12",
    "end_timestamp": "2023-10-14 23:53:43",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, there is contention for input/output (I/O) operations. This causes a slowdown in file transfers. By running the command \"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION\", the script simulates a scenario where multiple users are uploading, downloading, or editing files simultaneously, creating I/O contention.\n"
}
{
    "start_time": "1697298883",
    "end_time": "1697298943",
    "start_timestamp": "2023-10-14 23:54:43",
    "end_timestamp": "2023-10-14 23:55:43",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis task, the join operation between two large tables is performed, but the performance is poor due to inefficient join algorithms. Additionally, the CPU is heavily burdened by other processes running simultaneously, resulting in contention and further slowing down the join operation.\n"
}
{
    "start_time": "1697299003",
    "end_time": "1697299153",
    "start_timestamp": "2023-10-14 23:56:43",
    "end_timestamp": "2023-10-14 23:59:13",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In the database of an e-commerce platform, when querying the inventory for a large number of products, there is a possibility of using correlated subqueries, which can lead to a deterioration in performance. This can cause a delay in retrieving large amounts of data.\n"
}
{
    "start_time": "1697299213",
    "end_time": "1697299285",
    "start_timestamp": "2023-10-15 00:00:13",
    "end_timestamp": "2023-10-15 00:01:25",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 152\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 64\n    \n    # Size of each column (in characters)\n    column_size = 57\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a sensor data collection system, 152 sensors are generating a large amount of data that needs to be inserted into the database. Each data entry has 12 columns, with each column having a size of 57 characters. The system will simulate the database exception caused by this data insertion process.\n"
}
{
    "start_time": "1697299345",
    "end_time": "1697299417",
    "start_timestamp": "2023-10-15 00:02:25",
    "end_timestamp": "2023-10-15 00:03:37",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 152\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 34\n    \n    # Number of rows to insert\n    num_rows = 86\n    \n    # Size of each column (in characters)\n    column_size = 65\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an internet of things (IoT) system, 152 sensors generate a large amount of data that needs to be inserted into a database simultaneously. Each data entry consists of 34 columns, with each column having a size of 65 characters. There are a total of 86 data entries. This simulation aims to trigger a database exception due to the overwhelming amount of data being inserted at once.\n"
}
{
    "start_time": "1697299477",
    "end_time": "1697299537",
    "start_timestamp": "2023-10-15 00:04:37",
    "end_timestamp": "2023-10-15 00:05:37",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 156\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 72\n    \n    # Number of rows to insert\n    num_rows = 291\n    \n    # Size of each column (in characters)\n    column_size = 81\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online store, there is a situation where 156 users simultaneously try to perform frequent update operations. These operations involve a database table with 72 columns and 291 rows of records related to products. Each column is 81 characters in size. The users compete to lock the database table and perform the update operations, which might result in database contention and potential exceptions.\n"
}
{
    "start_time": "1697299598",
    "end_time": "1697299673",
    "start_timestamp": "2023-10-15 00:06:38",
    "end_timestamp": "2023-10-15 00:07:53",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 191\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 3771540\n    \n    # Size of each column (in characters)\n    column_size = 65\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the inventory database of a retail store, 191 employees simultaneously perform searches using various search criteria such as product name, category, and price range. The searches are executed on a database table containing 12 columns and 3,771,540 rows, with each column having a size of 65 characters. This simulation represents the potential exception that could occur due to a large number of search queries without proper index usage after a database vacuum operation.\n"
}
{
    "start_time": "1697299733",
    "end_time": "1697299848",
    "start_timestamp": "2023-10-15 00:08:53",
    "end_timestamp": "2023-10-15 00:10:48",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 7\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 59\n    \n    # Number of rows to insert\n    num_rows = 819303\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used by an e-commerce platform, 7 users perform a query that involves multiple unnecessary indexes. The database table contains 59 columns and 819,303 rows, with each column containing 99 characters of product information. This simulation aims to demonstrate the additional storage footprint and performance overhead caused by the redundant indexes.\n"
}
{
    "start_time": "1697299908",
    "end_time": "1697299999",
    "start_timestamp": "2023-10-15 00:11:48",
    "end_timestamp": "2023-10-15 00:13:19",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file-sharing system, multiple users are uploading, downloading, or editing files simultaneously. This creates contention for the I/O operations in the system, leading to slower file transfer speeds.\n"
}
{
    "start_time": "1697300059",
    "end_time": "1697300120",
    "start_timestamp": "2023-10-15 00:14:19",
    "end_timestamp": "2023-10-15 00:15:20",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a large-scale data analytics system, multiple joins are performed between tables with poor join performance. Additionally, there is contention for CPU resources during the join operation, resulting in degraded performance of the system.\n"
}
{
    "start_time": "1697300180",
    "end_time": "1697300330",
    "start_timestamp": "2023-10-15 00:16:20",
    "end_timestamp": "2023-10-15 00:18:50",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, when trying to fetch a large amount of data, particularly when using correlated subqueries, there is a potential for the performance of the query to degrade. This means that the system may take longer to provide the requested data if the subqueries are not optimized properly.\n"
}
{
    "start_time": "1697300390",
    "end_time": "1697300461",
    "start_timestamp": "2023-10-15 00:19:50",
    "end_timestamp": "2023-10-15 00:21:01",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 109\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 66\n    \n    # Size of each column (in characters)\n    column_size = 25\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data management system where multiple users are simultaneously inserting a large amount of data, 109 threads are used to simulate the insertion process. The data being inserted consists of 16 columns with each column containing a maximum of 25 characters. There are a total of 66 rows being inserted. This scenario helps identify any potential exceptions or issues that may arise during this process.\n"
}
{
    "start_time": "1697300521",
    "end_time": "1697300593",
    "start_timestamp": "2023-10-15 00:22:01",
    "end_timestamp": "2023-10-15 00:23:13",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 109\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 32\n    \n    # Number of rows to insert\n    num_rows = 94\n    \n    # Size of each column (in characters)\n    column_size = 83\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, a large amount of data generated by 109 sensors needs to be inserted into the database simultaneously. Each sensor generates data with 32 columns of information, each column containing 83 characters. The database table contains 94 rows to store this data. Running this script simulates the database exception caused by the insertion process.\n"
}
{
    "start_time": "1697300653",
    "end_time": "1697300713",
    "start_timestamp": "2023-10-15 00:24:13",
    "end_timestamp": "2023-10-15 00:25:13",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 176\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 63\n    \n    # Number of rows to insert\n    num_rows = 307\n    \n    # Size of each column (in characters)\n    column_size = 96\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online marketplace, 176 users are trying to perform simultaneous update operations on a product table that contains 63 columns and 307 rows. Each column has a size of 96 characters. Due to the high number of concurrent requests and the resulting lock contention, there might be a delay or exception in the database.\n"
}
{
    "start_time": "1697300773",
    "end_time": "1697300847",
    "start_timestamp": "2023-10-15 00:26:13",
    "end_timestamp": "2023-10-15 00:27:27",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 136\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 2211862\n    \n    # Size of each column (in characters)\n    column_size = 83\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online store's database with a table containing 16 columns and 2,211,862 rows of product records, each with a column size of 83 characters, 136 users simultaneously perform a search after a large-scale data cleaning operation. This simulates the scenario of users searching for products using various filters like product name, category, and price range, after performing a vacuum operation on the database table.\n"
}
{
    "start_time": "1697300907",
    "end_time": "1697301022",
    "start_timestamp": "2023-10-15 00:28:27",
    "end_timestamp": "2023-10-15 00:30:22",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 76\n    \n    # Number of rows to insert\n    num_rows = 733097\n    \n    # Size of each column (in characters)\n    column_size = 68\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a large online database, where there are 5 users searching for information in a table with 76 columns and 733,097 rows, each column having a size of 68 characters, the database has created redundant indexes for various attributes such as product name, category, and price range. This can lead to unnecessary storage and performance overhead.\n"
}
{
    "start_time": "1697301082",
    "end_time": "1697301173",
    "start_timestamp": "2023-10-15 00:31:22",
    "end_timestamp": "2023-10-15 00:32:53",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are simultaneously uploading, downloading, or editing files, causing competition for input/output operations. This results in slower file transfer speeds.\n"
}
{
    "start_time": "1697301233",
    "end_time": "1697301293",
    "start_timestamp": "2023-10-15 00:33:53",
    "end_timestamp": "2023-10-15 00:34:53",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In an online ticketing system, multiple users are searching for events based on different criteria such as location, date, and ticket price. However, the join operation between the event table and the location table is not optimized, leading to poor performance. Additionally, there is CPU contention as multiple users are running resource-intensive queries simultaneously, causing the CPU to be overloaded and slowing down the overall system performance.\n"
}
{
    "start_time": "1697301353",
    "end_time": "1697301503",
    "start_timestamp": "2023-10-15 00:35:53",
    "end_timestamp": "2023-10-15 00:38:23",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online store's database, there is a situation where a large amount of data needs to be fetched, specifically the inventory information for each product. This process involves executing correlated subqueries. If these subqueries are not optimized, the performance of retrieving the inventory data may be negatively impacted.\n"
}
{
    "start_time": "1697301564",
    "end_time": "1697301635",
    "start_timestamp": "2023-10-15 00:39:24",
    "end_timestamp": "2023-10-15 00:40:35",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 69\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 86\n    \n    # Size of each column (in characters)\n    column_size = 71\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, there is a situation where 69 sensors generate a large amount of data that needs to be simultaneously inserted into a database table. The table contains 16 columns, and each column can hold up to 71 characters. There are 86 rows in the table. This scenario simulates a database exception caused by the process of inserting the large data set.\n"
}
{
    "start_time": "1697301695",
    "end_time": "1697301766",
    "start_timestamp": "2023-10-15 00:41:35",
    "end_timestamp": "2023-10-15 00:42:46",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 69\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 22\n    \n    # Number of rows to insert\n    num_rows = 77\n    \n    # Size of each column (in characters)\n    column_size = 71\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, there is a need to insert a large amount of data into a database. This scenario simulates the database exception caused by inserting data from 69 sensors simultaneously. The database table contains 22 columns, with each column having a size of 71 characters. There are 77 rows of data to be inserted.\n"
}
{
    "start_time": "1697301826",
    "end_time": "1697301886",
    "start_timestamp": "2023-10-15 00:43:46",
    "end_timestamp": "2023-10-15 00:44:46",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 67\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 65\n    \n    # Number of rows to insert\n    num_rows = 361\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 67 users are simultaneously performing frequent update operations in a database table containing 65 columns and 361 rows of product records, each with a column size of 99 characters. The users compete with each other to lock the database table to perform the update operations.\n"
}
{
    "start_time": "1697301946",
    "end_time": "1697301992",
    "start_timestamp": "2023-10-15 00:45:46",
    "end_timestamp": "2023-10-15 00:46:32",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 56\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 3445648\n    \n    # Size of each column (in characters)\n    column_size = 71\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, if there are 56 users searching in the database table containing 16 columns, 3,445,648 rows, each column size of 71 characters, after a large-scale data cleaning operation, it may cause an exception in the database.\n"
}
{
    "start_time": "1697302052",
    "end_time": "1697302167",
    "start_timestamp": "2023-10-15 00:47:32",
    "end_timestamp": "2023-10-15 00:49:27",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 76\n    \n    # Number of rows to insert\n    num_rows = 589889\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database for an online marketplace, when 9 users concurrently perform a query operation on a database table with 76 columns and 589,889 rows of product records, where each column has a size of 52 characters, a large number of unnecessary indexes are created at the beginning of the query. These indexes cause additional storage usage and performance overhead.\n"
}
{
    "start_time": "1697302227",
    "end_time": "1697302318",
    "start_timestamp": "2023-10-15 00:50:27",
    "end_timestamp": "2023-10-15 00:51:58",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are sharing files, if there is a simultaneous upload, download, or editing of files, it creates I/O contention. This contention leads to a slowdown in file transfer and affects the overall performance of the system.\n"
}
{
    "start_time": "1697302378",
    "end_time": "1697302438",
    "start_timestamp": "2023-10-15 00:52:58",
    "end_timestamp": "2023-10-15 00:53:58",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system handling customer orders and product information, there is a scenario where multiple users are performing join queries between large tables. These join queries are not optimized, leading to poor performance. Additionally, there is contention among the users for CPU resources, further affecting the query performance.\n"
}
{
    "start_time": "1697302498",
    "end_time": "1697302648",
    "start_timestamp": "2023-10-15 00:54:58",
    "end_timestamp": "2023-10-15 00:57:28",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform, when trying to fetch a large amount of data, specifically the inventory of each product, the system uses correlated subqueries to find the necessary information. However, if these subqueries are not optimized, the performance of the inventory searching process may degrade.\n"
}
{
    "start_time": "1697302708",
    "end_time": "1697302780",
    "start_timestamp": "2023-10-15 00:58:28",
    "end_timestamp": "2023-10-15 00:59:40",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 19\n    \n    # Number of rows to insert\n    num_rows = 65\n    \n    # Size of each column (in characters)\n    column_size = 41\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a system where data is being inserted into a database, there is a need to insert a large amount of data. This simulation involves inserting data from 100 sources at the same time. The database table being used has 19 columns and each column can hold up to 41 characters. There are a total of 65 rows in the table. This process aims to identify and trigger any anomalies or exceptions that may occur during the insertion of such large amounts of data.\n"
}
{
    "start_time": "1697302840",
    "end_time": "1697302911",
    "start_timestamp": "2023-10-15 01:00:40",
    "end_timestamp": "2023-10-15 01:01:51",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 100\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 38\n    \n    # Number of rows to insert\n    num_rows = 53\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) application, 100 sensors are generating a large amount of data, which needs to be inserted into a database simultaneously. This process simulates the database exception caused by inserting data from these sensors.\n"
}
{
    "start_time": "1697302971",
    "end_time": "1697303031",
    "start_timestamp": "2023-10-15 01:02:51",
    "end_timestamp": "2023-10-15 01:03:51",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 127\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 69\n    \n    # Number of rows to insert\n    num_rows = 351\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a busy online marketplace, 127 users are simultaneously trying to update product records in a database table that has 69 columns and 351 rows. Each column contains data with a size of 92 characters. Due to the high number of users competing for access to the database table, there is a potential for lock contention and database exceptions may occur.\n"
}
{
    "start_time": "1697303092",
    "end_time": "1697303148",
    "start_timestamp": "2023-10-15 01:04:52",
    "end_timestamp": "2023-10-15 01:05:48",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 180\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 19\n    \n    # Number of rows to insert\n    num_rows = 2286854\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a mobile banking application, 180 users are simultaneously searching for transaction information in a database table that contains 19 columns, 2,286,854 rows, and each column with a size of 84 characters. This process triggers a simulated database exception due to the lack of optimization after performing a large-scale data cleaning operation.\n"
}
{
    "start_time": "1697303208",
    "end_time": "1697303323",
    "start_timestamp": "2023-10-15 01:06:48",
    "end_timestamp": "2023-10-15 01:08:43",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 89\n    \n    # Number of rows to insert\n    num_rows = 635369\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online store with a large database containing 89 columns and 635,369 rows, each column having a size of 92 characters, the script simulates the creation of redundant indexes on various attributes such as product name, category, and price range. After the indexes are created, 9 users perform simultaneous queries on the database. The script measures the additional storage requirements and performance overhead caused by the redundant indexes.\n"
}
{
    "start_time": "1697303383",
    "end_time": "1697303474",
    "start_timestamp": "2023-10-15 01:09:43",
    "end_timestamp": "2023-10-15 01:11:14",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are simultaneously uploading, downloading, or editing files, there is intense competition for input/output (I/O) resources. This leads to slowed down file transfers and overall system performance.\n"
}
{
    "start_time": "1697303534",
    "end_time": "1697303594",
    "start_timestamp": "2023-10-15 01:12:14",
    "end_timestamp": "2023-10-15 01:13:14",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics system, multiple users are performing join operations on large datasets. The join performance is poor due to suboptimal join algorithms or inefficient use of indexes. Additionally, there is CPU contention as multiple users are simultaneously executing resource-intensive queries, which leads to slower query execution times.\n"
}
{
    "start_time": "1697303655",
    "end_time": "1697303804",
    "start_timestamp": "2023-10-15 01:14:15",
    "end_timestamp": "2023-10-15 01:16:44",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online shopping platform's database, when trying to retrieve a large amount of data and perform related subqueries to determine the inventory for each product, the execution of these subqueries may not be optimized, leading to slower performance in querying the inventory.\n"
}
{
    "start_time": "1697303864",
    "end_time": "1697303935",
    "start_timestamp": "2023-10-15 01:17:44",
    "end_timestamp": "2023-10-15 01:18:55",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 98\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 66\n    \n    # Size of each column (in characters)\n    column_size = 48\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a real-life scenario, 98 sensors are generating a large amount of data that needs to be inserted into a database simultaneously. The database table has 16 columns, each with a size of 48 characters, and there are 66 rows of data to be inserted. By simulating this process, we can observe and analyze any exceptions or issues that might arise due to this high-volume data insertion.\n"
}
{
    "start_time": "1697303995",
    "end_time": "1697304067",
    "start_timestamp": "2023-10-15 01:19:55",
    "end_timestamp": "2023-10-15 01:21:07",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 98\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 75\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analysis project, 98 threads are used to simultaneously insert a large amount of data into a database table with 20 columns and 100 rows, each column having a size of 75 characters. This can cause a database exception due to the high volume of data being inserted at the same time.\n"
}
{
    "start_time": "1697304127",
    "end_time": "1697304187",
    "start_timestamp": "2023-10-15 01:22:07",
    "end_timestamp": "2023-10-15 01:23:07",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 161\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 90\n    \n    # Number of rows to insert\n    num_rows = 364\n    \n    # Size of each column (in characters)\n    column_size = 68\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for an online store, 161 users simultaneously attempt to perform frequent update operations in a database table containing 90 columns and 364 rows of product records. Each product record has a column size of 68 characters. These users compete with each other to lock the database table for updates, causing contention and potentially triggering a database exception.\n"
}
{
    "start_time": "1697304247",
    "end_time": "1697304319",
    "start_timestamp": "2023-10-15 01:24:07",
    "end_timestamp": "2023-10-15 01:25:19",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 130\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 2045737\n    \n    # Size of each column (in characters)\n    column_size = 75\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 130 users simultaneously perform a search after a large-scale data cleaning operation on a database table containing 16 columns, 2,045,737 rows, each column size of 75 characters of commodity records.\n"
}
{
    "start_time": "1697304379",
    "end_time": "1697304493",
    "start_timestamp": "2023-10-15 01:26:19",
    "end_timestamp": "2023-10-15 01:28:13",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 83\n    \n    # Number of rows to insert\n    num_rows = 935270\n    \n    # Size of each column (in characters)\n    column_size = 88\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database of an e-commerce platform, there is a scenario where multiple users are performing a query operation using redundant indexes. The specific scenario involves 5 users searching in a database table containing 83 columns and 935,270 rows. Each column has a size of 88 characters. The purpose of this scenario is to simulate the additional storage footprint and performance overhead that arise due to the presence of redundant indexes.\n"
}
{
    "start_time": "1697304553",
    "end_time": "1697304644",
    "start_timestamp": "2023-10-15 01:29:13",
    "end_timestamp": "2023-10-15 01:30:44",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users upload, download, or edit files at the same time, the system experiences contention in input/output operations due to the large amount of data being processed simultaneously. This leads to slower file transfer speeds.\n"
}
{
    "start_time": "1697304704",
    "end_time": "1697304764",
    "start_timestamp": "2023-10-15 01:31:44",
    "end_timestamp": "2023-10-15 01:32:44",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system, when performing join operations between multiple tables with poor join performance due to a large number of records and limited resources, high CPU contention can occur. This can lead to slower query execution and resource competition among multiple users or processes trying to access the CPU simultaneously.\n"
}
{
    "start_time": "1697304824",
    "end_time": "1697304974",
    "start_timestamp": "2023-10-15 01:33:44",
    "end_timestamp": "2023-10-15 01:36:14",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a database used by an e-commerce platform, there is a scenario where a large amount of data needs to be fetched from the database, specifically related to the inventory of each product. This requires executing correlated subqueries. However, if the subqueries are not optimized, the performance of the inventory querying process may be negatively impacted.\n"
}
{
    "start_time": "1697305034",
    "end_time": "1697305107",
    "start_timestamp": "2023-10-15 01:37:14",
    "end_timestamp": "2023-10-15 01:38:27",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 198\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 8\n    \n    # Number of rows to insert\n    num_rows = 65\n    \n    # Size of each column (in characters)\n    column_size = 38\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an industrial manufacturing process, when 198 machines are simultaneously generating a large amount of data and trying to insert it into the database, the process faces challenges due to the high number of threads and the specific characteristics of the data. This may result in exceptions or slow performance during the data insertion process. The data being inserted consists of 65 rows with 8 columns, each column having a size of 38 characters.\n"
}
{
    "start_time": "1697305167",
    "end_time": "1697305239",
    "start_timestamp": "2023-10-15 01:39:27",
    "end_timestamp": "2023-10-15 01:40:39",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 198\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 40\n    \n    # Number of rows to insert\n    num_rows = 93\n    \n    # Size of each column (in characters)\n    column_size = 73\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a real-life scenario, this script represents a situation where 198 devices are generating a large amount of data simultaneously. This data needs to be inserted into a database, which has a table with 40 columns and 93 rows. Each column has a size of 73 characters. The purpose of running this script is to simulate the database exception that can occur due to the high volume of data being inserted at the same time.\n"
}
{
    "start_time": "1697305299",
    "end_time": "1697305360",
    "start_timestamp": "2023-10-15 01:41:39",
    "end_timestamp": "2023-10-15 01:42:40",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 77\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 98\n    \n    # Number of rows to insert\n    num_rows = 380\n    \n    # Size of each column (in characters)\n    column_size = 87\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online marketplace, 77 users are trying to perform frequent update operations on a database table containing 98 columns and 380 rows of product records. Each column has a size of 87 characters. Due to contention for locking the database table, these users are competing with each other, resulting in a database exception.\n"
}
{
    "start_time": "1697305420",
    "end_time": "1697305533",
    "start_timestamp": "2023-10-15 01:43:40",
    "end_timestamp": "2023-10-15 01:45:33",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 60\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 8\n    \n    # Number of rows to insert\n    num_rows = 2799635\n    \n    # Size of each column (in characters)\n    column_size = 73\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online store, there are 60 users performing searches on a table with 8 columns and 2,799,635 rows of product records. Each column can hold up to 73 characters. The searches are done after a vacuum operation, which is a large-scale data cleaning process. This simulation is meant to test for any exceptions caused by the search operation.\n"
}
{
    "start_time": "1697305593",
    "end_time": "1697305710",
    "start_timestamp": "2023-10-15 01:46:33",
    "end_timestamp": "2023-10-15 01:48:30",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 59\n    \n    # Number of rows to insert\n    num_rows = 459312\n    \n    # Size of each column (in characters)\n    column_size = 87\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used by a financial institution, there are 5 users simultaneously executing queries on a table with 59 columns and 459,312 rows of financial transaction records. Each column has a size of 87 characters. The queries involve redundant indexes that were created at the beginning of the operations and deleted after. This process causes additional storage consumption and performance overhead in the database.\n"
}
{
    "start_time": "1697305770",
    "end_time": "1697305861",
    "start_timestamp": "2023-10-15 01:49:30",
    "end_timestamp": "2023-10-15 01:51:01",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users share files, there is contention for input/output (I/O) resources when users simultaneously upload, download, or edit files. This competition slows down the file transfer process.\n"
}
{
    "start_time": "1697305921",
    "end_time": "1697305981",
    "start_timestamp": "2023-10-15 01:52:01",
    "end_timestamp": "2023-10-15 01:53:01",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a multi-user system where queries involving complex joins are executed, if there is poor performance due to inefficient or improperly optimized join operations, combined with high CPU contention caused by multiple users competing for CPU resources, it can result in slower query execution times and system performance degradation.\n"
}
{
    "start_time": "1697306041",
    "end_time": "1697306198",
    "start_timestamp": "2023-10-15 01:54:01",
    "end_timestamp": "2023-10-15 01:56:38",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online shopping platform, there is a problem with fetching large amounts of data and running correlated subqueries. This could occur when trying to retrieve inventory information for numerous products. Without optimizing the subqueries, the performance of the inventory query may be negatively affected.\n"
}
{
    "start_time": "1697306259",
    "end_time": "1697306331",
    "start_timestamp": "2023-10-15 01:57:39",
    "end_timestamp": "2023-10-15 01:58:51",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 151\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 93\n    \n    # Size of each column (in characters)\n    column_size = 53\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application where 151 sensors generate a large amount of data, the data needs to be inserted into the database simultaneously. The database will face an exception due to the high volume of concurrent data insertion. The database table contains 5 columns, each with a size of 53 characters, and has 93 rows of data.\n"
}
{
    "start_time": "1697306391",
    "end_time": "1697306463",
    "start_timestamp": "2023-10-15 01:59:51",
    "end_timestamp": "2023-10-15 02:01:03",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 151\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 21\n    \n    # Number of rows to insert\n    num_rows = 71\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, where 151 sensors generate a large amount of data that needs to be inserted into the database simultaneously using a script. The database table contains 21 columns and 71 rows of sensor readings, with each column having a size of 70 characters. This process simulates the database exception caused by the simultaneous data insertion.\n"
}
{
    "start_time": "1697306523",
    "end_time": "1697306583",
    "start_timestamp": "2023-10-15 02:02:03",
    "end_timestamp": "2023-10-15 02:03:03",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 106\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 50\n    \n    # Number of rows to insert\n    num_rows = 396\n    \n    # Size of each column (in characters)\n    column_size = 64\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 106 users simultaneously attempt to perform frequent update operations in a database table containing 50 columns and 396 rows of product records, each with a column size of 64 characters. These users compete with each other to lock the database table and perform the update operations.\n"
}
{
    "start_time": "1697306643",
    "end_time": "1697306740",
    "start_timestamp": "2023-10-15 02:04:03",
    "end_timestamp": "2023-10-15 02:05:40",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 89\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2765909\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the back-end database of an online platform, 89 users are performing a search operation simultaneously, using 5 columns in a table with 2,765,909 rows. Each column can store up to 70 characters. The search operations are performed after a database maintenance operation called \"VACUUM\", which may cause a delay or exception in the database due to the large amounts of data being processed.\n"
}
{
    "start_time": "1697306800",
    "end_time": "1697306914",
    "start_timestamp": "2023-10-15 02:06:40",
    "end_timestamp": "2023-10-15 02:08:34",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 76\n    \n    # Number of rows to insert\n    num_rows = 427732\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used by an online retailer, a redundant index is created for a large dataset containing 76 columns and 427,732 rows. This index is unnecessary and will result in additional storage usage and performance overhead.\n"
}
{
    "start_time": "1697306974",
    "end_time": "1697307065",
    "start_timestamp": "2023-10-15 02:09:34",
    "end_timestamp": "2023-10-15 02:11:05",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are uploading, downloading, or editing files at the same time. This creates competition for input/output operations, causing a slowdown in file transfers.\n"
}
{
    "start_time": "1697307125",
    "end_time": "1697307185",
    "start_timestamp": "2023-10-15 02:12:05",
    "end_timestamp": "2023-10-15 02:13:05",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics system, multiple users are performing a join operation on a large dataset. However, the join performance is poor due to inefficient query optimization and lack of necessary indices. Additionally, the system is facing CPU contention as multiple users compete for computing resources, leading to delays in query execution.\n"
}
{
    "start_time": "1697307245",
    "end_time": "1697307386",
    "start_timestamp": "2023-10-15 02:14:05",
    "end_timestamp": "2023-10-15 02:16:26",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online shopping platform's database, retrieving a large amount of data using related subqueries can lead to a degradation in performance. This degradation occurs when querying inventory for each product, especially when there is a large number of products.\n"
}
{
    "start_time": "1697307446",
    "end_time": "1697307517",
    "start_timestamp": "2023-10-15 02:17:26",
    "end_timestamp": "2023-10-15 02:18:37",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 55\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 70\n    \n    # Size of each column (in characters)\n    column_size = 23\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 55 devices are all trying to send a large amount of data to a central database simultaneously. Each device has 9 data fields, with each field being 23 characters long, and there are a total of 70 data entries. This process may cause the database to encounter an exception due to the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697307577",
    "end_time": "1697307648",
    "start_timestamp": "2023-10-15 02:19:37",
    "end_timestamp": "2023-10-15 02:20:48",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 55\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 28\n    \n    # Number of rows to insert\n    num_rows = 95\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, 55 sensors generate a large amount of data that needs to be inserted into a database simultaneously. This data consists of 28 columns, with each column storing 54 characters, and there are a total of 95 rows. This process simulates a database exception caused by the insertion of such large amounts of data.\n"
}
{
    "start_time": "1697307708",
    "end_time": "1697307768",
    "start_timestamp": "2023-10-15 02:21:48",
    "end_timestamp": "2023-10-15 02:22:48",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 167\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 61\n    \n    # Number of rows to insert\n    num_rows = 338\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online database system, there are 167 users simultaneously trying to perform frequent update operations on a database table containing 61 columns and 338 rows of records. Each column has a size of 63 characters. The users are competing to lock the database table while performing the updates. This simulation aims to trigger a database exception due to the contention for locking in the system.\n"
}
{
    "start_time": "1697307828",
    "end_time": "1697307943",
    "start_timestamp": "2023-10-15 02:23:48",
    "end_timestamp": "2023-10-15 02:25:43",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 78\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 3555214\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of an e-commerce platform, 78 users simultaneously perform a search operation after a large-scale data cleaning operation on a table containing 9 columns, 3,555,214 rows, where each column has a size of 54 characters. This simulates the database exception that can occur in this scenario.\n"
}
{
    "start_time": "1697308003",
    "end_time": "1697308119",
    "start_timestamp": "2023-10-15 02:26:43",
    "end_timestamp": "2023-10-15 02:28:39",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 606760\n    \n    # Size of each column (in characters)\n    column_size = 57\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial institution's database, 9 users perform a query on a table with 95 columns and 606,760 rows. Each column has a size of 57 characters. The query involves redundant indexes on attributes such as customer ID, transaction amount, and transaction date. This can result in additional storage consumption and performance overhead.\n"
}
{
    "start_time": "1697308179",
    "end_time": "1697308270",
    "start_timestamp": "2023-10-15 02:29:39",
    "end_timestamp": "2023-10-15 02:31:10",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are uploading, downloading, or editing files simultaneously, the system experiences contention for input/output resources. This results in slower file transfer speeds.\n"
}
{
    "start_time": "1697308330",
    "end_time": "1697308390",
    "start_timestamp": "2023-10-15 02:32:10",
    "end_timestamp": "2023-10-15 02:33:10",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a business intelligence system, multiple complex join operations between large tables are performed. This puts a heavy load on the CPU, causing contention issues. The performance of join operations deteriorates, leading to slow query execution.\n"
}
{
    "start_time": "1697308450",
    "end_time": "1697308591",
    "start_timestamp": "2023-10-15 02:34:10",
    "end_timestamp": "2023-10-15 02:36:31",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a scenario where an e-commerce platform database is being used, there is a need to fetch a large amount of data related to inventory for each product. This requires executing correlated subqueries, which can be time-consuming and impact the performance of querying inventory, especially when dealing with a large number of products.\n"
}
{
    "start_time": "1697308651",
    "end_time": "1697308723",
    "start_timestamp": "2023-10-15 02:37:31",
    "end_timestamp": "2023-10-15 02:38:43",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 188\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 88\n    \n    # Size of each column (in characters)\n    column_size = 59\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data processing system, 188 sensors simultaneously generate large amounts of data that need to be inserted into a database. This process could cause a database exception if not properly handled. The data consists of 16 columns, each with a size of 59 characters, and there are 88 rows in total.\n"
}
{
    "start_time": "1697308783",
    "end_time": "1697308855",
    "start_timestamp": "2023-10-15 02:39:43",
    "end_timestamp": "2023-10-15 02:40:55",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 188\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 38\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, a large amount of data generated by 188 sources needs to be inserted into the database simultaneously. This can result in a database exception due to the high volume of data being processed.\n"
}
{
    "start_time": "1697308916",
    "end_time": "1697308976",
    "start_timestamp": "2023-10-15 02:41:56",
    "end_timestamp": "2023-10-15 02:42:56",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 122\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 74\n    \n    # Number of rows to insert\n    num_rows = 341\n    \n    # Size of each column (in characters)\n    column_size = 77\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a large company, 122 employees are simultaneously trying to perform frequent update operations on a database table containing 74 columns and 341 rows of records. Each record has a column size of 77 characters. Due to the high number of users competing to lock the database table, a database exception occurs.\n"
}
{
    "start_time": "1697309036",
    "end_time": "1697309076",
    "start_timestamp": "2023-10-15 02:43:56",
    "end_timestamp": "2023-10-15 02:44:36",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 169\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 3143893\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online platform, a large number of users are searching for products using various criteria such as product name, category, and price range. The database table contains 16 columns and 3,143,893 rows, with each column having a size of 72 characters. However, there is an issue with the database where it needs to be cleaned up and optimized. This script simulates the database exception that occurs when 169 users simultaneously perform a search after the VACUUM operation is performed on the database table.\n"
}
{
    "start_time": "1697309136",
    "end_time": "1697309205",
    "start_timestamp": "2023-10-15 02:45:36",
    "end_timestamp": "2023-10-15 02:46:45",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 83\n    \n    # Number of rows to insert\n    num_rows = 911621\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database system for an online marketplace, 10 users are conducting a query operation on a database table containing 83 columns and 911,621 rows of data. Each column has a size of 51 characters. However, these users are also creating redundant indexes for items such as product name, category, and price range during the query, which can lead to additional storage usage and performance overhead.\n"
}
{
    "start_time": "1697309265",
    "end_time": "1697309356",
    "start_timestamp": "2023-10-15 02:47:45",
    "end_timestamp": "2023-10-15 02:49:16",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files simultaneously, there is a high volume of data being uploaded, downloaded, or edited. This creates contention for input/output operations, resulting in slower file transfers.\n"
}
{
    "start_time": "1697309416",
    "end_time": "1697309476",
    "start_timestamp": "2023-10-15 02:50:16",
    "end_timestamp": "2023-10-15 02:51:16",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a business intelligence system, multiple users are performing join operations on large datasets simultaneously. The system experiences poor performance in the join operations, and there is competition for CPU resources among the users, resulting in slower execution.\n"
}
{
    "start_time": "1697309536",
    "end_time": "1697309695",
    "start_timestamp": "2023-10-15 02:52:16",
    "end_timestamp": "2023-10-15 02:54:55",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online store's database, retrieving a large amount of data for each product, such as inventory levels, may involve executing related subqueries. If these subqueries are not optimized, the performance of querying inventory may be negatively affected.\n"
}
{
    "start_time": "1697309756",
    "end_time": "1697309763",
    "start_timestamp": "2023-10-15 02:55:56",
    "end_timestamp": "2023-10-15 02:56:03",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 146\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 65\n    \n    # Size of each column (in characters)\n    column_size = 69\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application like an IoT system, a large number of data generated by 146 sensors needs to be inserted into the database simultaneously. Each data point contains 5 columns, with each column having a size of 69 characters. The database might experience an exception due to the high volume of data being inserted at the same time.\n"
}
{
    "start_time": "1697309823",
    "end_time": "1697309895",
    "start_timestamp": "2023-10-15 02:57:03",
    "end_timestamp": "2023-10-15 02:58:15",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 146\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 33\n    \n    # Number of rows to insert\n    num_rows = 94\n    \n    # Size of each column (in characters)\n    column_size = 69\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data logging system, 146 sensors are generating a large amount of data simultaneously. This data needs to be inserted into a database table consisting of 33 columns and 94 rows, with each column having a size of 69 characters. The purpose of this script is to simulate the database exception that can occur due to the simultaneous insertion of such a large amount of data.\n"
}
{
    "start_time": "1697309955",
    "end_time": "1697310015",
    "start_timestamp": "2023-10-15 02:59:15",
    "end_timestamp": "2023-10-15 03:00:15",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 125\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 55\n    \n    # Number of rows to insert\n    num_rows = 375\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online store, 125 users attempt to perform frequent update operations simultaneously. The database table contains 55 columns and 375 rows of product records, with each column having a size of 99 characters. These users compete with each other to lock the database table for performing update operations. Simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697310075",
    "end_time": "1697310166",
    "start_timestamp": "2023-10-15 03:01:15",
    "end_timestamp": "2023-10-15 03:02:46",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 144\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2075265\n    \n    # Size of each column (in characters)\n    column_size = 69\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, if there are 144 users simultaneously performing a search operation after a large-scale data cleaning operation on a database table containing 5 columns, 2,075,265 rows, each column size of 69 characters of product records, an exception is caused due to the competition for database resources.\n"
}
{
    "start_time": "1697310226",
    "end_time": "1697310341",
    "start_timestamp": "2023-10-15 03:03:46",
    "end_timestamp": "2023-10-15 03:05:41",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 7\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 92\n    \n    # Number of rows to insert\n    num_rows = 577945\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace's database, there are 7 users searching for products in a table containing 92 columns and 577,945 rows. Each column has a size of 84 characters. However, there are redundant indexes created at the beginning of the search operation, which may lead to additional storage usage and performance overhead.\n"
}
{
    "start_time": "1697310401",
    "end_time": "1697310492",
    "start_timestamp": "2023-10-15 03:06:41",
    "end_timestamp": "2023-10-15 03:08:12",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users upload, download, or edit files simultaneously, there is a contention for input/output operations. This results in slower file transfers.\n"
}
{
    "start_time": "1697310552",
    "end_time": "1697310612",
    "start_timestamp": "2023-10-15 03:09:12",
    "end_timestamp": "2023-10-15 03:10:12",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In an enterprise resource planning (ERP) system, multiple users are simultaneously performing join operations on a database table containing a large amount of data. However, the join operation is not optimized, leading to poor performance. Additionally, there is contention for CPU resources among the users, further impacting the performance of the join operation.\n"
}
{
    "start_time": "1697310672",
    "end_time": "1697310822",
    "start_timestamp": "2023-10-15 03:11:12",
    "end_timestamp": "2023-10-15 03:13:42",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, this statement might be used in an e-commerce platform where the inventory levels of each product need to be determined. This requires executing subqueries that are related to each product. If these subqueries are not optimized, querying the inventory for a large number of products may result in poor performance.\n"
}
{
    "start_time": "1697310882",
    "end_time": "1697310953",
    "start_timestamp": "2023-10-15 03:14:42",
    "end_timestamp": "2023-10-15 03:15:53",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 72\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 87\n    \n    # Size of each column (in characters)\n    column_size = 24\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, 72 sensors generate a large amount of data that needs to be simultaneously inserted into a database. The database table has 7 columns and 87 rows of data, with each column containing 24 characters. This simulates an exception caused by the insertion of a large amount of data into the database.\n"
}
{
    "start_time": "1697311014",
    "end_time": "1697311085",
    "start_timestamp": "2023-10-15 03:16:54",
    "end_timestamp": "2023-10-15 03:18:05",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 72\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 36\n    \n    # Number of rows to insert\n    num_rows = 62\n    \n    # Size of each column (in characters)\n    column_size = 65\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home automation system, 72 sensors generate a large amount of data that needs to be inserted into the database simultaneously. This data includes information about various aspects of the smart home, such as temperature, lighting, security, and energy consumption. Each data point is represented by 36 columns, with each column having a size of 65 characters. The data is organized into 62 rows, with each row representing a specific time period or event. This simulation helps identify any database exceptions that may occur due to the insertion of such large amounts of data.\n"
}
{
    "start_time": "1697311145",
    "end_time": "1697311205",
    "start_timestamp": "2023-10-15 03:19:05",
    "end_timestamp": "2023-10-15 03:20:05",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 174\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 63\n    \n    # Number of rows to insert\n    num_rows = 227\n    \n    # Size of each column (in characters)\n    column_size = 68\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online store, 174 users are simultaneously attempting to perform frequent update operations on a table containing 63 columns and 227 rows of product records, each with a column size of 68 characters. These users are competing with each other to lock the database table, resulting in database contention and potential exceptions.\n"
}
{
    "start_time": "1697311265",
    "end_time": "1697311352",
    "start_timestamp": "2023-10-15 03:21:05",
    "end_timestamp": "2023-10-15 03:22:32",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 200\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 3843688\n    \n    # Size of each column (in characters)\n    column_size = 65\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database scenario, where an online store has 200 users simultaneously searching the database table containing 7 columns and 3,843,688 rows, each column with a size of 65 characters, a vacuum operation is performed on the database. This operation involves cleaning up and reorganizing the data to optimize its storage and performance. However, the large number of concurrent search operations and the vacuum process may result in a database exception or performance degradation.\n"
}
{
    "start_time": "1697311412",
    "end_time": "1697311526",
    "start_timestamp": "2023-10-15 03:23:32",
    "end_timestamp": "2023-10-15 03:25:26",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 83\n    \n    # Number of rows to insert\n    num_rows = 563820\n    \n    # Size of each column (in characters)\n    column_size = 82\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace, 10 users simultaneously perform a query operation on a database table containing 83 columns and 563,820 rows of product records, each column having a size of 82 characters. These queries involve redundant indexes that were created at the beginning of the query and will be deleted after the operation. This process can result in additional storage requirements and performance overhead.\n"
}
{
    "start_time": "1697311586",
    "end_time": "1697311677",
    "start_timestamp": "2023-10-15 03:26:26",
    "end_timestamp": "2023-10-15 03:27:57",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing platform, multiple users are simultaneously uploading, downloading, or editing files, causing competition for input/output resources. This leads to a slowdown in the file transfer process.\n"
}
{
    "start_time": "1697311737",
    "end_time": "1697311797",
    "start_timestamp": "2023-10-15 03:28:57",
    "end_timestamp": "2023-10-15 03:29:57",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system used for analyzing customer data in a marketing campaign, there is a poor performance issue when performing join operations between multiple tables. This is due to the lack of proper indexing and optimization techniques. Additionally, there is a high level of contention for CPU resources caused by concurrent processes running on the same machine. This leads to slow query execution and reduced overall system performance.\n"
}
{
    "start_time": "1697311857",
    "end_time": "1697312007",
    "start_timestamp": "2023-10-15 03:30:57",
    "end_timestamp": "2023-10-15 03:33:27",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a retail database, fetching large amounts of inventory data for each product may involve executing related subqueries. This can lead to poor performance if the subqueries are not optimized.\n"
}
{
    "start_time": "1697312067",
    "end_time": "1697312140",
    "start_timestamp": "2023-10-15 03:34:27",
    "end_timestamp": "2023-10-15 03:35:40",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 173\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 14\n    \n    # Number of rows to insert\n    num_rows = 77\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, a large amount of data generated by 173 sensors needs to be inserted into the database simultaneously. This process will simulate the database exception caused by the insertion of data. The database table contains 14 columns, with each column having a size of 56 characters, and there are 77 rows of data.\n"
}
{
    "start_time": "1697312200",
    "end_time": "1697312272",
    "start_timestamp": "2023-10-15 03:36:40",
    "end_timestamp": "2023-10-15 03:37:52",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 173\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 36\n    \n    # Number of rows to insert\n    num_rows = 95\n    \n    # Size of each column (in characters)\n    column_size = 80\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) application, there are 173 devices generating a large amount of data that needs to be inserted into a database simultaneously. Each device produces data with 36 columns, each with a size of 80 characters, and there are a total of 95 rows of data. This simulation aims to test the database's performance and potential exceptions that may occur during this process.\n"
}
{
    "start_time": "1697312332",
    "end_time": "1697312392",
    "start_timestamp": "2023-10-15 03:38:52",
    "end_timestamp": "2023-10-15 03:39:52",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 71\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 61\n    \n    # Number of rows to insert\n    num_rows = 352\n    \n    # Size of each column (in characters)\n    column_size = 98\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online database with 61 columns and 352 rows of data records, each column having a size of 98 characters, simulate a scenario where 71 users simultaneously compete to lock the database table for performing update operations. This may cause a database exception due to contention for locking resources.\n"
}
{
    "start_time": "1697312452",
    "end_time": "1697312518",
    "start_timestamp": "2023-10-15 03:40:52",
    "end_timestamp": "2023-10-15 03:41:58",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 151\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 14\n    \n    # Number of rows to insert\n    num_rows = 2355912\n    \n    # Size of each column (in characters)\n    column_size = 80\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a busy online marketplace, 151 users are simultaneously searching the database table containing 14 columns and 2,355,912 rows of product records. Each column has a size of 80 characters. This scenario simulates the occurrence of an exception due to the lack of a necessary index after a vacuum operation, which is a large-scale data cleaning process.\n"
}
{
    "start_time": "1697312578",
    "end_time": "1697312693",
    "start_timestamp": "2023-10-15 03:42:58",
    "end_timestamp": "2023-10-15 03:44:53",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 86\n    \n    # Number of rows to insert\n    num_rows = 445743\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial database with 86 columns and 445,743 rows, each with a column size of 54 characters, a large number of indexes are created for various financial metrics at the beginning of the query. Eight users then perform queries on the database, causing additional storage and performance overhead due to the redundant indexes.\n"
}
{
    "start_time": "1697312753",
    "end_time": "1697312844",
    "start_timestamp": "2023-10-15 03:45:53",
    "end_timestamp": "2023-10-15 03:47:24",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "This script simulates a scenario in a file sharing system where multiple users are uploading, downloading, or editing files at the same time. As a result, there is I/O contention, which causes a slowdown in file transfers.\n"
}
{
    "start_time": "1697312904",
    "end_time": "1697312964",
    "start_timestamp": "2023-10-15 03:48:24",
    "end_timestamp": "2023-10-15 03:49:24",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis system, multiple users are executing a join operation that involves multiple tables with poor join performance. At the same time, there is a high level of competition for CPU resources, which further degrades the performance of the join operation.\n"
}
{
    "start_time": "1697313024",
    "end_time": "1697313174",
    "start_timestamp": "2023-10-15 03:50:24",
    "end_timestamp": "2023-10-15 03:52:54",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail system, when retrieving inventory data for each product, related subqueries are used. However, if these subqueries are not optimized and there is a large number of products, the performance of the inventory query may be negatively affected.\n"
}
{
    "start_time": "1697313234",
    "end_time": "1697313305",
    "start_timestamp": "2023-10-15 03:53:54",
    "end_timestamp": "2023-10-15 03:55:05",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 70\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 51\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 70 devices are simultaneously sending a large amount of data to be inserted into the database. These devices have 10 different types of data and each data type has a size of 70 characters. The number of data rows to be inserted is 51.\n"
}
{
    "start_time": "1697313366",
    "end_time": "1697313437",
    "start_timestamp": "2023-10-15 03:56:06",
    "end_timestamp": "2023-10-15 03:57:17",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 70\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 25\n    \n    # Number of rows to insert\n    num_rows = 82\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 70 sensors generate a large amount of data simultaneously. This data needs to be inserted into the database, which may lead to a database exception. The data consists of 25 columns, with each column having a size of 63 characters. There are a total of 82 rows in the data.\n"
}
{
    "start_time": "1697313497",
    "end_time": "1697313557",
    "start_timestamp": "2023-10-15 03:58:17",
    "end_timestamp": "2023-10-15 03:59:17",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 70\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 83\n    \n    # Number of rows to insert\n    num_rows = 228\n    \n    # Size of each column (in characters)\n    column_size = 91\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for an online store, 70 users simultaneously attempt to perform frequent update operations on a database table containing 83 columns and 228 rows of product records, each with a column size of 91 characters. These users compete with each other to lock the database table, resulting in contention and potentially causing database exceptions.\n"
}
{
    "start_time": "1697313617",
    "end_time": "1697313709",
    "start_timestamp": "2023-10-15 04:00:17",
    "end_timestamp": "2023-10-15 04:01:49",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 178\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 3628154\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 178 users are simultaneously performing searches after a large-scale data cleaning operation on a database table that contains 10 columns and 3,628,154 rows. Each column has a size of 63 characters. This process simulates the potential exception that can occur due to the search operation.\n"
}
{
    "start_time": "1697313770",
    "end_time": "1697313883",
    "start_timestamp": "2023-10-15 04:02:50",
    "end_timestamp": "2023-10-15 04:04:43",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 83\n    \n    # Number of rows to insert\n    num_rows = 532309\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database of an e-commerce platform with 83 columns and 532,309 rows, each with a column size of 100 characters, a large number of redundant indexes are created for attributes such as product name, category, and price range. These redundant indexes can cause additional storage overhead and impact the performance of query operations.\n"
}
{
    "start_time": "1697313943",
    "end_time": "1697314034",
    "start_timestamp": "2023-10-15 04:05:43",
    "end_timestamp": "2023-10-15 04:07:14",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, if there is a high demand for uploading, downloading, or editing files simultaneously, it can cause contention in the input/output (I/O) operations. This can result in slower file transfer speeds and decreased performance.\n"
}
{
    "start_time": "1697314094",
    "end_time": "1697314154",
    "start_timestamp": "2023-10-15 04:08:14",
    "end_timestamp": "2023-10-15 04:09:14",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a busy office environment, multiple employees simultaneously execute resource-intensive join operations on a large database table with poor join performance. This results in excessive CPU usage, causing contention among the employees and slowing down their tasks.\n"
}
{
    "start_time": "1697314214",
    "end_time": "1697314365",
    "start_timestamp": "2023-10-15 04:10:14",
    "end_timestamp": "2023-10-15 04:12:45",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online store's database, there is a scenario where the system needs to fetch a large amount of data and perform correlated subqueries. For example, when trying to calculate the inventory of each product, the system might need to run subqueries to retrieve related data. If not optimized properly, this process can lead to a decrease in query performance.\n"
}
{
    "start_time": "1697314425",
    "end_time": "1697314496",
    "start_timestamp": "2023-10-15 04:13:45",
    "end_timestamp": "2023-10-15 04:14:56",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 15\n    \n    # Number of rows to insert\n    num_rows = 50\n    \n    # Size of each column (in characters)\n    column_size = 31\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, 53 sensors are generating a large amount of data that needs to be inserted into a database. Each data entry consists of 15 columns, with each column having a size of 31 characters. In total, there are 50 data entries. This process may cause an exception in the database due to the simultaneous insertion of a large amount of data.\n"
}
{
    "start_time": "1697314556",
    "end_time": "1697314627",
    "start_timestamp": "2023-10-15 04:15:56",
    "end_timestamp": "2023-10-15 04:17:07",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 30\n    \n    # Number of rows to insert\n    num_rows = 55\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a real-life scenario, there is a data-intensive application where 53 users simultaneously insert a large amount of data into a database table. The table contains 30 columns, each with a size of 99 characters, and there are 55 rows of data. This simulation aims to test the performance and stability of the database under such conditions.\n"
}
{
    "start_time": "1697314687",
    "end_time": "1697314747",
    "start_timestamp": "2023-10-15 04:18:07",
    "end_timestamp": "2023-10-15 04:19:07",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 96\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 80\n    \n    # Number of rows to insert\n    num_rows = 314\n    \n    # Size of each column (in characters)\n    column_size = 55\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for an online store, 96 users simultaneously attempt to perform frequent update operations on a database table with 80 columns and 314 rows of product records, each with a column size of 55 characters. These users compete with each other to lock the database table and perform the update operations, simulating a database exception caused by the contention for locks.\n"
}
{
    "start_time": "1697314807",
    "end_time": "1697314859",
    "start_timestamp": "2023-10-15 04:20:07",
    "end_timestamp": "2023-10-15 04:20:59",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 162\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 15\n    \n    # Number of rows to insert\n    num_rows = 3400337\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system that supports an online store, there are 162 users simultaneously searching for products using various criteria such as product name, category, and price range. The search is performed on a table with 15 columns and 3,400,337 rows, where each column can hold up to 99 characters. In this scenario, the search may encounter an exception due to the lack of optimization or resource limitations in the database system.\n"
}
{
    "start_time": "1697314919",
    "end_time": "1697315033",
    "start_timestamp": "2023-10-15 04:21:59",
    "end_timestamp": "2023-10-15 04:23:53",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 78\n    \n    # Number of rows to insert\n    num_rows = 512563\n    \n    # Size of each column (in characters)\n    column_size = 80\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a large-scale e-commerce database, multiple users are performing queries on a table with 78 columns and 512,563 rows of product records, each with a column size of 80 characters. However, there is a performance issue caused by the creation of redundant indexes for various search parameters such as product name, category, and price range. This impacts both the storage footprint and query performance.\n"
}
{
    "start_time": "1697315093",
    "end_time": "1697315183",
    "start_timestamp": "2023-10-15 04:24:53",
    "end_timestamp": "2023-10-15 04:26:23",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are simultaneously uploading, downloading, or editing files, there is intense competition for I/O resources. As a result, the file transfer process slows down due to the increased congestion and contention for access to the underlying storage system.\n"
}
{
    "start_time": "1697315243",
    "end_time": "1697315304",
    "start_timestamp": "2023-10-15 04:27:23",
    "end_timestamp": "2023-10-15 04:28:24",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a financial system that processes large amounts of transaction data, there is a performance issue when performing a join operation to combine data from multiple tables. Additionally, there is CPU contention as multiple users are simultaneously executing complex calculations and algorithms, causing a slowdown in overall system performance.\n"
}
{
    "start_time": "1697315364",
    "end_time": "1697315514",
    "start_timestamp": "2023-10-15 04:29:24",
    "end_timestamp": "2023-10-15 04:31:54",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online store's inventory management system, fetching large amounts of data from the database and executing correlated subqueries to find the quantity of inventory for each product can lead to performance issues. This script simulates the database exception caused by this process.\n"
}
{
    "start_time": "1697315574",
    "end_time": "1697315645",
    "start_timestamp": "2023-10-15 04:32:54",
    "end_timestamp": "2023-10-15 04:34:05",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 65\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 79\n    \n    # Size of each column (in characters)\n    column_size = 60\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a sensor data collection system, 65 sensors are generating a large amount of data simultaneously. This data needs to be inserted into a database table with 9 columns and 79 rows. Each column has a size of 60 characters. This process may cause a database exception due to the high volume of data being inserted.\n"
}
{
    "start_time": "1697315706",
    "end_time": "1697315777",
    "start_timestamp": "2023-10-15 04:35:06",
    "end_timestamp": "2023-10-15 04:36:17",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 65\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 30\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 91\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a scientific research project, 65 sensors are collecting data simultaneously and trying to insert it into the database. Each data point contains 30 columns with a size of 91 characters, and there are a total of 68 data points. This simulates the scenario where a large amount of data is being inserted into the database, causing a potential exception due to the high volume of concurrent insertions.\n"
}
{
    "start_time": "1697315837",
    "end_time": "1697315897",
    "start_timestamp": "2023-10-15 04:37:17",
    "end_timestamp": "2023-10-15 04:38:17",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 178\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 89\n    \n    # Number of rows to insert\n    num_rows = 289\n    \n    # Size of each column (in characters)\n    column_size = 90\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online marketplace, there are 178 users simultaneously competing to update product records. The database table contains 89 columns and 289 rows, with each column having a size of 90 characters. This process simulates a scenario where multiple users are contending for locks on the database table, causing contention and potential exceptions.\n"
}
{
    "start_time": "1697315957",
    "end_time": "1697316024",
    "start_timestamp": "2023-10-15 04:39:17",
    "end_timestamp": "2023-10-15 04:40:24",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 161\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 2927750\n    \n    # Size of each column (in characters)\n    column_size = 91\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for managing customer orders in an e-commerce platform, there is a need to periodically clean up and optimize the database table. However, if 161 users simultaneously perform searches on a table with 9 columns and 2,927,750 rows of order records, with each column containing 91 characters, it may cause a database exception due to the ongoing vacuuming process.\n"
}
{
    "start_time": "1697316084",
    "end_time": "1697316198",
    "start_timestamp": "2023-10-15 04:41:24",
    "end_timestamp": "2023-10-15 04:43:18",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 74\n    \n    # Number of rows to insert\n    num_rows = 746668\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial database with 74 columns and 746,668 rows, each with a column size of 70 characters, multiple indexes are created for fields such as account number, transaction date, and amount at the beginning of a query. Five users then perform a query operation, and these indexes are removed after the query. This simulates the increased storage usage and performance impact caused by having redundant indexes in the database.\n"
}
{
    "start_time": "1697316258",
    "end_time": "1697316349",
    "start_timestamp": "2023-10-15 04:44:18",
    "end_timestamp": "2023-10-15 04:45:49",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a collaborative document editing tool, multiple users are simultaneously uploading, downloading, or editing files, causing contention in the input/output operations of the file system. This results in slower file transfers and editing processes.\n"
}
{
    "start_time": "1697316409",
    "end_time": "1697316470",
    "start_timestamp": "2023-10-15 04:46:49",
    "end_timestamp": "2023-10-15 04:47:50",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database used by an online gaming platform, there is a performance issue when executing a join operation due to poor optimization. This operation involves joining multiple tables related to player data. Additionally, there is CPU contention caused by multiple players actively using the gaming platform at the same time, which further affects the overall system performance.\n"
}
{
    "start_time": "1697316530",
    "end_time": "1697316680",
    "start_timestamp": "2023-10-15 04:48:50",
    "end_timestamp": "2023-10-15 04:51:20",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, there is a scenario where a large amount of data needs to be fetched while executing correlated subqueries. This can result in reduced performance while querying the inventory if the related subqueries are not optimized.\n"
}
{
    "start_time": "1697316740",
    "end_time": "1697316812",
    "start_timestamp": "2023-10-15 04:52:20",
    "end_timestamp": "2023-10-15 04:53:32",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 178\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 60\n    \n    # Size of each column (in characters)\n    column_size = 27\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) system, 178 sensors generate a large amount of data to be inserted into a database simultaneously. Each data record contains 18 columns, with each column having a size of 27 characters. There are a total of 60 data records. This script simulates the database exception that can occur due to the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697316872",
    "end_time": "1697316945",
    "start_timestamp": "2023-10-15 04:54:32",
    "end_timestamp": "2023-10-15 04:55:45",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 178\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 25\n    \n    # Number of rows to insert\n    num_rows = 56\n    \n    # Size of each column (in characters)\n    column_size = 81\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, 178 threads are used to simultaneously insert a large amount of data into a database table. Each row in the table contains 25 columns, with each column having a size of 81 characters. The table contains a total of 56 rows. This process is designed to simulate the database exception that occurs when a high volume of data is being inserted into the database.\n"
}
{
    "start_time": "1697317005",
    "end_time": "1697317065",
    "start_timestamp": "2023-10-15 04:56:45",
    "end_timestamp": "2023-10-15 04:57:45",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 84\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 87\n    \n    # Number of rows to insert\n    num_rows = 226\n    \n    # Size of each column (in characters)\n    column_size = 79\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online marketplace, 84 users simultaneously attempt to perform frequent update operations on a database table containing 87 columns and 226 rows of product records. Each column has a size of 79 characters. Due to the high number of users competing to lock the database table, a database exception occurs as they try to perform the update operations.\n"
}
{
    "start_time": "1697317125",
    "end_time": "1697317168",
    "start_timestamp": "2023-10-15 04:58:45",
    "end_timestamp": "2023-10-15 04:59:28",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 136\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 3657776\n    \n    # Size of each column (in characters)\n    column_size = 81\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the scenario of an online store's database, when 136 users simultaneously search for products using terms such as name, category, and price range in a database table containing 18 columns, 3,657,776 rows of product records, each column size being 81 characters, an exception is triggered due to the lack of a necessary index.\n"
}
{
    "start_time": "1697317228",
    "end_time": "1697317341",
    "start_timestamp": "2023-10-15 05:00:28",
    "end_timestamp": "2023-10-15 05:02:21",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 86\n    \n    # Number of rows to insert\n    num_rows = 415253\n    \n    # Size of each column (in characters)\n    column_size = 87\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database for an e-commerce platform, there are 5 users simultaneously querying a table with 86 columns and 415,253 rows of product records. Each column has a size of 87 characters. The query involves redundant indexes that were created initially but are later deleted. The purpose is to simulate the impact of additional storage and performance overhead caused by this process.\n"
}
{
    "start_time": "1697317402",
    "end_time": "1697317492",
    "start_timestamp": "2023-10-15 05:03:22",
    "end_timestamp": "2023-10-15 05:04:52",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users upload, download, or edit files simultaneously, the file system experiences competition for input/output (I/O) resources. This results in a slowdown of file transfer operations.\n"
}
{
    "start_time": "1697317552",
    "end_time": "1697317613",
    "start_timestamp": "2023-10-15 05:05:52",
    "end_timestamp": "2023-10-15 05:06:53",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a busy online platform, multiple users simultaneously perform join operations on a database table with poor performance. This causes high contention for CPU resources and slows down the performance of the system.\n"
}
{
    "start_time": "1697317673",
    "end_time": "1697317820",
    "start_timestamp": "2023-10-15 05:07:53",
    "end_timestamp": "2023-10-15 05:10:20",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce database, when searching for the amount of inventory for each product, the system performs related subqueries. However, if these subqueries are not optimized and there are a large number of products, the performance of the inventory query may be adversely affected.\n"
}
{
    "start_time": "1697317880",
    "end_time": "1697317952",
    "start_timestamp": "2023-10-15 05:11:20",
    "end_timestamp": "2023-10-15 05:12:32",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 155\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 17\n    \n    # Number of rows to insert\n    num_rows = 60\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data logging system, 155 data logs need to be inserted into a database table simultaneously. Each data log contains 17 columns, with each column having a size of 51 characters. The dataset consists of 60 logs. This script simulates the database exception caused by the simultaneous insertion of a large amount of data.\n"
}
{
    "start_time": "1697318012",
    "end_time": "1697318084",
    "start_timestamp": "2023-10-15 05:13:32",
    "end_timestamp": "2023-10-15 05:14:44",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 155\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 26\n    \n    # Number of rows to insert\n    num_rows = 85\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 155 sensors are collecting data from various devices and sending it to the database. This large amount of data needs to be inserted into the database simultaneously. Each device has 26 parameters with a column size of 84 characters, and there are a total of 85 devices. This process may result in an exception in the database due to the high volume of data being inserted.\n"
}
{
    "start_time": "1697318144",
    "end_time": "1697318204",
    "start_timestamp": "2023-10-15 05:15:44",
    "end_timestamp": "2023-10-15 05:16:44",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 102\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 52\n    \n    # Number of rows to insert\n    num_rows = 340\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a multi-user system, 102 users simultaneously attempt to update a database table with 52 columns and 340 rows of records, each with a column size of 51 characters. These users compete with each other to lock the database table during the update operation, causing contention and potentially leading to a database exception.\n"
}
{
    "start_time": "1697318264",
    "end_time": "1697318306",
    "start_timestamp": "2023-10-15 05:17:44",
    "end_timestamp": "2023-10-15 05:18:26",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 107\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 17\n    \n    # Number of rows to insert\n    num_rows = 3637430\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a large e-commerce platform, multiple users perform a search operation after conducting a data cleaning process on a database table containing 17 columns and 3,637,430 rows of product records, each with a column size of 84 characters. This search operation is simulated to trigger an exception in the database due to the lack of necessary optimization for such a large dataset. The search is performed concurrently by 107 users, emphasizing the potential performance and scalability issues that may arise in real-life scenarios.\n"
}
{
    "start_time": "1697318366",
    "end_time": "1697318480",
    "start_timestamp": "2023-10-15 05:19:26",
    "end_timestamp": "2023-10-15 05:21:20",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 804778\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online retail database, indexes are created redundantly for various product attributes like name, category, and price range. This leads to additional storage requirements and performance overhead. Simulate the effects of this situation with 8 threads searching in a database table containing 95 columns and 804,778 rows of product records, where each column has a size of 78 characters.\n"
}
{
    "start_time": "1697318540",
    "end_time": "1697318631",
    "start_timestamp": "2023-10-15 05:22:20",
    "end_timestamp": "2023-10-15 05:23:51",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a busy file sharing system, multiple users are uploading, downloading, or editing files at the same time. This creates competition for I/O resources, causing file transfer to slow down.\n"
}
{
    "start_time": "1697318691",
    "end_time": "1697318752",
    "start_timestamp": "2023-10-15 05:24:51",
    "end_timestamp": "2023-10-15 05:25:52",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database used for analyzing customer data, there is a situation where the execution of join operations between multiple tables is not optimized, leading to poor performance. Additionally, there is high competition for CPU resources due to other processes running on the system, resulting in slower query execution.\n"
}
{
    "start_time": "1697318812",
    "end_time": "1697318959",
    "start_timestamp": "2023-10-15 05:26:52",
    "end_timestamp": "2023-10-15 05:29:19",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online marketplace, when trying to retrieve a large amount of data such as inventory information for each product, the query involves correlated subqueries. If these subqueries are not optimized properly, the performance of the inventory retrieval process may be negatively affected.\n"
}
{
    "start_time": "1697319019",
    "end_time": "1697319091",
    "start_timestamp": "2023-10-15 05:30:19",
    "end_timestamp": "2023-10-15 05:31:31",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 161\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 85\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, a large amount of data generated by 161 sensors needs to be inserted into the database simultaneously. The database contains a table with 9 columns and 85 rows, with each column having a size of 72 characters. This process simulates the database exception caused by the high volume of data insertion.\n"
}
{
    "start_time": "1697319151",
    "end_time": "1697319223",
    "start_timestamp": "2023-10-15 05:32:31",
    "end_timestamp": "2023-10-15 05:33:43",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 161\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 28\n    \n    # Number of rows to insert\n    num_rows = 94\n    \n    # Size of each column (in characters)\n    column_size = 95\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, 161 data sources are simultaneously inserting a large amount of data with 28 columns, where each column can store up to 95 characters, into the database. This process may lead to database exceptions due to the high volume and concurrency of data insertion.\n"
}
{
    "start_time": "1697319283",
    "end_time": "1697319343",
    "start_timestamp": "2023-10-15 05:34:43",
    "end_timestamp": "2023-10-15 05:35:43",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 183\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 72\n    \n    # Number of rows to insert\n    num_rows = 359\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online gaming platform's database, 183 players are simultaneously trying to update information in a table containing 72 columns and 359 rows of player records, with each column having a size of 84 characters. These players are competing with each other to lock the table for updates, leading to a database exception.\n"
}
{
    "start_time": "1697319403",
    "end_time": "1697319469",
    "start_timestamp": "2023-10-15 05:36:43",
    "end_timestamp": "2023-10-15 05:37:49",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 188\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 2343320\n    \n    # Size of each column (in characters)\n    column_size = 95\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, there are 188 users searching in the database table containing 9 columns, 2,343,320 rows. Each column has a size of 95 characters. The search operation is performed after a large-scale data cleaning operation.\n"
}
{
    "start_time": "1697319530",
    "end_time": "1697319644",
    "start_timestamp": "2023-10-15 05:38:50",
    "end_timestamp": "2023-10-15 05:40:44",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 91\n    \n    # Number of rows to insert\n    num_rows = 928167\n    \n    # Size of each column (in characters)\n    column_size = 94\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace, the database is configured with redundant indexes for various attributes such as product name, category, and price range. However, when 9 users simultaneously perform queries on a database table with 91 columns, 928,167 rows, and column sizes of 94 characters, the performance of the database may be negatively impacted due to the extra storage footprint and overhead caused by the redundant indexes.\n"
}
{
    "start_time": "1697319704",
    "end_time": "1697319795",
    "start_timestamp": "2023-10-15 05:41:44",
    "end_timestamp": "2023-10-15 05:43:15",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a shared file system where multiple users are simultaneously uploading, downloading, or editing files, there is a high amount of I/O contention, causing delays in file transfers. This scenario is simulated using the given script \"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION\".\n"
}
{
    "start_time": "1697319855",
    "end_time": "1697319915",
    "start_timestamp": "2023-10-15 05:44:15",
    "end_timestamp": "2023-10-15 05:45:15",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system for a financial institution, multiple users are attempting to perform join operations on large tables, which require significant computational power. However, due to high CPU contention, the performance of these join operations is poor.\n"
}
{
    "start_time": "1697319975",
    "end_time": "1697320121",
    "start_timestamp": "2023-10-15 05:46:15",
    "end_timestamp": "2023-10-15 05:48:41",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online e-commerce platform, when trying to retrieve a large amount of data from the database, specifically the inventory information for each product, the process may involve executing related subqueries. If these subqueries are not optimized, the performance of retrieving inventory data can decrease.\n"
}
{
    "start_time": "1697320182",
    "end_time": "1697320253",
    "start_timestamp": "2023-10-15 05:49:42",
    "end_timestamp": "2023-10-15 05:50:53",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 98\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 14\n    \n    # Number of rows to insert\n    num_rows = 78\n    \n    # Size of each column (in characters)\n    column_size = 44\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analysis system, there is a need to insert a large amount of data into the database simultaneously. This process involves inserting data from 98 sources, with each source having 14 columns and each column having a size of 44 characters. The total number of rows being inserted is 78.\n"
}
{
    "start_time": "1697320313",
    "end_time": "1697320384",
    "start_timestamp": "2023-10-15 05:51:53",
    "end_timestamp": "2023-10-15 05:53:04",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 98\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 33\n    \n    # Number of rows to insert\n    num_rows = 62\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a system where sensor data is being collected and inserted into a database, 98 sensors are generating a large amount of data. The database table has 33 columns, each with a size of 92 characters, and there are 62 rows of data to be inserted. This simulates the scenario of inserting a large volume of data into the database, which can potentially cause exceptions or slow down the insertion process.\n"
}
{
    "start_time": "1697320444",
    "end_time": "1697320505",
    "start_timestamp": "2023-10-15 05:54:04",
    "end_timestamp": "2023-10-15 05:55:05",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 60\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 55\n    \n    # Number of rows to insert\n    num_rows = 340\n    \n    # Size of each column (in characters)\n    column_size = 88\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 60 users simultaneously attempt to perform frequent update operations in a database table containing 55 columns and 340 rows of product records each with a column size of 88 characters. These users compete with each other to lock the database table to perform the update operations.\n"
}
{
    "start_time": "1697320565",
    "end_time": "1697320625",
    "start_timestamp": "2023-10-15 05:56:05",
    "end_timestamp": "2023-10-15 05:57:05",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 187\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 14\n    \n    # Number of rows to insert\n    num_rows = 2541105\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 187 users concurrently search for products using terms such as product name, category, price range, etc. on a database table with 14 columns and 2,541,105 rows. Each column has a size of 92 characters. This scenario simulates an exception caused by heavy search activity after a large-scale data cleaning operation (vacuum) on the database table.\n"
}
{
    "start_time": "1697320685",
    "end_time": "1697320799",
    "start_timestamp": "2023-10-15 05:58:05",
    "end_timestamp": "2023-10-15 05:59:59",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 75\n    \n    # Number of rows to insert\n    num_rows = 638193\n    \n    # Size of each column (in characters)\n    column_size = 53\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a large e-commerce database with 75 columns and 638,193 rows, each column having a size of 53 characters, there is a scenario where excessive indexes are created for product information such as name, category, and price range. This is followed by a simulation of queries from multiple users, causing unnecessary storage overhead and decreased performance.\n"
}
{
    "start_time": "1697320859",
    "end_time": "1697320950",
    "start_timestamp": "2023-10-15 06:00:59",
    "end_timestamp": "2023-10-15 06:02:30",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a shared file system, when multiple users are simultaneously uploading, downloading, or editing files, there is a competition for input/output operations (IO contention). This leads to a slowdown in file transfer and impacts the overall performance of the system.\n"
}
{
    "start_time": "1697321010",
    "end_time": "1697321071",
    "start_timestamp": "2023-10-15 06:03:30",
    "end_timestamp": "2023-10-15 06:04:31",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis system, there are simultaneous join operations being performed between multiple tables, but the join performance is poor due to inefficient query optimization. Additionally, there is high CPU contention as multiple users are competing for CPU resources, leading to slower query execution and performance degradation.\n"
}
{
    "start_time": "1697321131",
    "end_time": "1697321277",
    "start_timestamp": "2023-10-15 06:05:31",
    "end_timestamp": "2023-10-15 06:07:57",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, when trying to retrieve a large amount of data, such as inventory information for each product, the retrieval process uses correlated subqueries. If these subqueries are not optimized, executing them for a large number of products can result in poor query performance.\n"
}
{
    "start_time": "1697321338",
    "end_time": "1697321410",
    "start_timestamp": "2023-10-15 06:08:58",
    "end_timestamp": "2023-10-15 06:10:10",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 195\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 86\n    \n    # Size of each column (in characters)\n    column_size = 33\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a logging system where multiple devices constantly send logs, a large number of logs need to be simultaneously inserted into the database. The insertion process is simulated with 195 threads, inserting logs into a database table with 12 columns and 86 rows. Each column has a size of 33 characters. This can trigger a database exception due to the high volume of simultaneous insertions.\n"
}
{
    "start_time": "1697321470",
    "end_time": "1697321543",
    "start_timestamp": "2023-10-15 06:11:10",
    "end_timestamp": "2023-10-15 06:12:23",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 195\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 33\n    \n    # Number of rows to insert\n    num_rows = 71\n    \n    # Size of each column (in characters)\n    column_size = 81\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, such as a financial system, a large amount of data generated by 195 sources needs to be inserted into the database simultaneously. This data insertion process involves 33 columns and 71 rows, with each column size set to 81 characters. The aim is to mimic the database exception that could occur due to the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697321603",
    "end_time": "1697321663",
    "start_timestamp": "2023-10-15 06:13:23",
    "end_timestamp": "2023-10-15 06:14:23",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 157\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 98\n    \n    # Number of rows to insert\n    num_rows = 376\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online store, there is a scenario where 157 users simultaneously attempt to perform frequent update operations. The database table they are updating has 98 columns and 376 rows of product records, each with a column size of 67 characters. These users compete with each other to lock the database table for updates, simulating a scenario of lock contention.\n"
}
{
    "start_time": "1697321723",
    "end_time": "1697321805",
    "start_timestamp": "2023-10-15 06:15:23",
    "end_timestamp": "2023-10-15 06:16:45",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 88\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 2664318\n    \n    # Size of each column (in characters)\n    column_size = 81\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an e-commerce platform's database, 88 users simultaneously perform a search operation after a large-scale data cleaning process on a database table containing 12 columns, 2,664,318 rows, with each column having a size of 81 characters for product records. The aim is to simulate the exception that occurs during this process.\n"
}
{
    "start_time": "1697321865",
    "end_time": "1697321978",
    "start_timestamp": "2023-10-15 06:17:45",
    "end_timestamp": "2023-10-15 06:19:38",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 79\n    \n    # Number of rows to insert\n    num_rows = 990412\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online platform database, a search operation is performed by 6 users simultaneously on a table with 79 columns and 990,412 rows, with each column containing 58 characters. This dataset has a large number of redundant indexes, which can result in additional storage space and performance overhead.\n"
}
{
    "start_time": "1697322038",
    "end_time": "1697322129",
    "start_timestamp": "2023-10-15 06:20:38",
    "end_timestamp": "2023-10-15 06:22:09",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, there is a high amount of I/O contention. This occurs when multiple users are simultaneously uploading, downloading, or editing files, causing the file transfer process to slow down.\n"
}
{
    "start_time": "1697322189",
    "end_time": "1697322249",
    "start_timestamp": "2023-10-15 06:23:09",
    "end_timestamp": "2023-10-15 06:24:09",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a large database containing multiple tables, there is a poor performance issue with joining tables due to a lack of optimization. This is compounded by high CPU contention, where multiple users are competing for CPU resources, resulting in slower data processing and query response times.\n"
}
{
    "start_time": "1697322309",
    "end_time": "1697322457",
    "start_timestamp": "2023-10-15 06:25:09",
    "end_timestamp": "2023-10-15 06:27:37",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail business, there is a need to fetch a large amount of data from the database, specifically related to inventory information. The query involves executing subqueries that are interdependent. If these subqueries are not optimized, the performance of retrieving inventory data may be negatively affected.\n"
}
{
    "start_time": "1697322518",
    "end_time": "1697322590",
    "start_timestamp": "2023-10-15 06:28:38",
    "end_timestamp": "2023-10-15 06:29:50",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 179\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 15\n    \n    # Number of rows to insert\n    num_rows = 67\n    \n    # Size of each column (in characters)\n    column_size = 25\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a real-life scenario, there is a data collection system that involves 179 sensors. These sensors generate a large amount of data simultaneously, which needs to be inserted into a database. The database table has 15 columns, each with a size of 25 characters, and there are 67 rows of data. This process aims to simulate the database exception that can occur due to the insertion of a large amount of data from multiple sensors at the same time.\n"
}
{
    "start_time": "1697322650",
    "end_time": "1697322722",
    "start_timestamp": "2023-10-15 06:30:50",
    "end_timestamp": "2023-10-15 06:32:02",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 179\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 26\n    \n    # Number of rows to insert\n    num_rows = 66\n    \n    # Size of each column (in characters)\n    column_size = 55\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analysis system, a large amount of data generated by 179 sensors needs to be inserted into the database simultaneously. Simulate the database exception caused by this process, where the database table contains 26 columns, 66 rows, and each column has a size of 55 characters.\n"
}
{
    "start_time": "1697322782",
    "end_time": "1697322843",
    "start_timestamp": "2023-10-15 06:33:02",
    "end_timestamp": "2023-10-15 06:34:03",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 119\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 92\n    \n    # Number of rows to insert\n    num_rows = 271\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a mobile gaming platform, 119 players are simultaneously trying to update their game progress in a database table containing 92 columns and 271 rows of player information, with each column having a size of 52 characters. Due to the high number of players competing for locks on the database table, a database exception is triggered during the update process.\n"
}
{
    "start_time": "1697322903",
    "end_time": "1697322981",
    "start_timestamp": "2023-10-15 06:35:03",
    "end_timestamp": "2023-10-15 06:36:21",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 166\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 15\n    \n    # Number of rows to insert\n    num_rows = 2207549\n    \n    # Size of each column (in characters)\n    column_size = 55\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an e-commerce platform's database, multiple users are searching for products using various criteria such as product name, category, and price range. The database table being searched contains 15 columns and 2,207,549 rows, with each column size being 55 characters. However, there is an exception in the database caused by a simultaneous search performed by 166 users after a large-scale data cleaning operation called \"VACUUM\".\n"
}
{
    "start_time": "1697323041",
    "end_time": "1697323154",
    "start_timestamp": "2023-10-15 06:37:21",
    "end_timestamp": "2023-10-15 06:39:14",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 851284\n    \n    # Size of each column (in characters)\n    column_size = 76\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial system, 8 users are simultaneously querying a database table with 95 columns and 851,284 rows, each column size being 76 characters. Initially, a large number of indexes are created for items such as account number, transaction type, and amount, but these indexes are deleted after the query operation. The purpose of this scenario is to simulate the additional storage footprint and performance overhead caused by the creation and deletion of redundant indexes in the system.\n"
}
{
    "start_time": "1697323214",
    "end_time": "1697323304",
    "start_timestamp": "2023-10-15 06:40:14",
    "end_timestamp": "2023-10-15 06:41:44",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are uploading, downloading, or editing files at the same time, the system experiences contention for input/output resources. This leads to slower file transfers and overall system performance.\n"
}
{
    "start_time": "1697323364",
    "end_time": "1697323425",
    "start_timestamp": "2023-10-15 06:42:44",
    "end_timestamp": "2023-10-15 06:43:45",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system experiencing poor join performance and CPU contention, the script \"anomaly_trigger/main.py\" is used to simulate a scenario where multiple users simultaneously perform join operations on a database with high CPU usage. This can result in slower query performance due to the competition for CPU resources among the users.\n"
}
{
    "start_time": "1697323485",
    "end_time": "1697323631",
    "start_timestamp": "2023-10-15 06:44:45",
    "end_timestamp": "2023-10-15 06:47:11",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, there is a scenario where a large amount of data needs to be fetched, specifically the inventory for each product. This involves executing correlated subqueries, which are subqueries that depend on the results of other queries. If these subqueries are not optimized and there are a large number of products, the performance of the inventory query may deteriorate.\n"
}
{
    "start_time": "1697323692",
    "end_time": "1697323764",
    "start_timestamp": "2023-10-15 06:48:12",
    "end_timestamp": "2023-10-15 06:49:24",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 102\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 67\n    \n    # Size of each column (in characters)\n    column_size = 27\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, if there are 102 devices simultaneously sending large amounts of data to the database, each device having 9 data points with a size of 27 characters, and a total of 67 data entries, the database may experience slow insertion and potential exceptions due to the high volume of data being inserted simultaneously.\n"
}
{
    "start_time": "1697323824",
    "end_time": "1697323895",
    "start_timestamp": "2023-10-15 06:50:24",
    "end_timestamp": "2023-10-15 06:51:35",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 102\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 75\n    \n    # Size of each column (in characters)\n    column_size = 82\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analytics system, 102 threads are simultaneously inserting a large amount of data into a database table. The table has 20 columns, with each column having a size of 82 characters, and a total of 75 rows. This simulation aims to trigger a database exception caused by the high volume of data being inserted.\n"
}
{
    "start_time": "1697323955",
    "end_time": "1697324015",
    "start_timestamp": "2023-10-15 06:52:35",
    "end_timestamp": "2023-10-15 06:53:35",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 89\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 53\n    \n    # Number of rows to insert\n    num_rows = 211\n    \n    # Size of each column (in characters)\n    column_size = 93\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, data is being updated by multiple users at the same time. There are 89 users simultaneously performing a frequent update operation in a database table containing 53 columns and 211 rows of product records. Each column has a size of 93 characters. The multiple users are competing with each other to lock the database table and perform the update operation. This simulates a database exception caused by contention for locking the table.\n"
}
{
    "start_time": "1697324075",
    "end_time": "1697324145",
    "start_timestamp": "2023-10-15 06:54:35",
    "end_timestamp": "2023-10-15 06:55:45",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 89\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 3648387\n    \n    # Size of each column (in characters)\n    column_size = 82\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of an online marketplace, when 89 users simultaneously search for products using terms such as product name, category, and price range, after performing a large-scale data cleaning operation on a table containing 9 columns, 3,648,387 rows, with each column having a size of 82 characters, an exception may occur due to the high workload and lack of optimization in the search process.\n"
}
{
    "start_time": "1697324205",
    "end_time": "1697324319",
    "start_timestamp": "2023-10-15 06:56:45",
    "end_timestamp": "2023-10-15 06:58:39",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 78\n    \n    # Number of rows to insert\n    num_rows = 485208\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database with 78 columns and 485,208 rows, each with a column size of 66 characters, a large number of indexes are created for items such as product name, category, and price range at the beginning of the query, followed by a query of 6 users. The purpose is to simulate the additional storage footprint and performance overhead caused by this process.\n"
}
{
    "start_time": "1697324379",
    "end_time": "1697324470",
    "start_timestamp": "2023-10-15 06:59:39",
    "end_timestamp": "2023-10-15 07:01:10",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a shared document platform, multiple users are simultaneously uploading, downloading, or editing files, causing contention for I/O resources. This leads to slower file transfer speeds.\n"
}
{
    "start_time": "1697324530",
    "end_time": "1697324590",
    "start_timestamp": "2023-10-15 07:02:10",
    "end_timestamp": "2023-10-15 07:03:10",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics company, several data analysts are performing join queries on large datasets using the Python script \"anomaly_trigger/main.py\". The join performance is poor due to inefficient join algorithms and the lack of proper indexing on the join columns. Additionally, there is CPU contention as multiple analysts are running their queries simultaneously, competing for CPU resources. This leads to slow query execution and overall poor performance in analyzing the data.\n"
}
{
    "start_time": "1697324650",
    "end_time": "1697324798",
    "start_timestamp": "2023-10-15 07:04:10",
    "end_timestamp": "2023-10-15 07:06:38",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, when fetching large amounts of data, specifically inventory information for each product, the execution of correlated subqueries is required. The performance of this query may be affected if the subqueries are not optimized properly.\n"
}
{
    "start_time": "1697324859",
    "end_time": "1697324931",
    "start_timestamp": "2023-10-15 07:07:39",
    "end_timestamp": "2023-10-15 07:08:51",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 144\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 61\n    \n    # Size of each column (in characters)\n    column_size = 33\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home automation system, there are 144 devices generating a large amount of data. This data needs to be inserted into the database simultaneously. However, the database is only configured to handle 10 columns, each with a size of 33 characters, and has a table with 61 rows. This simulation triggers a database exception due to the overload caused by the insertion of such large data.\n"
}
{
    "start_time": "1697324991",
    "end_time": "1697325063",
    "start_timestamp": "2023-10-15 07:09:51",
    "end_timestamp": "2023-10-15 07:11:03",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 144\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 28\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 83\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a collaborative project management tool, 144 users simultaneously attempt to insert a large amount of data into a database table with 28 columns and 68 rows of records. Each column has a size of 83 characters. This simulates a database exception caused by the high volume of data being inserted.\n"
}
{
    "start_time": "1697325123",
    "end_time": "1697325183",
    "start_timestamp": "2023-10-15 07:12:03",
    "end_timestamp": "2023-10-15 07:13:03",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 56\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 69\n    \n    # Number of rows to insert\n    num_rows = 231\n    \n    # Size of each column (in characters)\n    column_size = 62\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for a medical research organization, 56 researchers simultaneously attempt to update a database table containing 69 columns and 231 rows of patient records. Each column has a size of 62 characters. There is a high level of contention between the researchers, causing the database table to be frequently locked and resulting in a slowdown in the update process.\n"
}
{
    "start_time": "1697325243",
    "end_time": "1697325353",
    "start_timestamp": "2023-10-15 07:14:03",
    "end_timestamp": "2023-10-15 07:15:53",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 61\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2097306\n    \n    # Size of each column (in characters)\n    column_size = 83\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online retail database with 10 columns and 2,097,306 rows of product records, each column having a size of 83 characters, a large number of users (61 in this case) perform searches on the database after a vacuum operation, which is a process of reclaiming storage space and optimizing performance. This simulates the scenario where multiple users search for products simultaneously, after a cleanup operation, leading to potential database exceptions.\n"
}
{
    "start_time": "1697325413",
    "end_time": "1697325528",
    "start_timestamp": "2023-10-15 07:16:53",
    "end_timestamp": "2023-10-15 07:18:48",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 71\n    \n    # Number of rows to insert\n    num_rows = 819314\n    \n    # Size of each column (in characters)\n    column_size = 75\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace database, there are 10 users querying a table with 71 columns and 819,314 rows, where each column has a size of 75 characters. Initially, there are redundant indexes created for columns such as product name, category, and price range. These indexes are later removed after the query operation. This simulates the impact of additional storage space and performance overhead caused by redundant indexes in a real-life scenario.\n"
}
{
    "start_time": "1697325588",
    "end_time": "1697325679",
    "start_timestamp": "2023-10-15 07:19:48",
    "end_timestamp": "2023-10-15 07:21:19",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a cloud storage service, multiple users are simultaneously uploading, downloading, or editing large files, causing contention in the I/O operations. This results in slower file transfer speeds and potential performance issues.\n"
}
{
    "start_time": "1697325739",
    "end_time": "1697325800",
    "start_timestamp": "2023-10-15 07:22:19",
    "end_timestamp": "2023-10-15 07:23:20",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a software development company, multiple developers are performing a join operation on two large database tables to merge data. This join operation is causing poor performance due to the lack of optimization, which is putting a heavy load on the CPU and causing contention among the developers.\n"
}
{
    "start_time": "1697325860",
    "end_time": "1697326000",
    "start_timestamp": "2023-10-15 07:24:20",
    "end_timestamp": "2023-10-15 07:26:40",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online marketplace, when retrieving data for a large number of products and executing related subqueries to find inventory information, the performance of the query may suffer if the subqueries are not optimized.\n"
}
{
    "start_time": "1697326060",
    "end_time": "1697326133",
    "start_timestamp": "2023-10-15 07:27:40",
    "end_timestamp": "2023-10-15 07:28:53",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 179\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 89\n    \n    # Size of each column (in characters)\n    column_size = 43\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, if there are 179 devices trying to send a large amount of data to the central database simultaneously, each device having 10 different types of data to be stored in a table with 43 characters per column, and a total of 89 records, this process may cause a database exception due to the high data load.\n"
}
{
    "start_time": "1697326193",
    "end_time": "1697326265",
    "start_timestamp": "2023-10-15 07:29:53",
    "end_timestamp": "2023-10-15 07:31:05",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 179\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 39\n    \n    # Number of rows to insert\n    num_rows = 70\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an internet of things (IoT) application, a large amount of data generated by 179 sensors needs to be inserted into the database simultaneously. The database table contains 39 columns, each with a column size of 52 characters, and a total of 70 rows of data. This is simulated to trigger a database exception.\n"
}
{
    "start_time": "1697326325",
    "end_time": "1697326385",
    "start_timestamp": "2023-10-15 07:32:05",
    "end_timestamp": "2023-10-15 07:33:05",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 76\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 99\n    \n    # Number of rows to insert\n    num_rows = 213\n    \n    # Size of each column (in characters)\n    column_size = 84\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the scenario of a database for an e-commerce platform, 76 users try to perform simultaneous update operations on a database table containing 99 columns and 213 rows of records, with each column having a size of 84 characters. These users compete with each other to lock the database table, causing a database exception.\n"
}
{
    "start_time": "1697326445",
    "end_time": "1697326506",
    "start_timestamp": "2023-10-15 07:34:05",
    "end_timestamp": "2023-10-15 07:35:06",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 184\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2735363\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for an online store, when 184 users simultaneously perform search operations after a large-scale data cleaning operation on a table with 10 columns, 2,735,363 rows, and each column containing 52 characters of product records, an exception may occur due to the increased workload and lack of optimization.\n"
}
{
    "start_time": "1697326566",
    "end_time": "1697326680",
    "start_timestamp": "2023-10-15 07:36:06",
    "end_timestamp": "2023-10-15 07:38:00",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 7\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 74\n    \n    # Number of rows to insert\n    num_rows = 890431\n    \n    # Size of each column (in characters)\n    column_size = 55\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an inventory management system for a retail store, 7 users simultaneously perform a search operation on a database table containing 74 columns and 890,431 rows of product records. Each column has a size of 55 characters. However, a large number of unnecessary indexes are created for attributes like product name, category, and price range in the beginning of the query, which introduces additional storage overhead and slows down the search operation.\n"
}
{
    "start_time": "1697326740",
    "end_time": "1697326831",
    "start_timestamp": "2023-10-15 07:39:00",
    "end_timestamp": "2023-10-15 07:40:31",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a collaborative document editing system, multiple users are uploading, downloading, or editing documents simultaneously. The system experiences contention with input/output (I/O) operations, leading to slower file transfers.\n"
}
{
    "start_time": "1697326891",
    "end_time": "1697326951",
    "start_timestamp": "2023-10-15 07:41:31",
    "end_timestamp": "2023-10-15 07:42:31",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis scenario, multiple users are performing join operations on large datasets using a Python script. The join operations are not optimized, leading to poor performance. Additionally, these operations are causing CPU contention, resulting in slower processing times and resource conflicts.\n"
}
{
    "start_time": "1697327011",
    "end_time": "1697327158",
    "start_timestamp": "2023-10-15 07:43:31",
    "end_timestamp": "2023-10-15 07:45:58",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, there is a query to fetch a large amount of data involving correlated subqueries. This query retrieves the inventory of each product, but if the subqueries are not optimized, the performance of the query may deteriorate.\n"
}
{
    "start_time": "1697327219",
    "end_time": "1697327291",
    "start_timestamp": "2023-10-15 07:46:59",
    "end_timestamp": "2023-10-15 07:48:11",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 184\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 77\n    \n    # Size of each column (in characters)\n    column_size = 27\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application where 184 sensors generate a large amount of data, there is a need to insert this data into the database. The database table has 5 columns, each with a size of 27 characters, and contains 77 rows. This process simulates a database exception caused by simultaneously inserting a large amount of data.\n"
}
{
    "start_time": "1697327351",
    "end_time": "1697327423",
    "start_timestamp": "2023-10-15 07:49:11",
    "end_timestamp": "2023-10-15 07:50:23",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 184\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 29\n    \n    # Number of rows to insert\n    num_rows = 77\n    \n    # Size of each column (in characters)\n    column_size = 89\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a scientific research project, there is a need to insert a large amount of data generated by 184 experiments simultaneously. This data includes 29 different parameters, each with a maximum size of 89 characters. The data is being inserted into a table that has 77 rows. The purpose of this script is to simulate the database exception that may occur during the insertion process.\n"
}
{
    "start_time": "1697327483",
    "end_time": "1697327544",
    "start_timestamp": "2023-10-15 07:51:23",
    "end_timestamp": "2023-10-15 07:52:24",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 181\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 92\n    \n    # Number of rows to insert\n    num_rows = 260\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 181 users simultaneously attempt to perform frequent update operations in a database table containing 92 columns and 260 rows of product records, where each column has a size of 52 characters. The users compete with each other to lock the database table to perform the update operation, which leads to contention and potential database exceptions.\n"
}
{
    "start_time": "1697327604",
    "end_time": "1697327722",
    "start_timestamp": "2023-10-15 07:53:24",
    "end_timestamp": "2023-10-15 07:55:22",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 173\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 3788837\n    \n    # Size of each column (in characters)\n    column_size = 89\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a large-scale online platform, when 173 users simultaneously perform searches after a vacuum operation on a database table with 5 columns and 3,788,837 rows, each column's size being 89 characters, an exception in the database may occur due to the increased search load and the absence of necessary indexing.\n"
}
{
    "start_time": "1697327782",
    "end_time": "1697327897",
    "start_timestamp": "2023-10-15 07:56:22",
    "end_timestamp": "2023-10-15 07:58:17",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 79\n    \n    # Number of rows to insert\n    num_rows = 732240\n    \n    # Size of each column (in characters)\n    column_size = 81\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace database, there are six users performing a query operation on a database table containing 79 columns and 732,240 rows of product records, each with a column size of 81 characters. At the beginning of the query, a large number of indexes are created for items such as product name, category, and price range. However, after the query operation, these indexes are deleted. This simulation showcases the additional storage footprint and performance overhead caused by creating and deleting redundant indexes.\n"
}
{
    "start_time": "1697327957",
    "end_time": "1697328048",
    "start_timestamp": "2023-10-15 07:59:17",
    "end_timestamp": "2023-10-15 08:00:48",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are simultaneously uploading, downloading, or editing files, causing contention for input/output operations. As a result, the file transfer process becomes slower due to the competition for input/output resources.\n"
}
{
    "start_time": "1697328108",
    "end_time": "1697328168",
    "start_timestamp": "2023-10-15 08:01:48",
    "end_timestamp": "2023-10-15 08:02:48",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a financial reporting system, multiple users are executing join queries to retrieve data from large database tables. The join operations are not optimized, causing poor performance. Additionally, there is contention for CPU resources among the users, resulting in further degradation of query performance.\n"
}
{
    "start_time": "1697328228",
    "end_time": "1697328367",
    "start_timestamp": "2023-10-15 08:03:48",
    "end_timestamp": "2023-10-15 08:06:07",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online marketplace, when trying to fetch a large amount of data and execute related subqueries to determine the inventory for each product, the performance of the query may deteriorate if the subqueries are not optimized.\n"
}
{
    "start_time": "1697328428",
    "end_time": "1697328500",
    "start_timestamp": "2023-10-15 08:07:08",
    "end_timestamp": "2023-10-15 08:08:20",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 108\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 54\n    \n    # Size of each column (in characters)\n    column_size = 30\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, a large amount of data generated by 108 sensors needs to be inserted into the database simultaneously. This process can cause a database exception. The data is being inserted into a table with 18 columns and 54 rows, and each column can hold up to 30 characters.\n"
}
{
    "start_time": "1697328560",
    "end_time": "1697328631",
    "start_timestamp": "2023-10-15 08:09:20",
    "end_timestamp": "2023-10-15 08:10:31",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 108\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 37\n    \n    # Number of rows to insert\n    num_rows = 95\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, 108 sensors generate a large amount of data that needs to be inserted into a database simultaneously. This process of inserting data into a database with 37 columns, each with a size of 92 characters, and a total of 95 rows, causes a database exception.\n"
}
{
    "start_time": "1697328691",
    "end_time": "1697328752",
    "start_timestamp": "2023-10-15 08:11:31",
    "end_timestamp": "2023-10-15 08:12:32",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 140\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 82\n    \n    # Number of rows to insert\n    num_rows = 395\n    \n    # Size of each column (in characters)\n    column_size = 93\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 140 users are trying to perform simultaneous update operations on a database table containing 82 columns and 395 rows of product records, where each column has a size of 93 characters. These users are competing with each other to lock the database table, which may lead to a database exception.\n"
}
{
    "start_time": "1697328812",
    "end_time": "1697328864",
    "start_timestamp": "2023-10-15 08:13:32",
    "end_timestamp": "2023-10-15 08:14:24",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 117\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 2809399\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large online store, there are 117 simultaneous searches performed by users in the database table containing 18 columns and 2,809,399 rows of product records, each column size being 92 characters. This search is performed after a vacuum operation, which is a data cleaning process in the database.\n"
}
{
    "start_time": "1697328924",
    "end_time": "1697329039",
    "start_timestamp": "2023-10-15 08:15:24",
    "end_timestamp": "2023-10-15 08:17:19",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 72\n    \n    # Number of rows to insert\n    num_rows = 462113\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial database that stores transaction records, if multiple users simultaneously perform queries on a table containing 72 columns and 462,113 rows, each column having a size of 50 characters, and there are redundant indexes created initially for transaction details such as account number, transaction type, and date, it may result in additional storage overhead and decreased query performance.\n"
}
{
    "start_time": "1697329099",
    "end_time": "1697329190",
    "start_timestamp": "2023-10-15 08:18:19",
    "end_timestamp": "2023-10-15 08:19:50",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are trying to upload, download, or edit files at the same time. This creates competition for input/output resources, leading to slower file transfers.\n"
}
{
    "start_time": "1697329250",
    "end_time": "1697329310",
    "start_timestamp": "2023-10-15 08:20:50",
    "end_timestamp": "2023-10-15 08:21:50",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics company, multiple data analysts are executing complex queries that involve joining large tables. The queries are resource-intensive and cause contention for CPU resources, resulting in degraded performance.\n"
}
{
    "start_time": "1697329370",
    "end_time": "1697329518",
    "start_timestamp": "2023-10-15 08:22:50",
    "end_timestamp": "2023-10-15 08:25:18",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online shopping platform's database, retrieving a large amount of data and executing related subqueries to find the inventory for each product can lead to performance issues.\n"
}
{
    "start_time": "1697329579",
    "end_time": "1697329651",
    "start_timestamp": "2023-10-15 08:26:19",
    "end_timestamp": "2023-10-15 08:27:31",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 158\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 99\n    \n    # Size of each column (in characters)\n    column_size = 45\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, there is a need to insert a large amount of data into a database simultaneously. This specific scenario involves inserting data generated by 158 sources, with each source generating data for 20 columns and 99 rows. The size of each column is 45 characters. The purpose of this simulation is to identify any potential issues or exceptions that may arise due to the high volume of data being inserted.\n"
}
{
    "start_time": "1697329711",
    "end_time": "1697329783",
    "start_timestamp": "2023-10-15 08:28:31",
    "end_timestamp": "2023-10-15 08:29:43",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 158\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 34\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data logging system, there are 158 sensor devices generating a large amount of data. This data needs to be simultaneously inserted into a database table with 34 columns and 58 rows of records, where each column can hold up to 51 characters. This simulation triggers a database exception to simulate the performance impact caused by inserting a large volume of data.\n"
}
{
    "start_time": "1697329843",
    "end_time": "1697329903",
    "start_timestamp": "2023-10-15 08:30:43",
    "end_timestamp": "2023-10-15 08:31:43",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 62\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 92\n    \n    # Number of rows to insert\n    num_rows = 320\n    \n    # Size of each column (in characters)\n    column_size = 82\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, there is a situation where 62 users try to update the database simultaneously. The database table has 92 columns, with each column having a size of 82 characters, and there are 320 rows of data. Due to the high number of users competing for database table locks, a database exception is simulated.\n"
}
{
    "start_time": "1697329963",
    "end_time": "1697330005",
    "start_timestamp": "2023-10-15 08:32:43",
    "end_timestamp": "2023-10-15 08:33:25",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 101\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 3551817\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, there are 101 users searching for products using terms such as product name, category, and price range. These users are searching in a database table containing 20 columns and 3,551,817 rows of product records, each with a column size of 51 characters. This search operation occurs after a large-scale data cleaning operation on the database table. The aim is to simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697330065",
    "end_time": "1697330179",
    "start_timestamp": "2023-10-15 08:34:25",
    "end_timestamp": "2023-10-15 08:36:19",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 77\n    \n    # Number of rows to insert\n    num_rows = 698055\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database, when conducting queries with redundant indexes on a database table containing 77 columns, 698,055 rows, each column having a size of 78 characters, and with 5 users performing the queries simultaneously, there may be additional storage requirements and performance overhead due to the redundant indexes.\n"
}
{
    "start_time": "1697330239",
    "end_time": "1697330330",
    "start_timestamp": "2023-10-15 08:37:19",
    "end_timestamp": "2023-10-15 08:38:50",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, there is a high volume of data being uploaded, downloaded, or edited simultaneously. This causes contention for input/output (I/O) resources, resulting in slower file transfers.\n"
}
{
    "start_time": "1697330390",
    "end_time": "1697330451",
    "start_timestamp": "2023-10-15 08:39:50",
    "end_timestamp": "2023-10-15 08:40:51",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database for a logistics management system, there is a poor join performance issue when retrieving data related to shipments and routes. Additionally, CPU contention occurs when multiple users simultaneously perform calculations and data processing tasks, causing the system to slow down.\n"
}
{
    "start_time": "1697330511",
    "end_time": "1697330659",
    "start_timestamp": "2023-10-15 08:41:51",
    "end_timestamp": "2023-10-15 08:44:19",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In the database of an e-commerce platform, there is a scenario where a large amount of data needs to be fetched, and the query involves executing correlated subqueries. This can lead to a performance degradation when retrieving inventory information for each product if the correlated subqueries are not optimized.\n"
}
{
    "start_time": "1697330719",
    "end_time": "1697330791",
    "start_timestamp": "2023-10-15 08:45:19",
    "end_timestamp": "2023-10-15 08:46:31",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 191\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 19\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 191 devices, such as sensors and actuators, simultaneously generate a large amount of data that needs to be inserted into the central database. Each device has 19 data points, with each data point containing 67 characters. The total number of data points is 58. However, as a result of this simultaneous data insertion process, the performance of the database may be affected and an exception may occur.\n"
}
{
    "start_time": "1697330852",
    "end_time": "1697330924",
    "start_timestamp": "2023-10-15 08:47:32",
    "end_timestamp": "2023-10-15 08:48:44",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 191\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 21\n    \n    # Number of rows to insert\n    num_rows = 99\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, a high number of sensors generate a large amount of data that needs to be inserted into the database simultaneously. This script simulates the scenario by inserting data from 191 sensors into a database table with 21 columns and 99 rows. Each column has a size of 92 characters. This may cause a database exception due to the simultaneous insertion of large amounts of data.\n"
}
{
    "start_time": "1697330984",
    "end_time": "1697331044",
    "start_timestamp": "2023-10-15 08:49:44",
    "end_timestamp": "2023-10-15 08:50:44",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 117\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 209\n    \n    # Size of each column (in characters)\n    column_size = 82\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online database system, 117 users simultaneously attempt to perform updates on a database table containing 95 columns and 209 rows of records. Each column has a size of 82 characters. During the update operations, the users compete with each other to lock the database table, causing contention and potentially resulting in a database exception.\n"
}
{
    "start_time": "1697331105",
    "end_time": "1697331145",
    "start_timestamp": "2023-10-15 08:51:45",
    "end_timestamp": "2023-10-15 08:52:25",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 199\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 19\n    \n    # Number of rows to insert\n    num_rows = 3090193\n    \n    # Size of each column (in characters)\n    column_size = 92\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, there is a need to perform a vacuum operation to reclaim space and optimize the performance of the database. This operation is triggered by simulating 199 users searching in a database table containing 19 columns, 3,090,193 rows, where each column has a size of 92 characters. The purpose is to mimic the potential exception that could occur when a large-scale data cleaning operation coincides with a search by multiple users.\n"
}
{
    "start_time": "1697331205",
    "end_time": "1697331319",
    "start_timestamp": "2023-10-15 08:53:25",
    "end_timestamp": "2023-10-15 08:55:19",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 68\n    \n    # Number of rows to insert\n    num_rows = 958228\n    \n    # Size of each column (in characters)\n    column_size = 75\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database with 68 columns and 958,228 rows, each with a column size of 75 characters, redundant indexes are created for various attributes such as product name, category, and price range before a query operation. Simulate the performance overhead and additional storage caused by these redundant indexes, with 5 users performing the query operation simultaneously.\n"
}
{
    "start_time": "1697331379",
    "end_time": "1697331470",
    "start_timestamp": "2023-10-15 08:56:19",
    "end_timestamp": "2023-10-15 08:57:50",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a collaborative document editing system, multiple users are simultaneously uploading, downloading, or editing documents. This creates a competition for input/output resources, leading to slower file transfer speeds.\n"
}
{
    "start_time": "1697331530",
    "end_time": "1697331590",
    "start_timestamp": "2023-10-15 08:58:50",
    "end_timestamp": "2023-10-15 08:59:50",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a multi-threaded system where data is accessed and manipulated simultaneously, there is a scenario where poor join performance and CPU contention can occur. This means that when performing database queries involving multiple tables and joins, the system experiences slow performance due to high CPU usage and contention among multiple threads competing for CPU resources.\n"
}
{
    "start_time": "1697331650",
    "end_time": "1697331798",
    "start_timestamp": "2023-10-15 09:00:50",
    "end_timestamp": "2023-10-15 09:03:18",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online marketplace, the system needs to retrieve a large amount of data about the inventory for each product. This involves executing related subqueries to gather the necessary information. However, if these subqueries are not optimized, the performance of the system may be negatively affected, resulting in slower retrieval of inventory data.\n"
}
{
    "start_time": "1697331859",
    "end_time": "1697331930",
    "start_timestamp": "2023-10-15 09:04:19",
    "end_timestamp": "2023-10-15 09:05:30",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 84\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 80\n    \n    # Size of each column (in characters)\n    column_size = 41\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) application, 84 sensors are generating a large amount of data that needs to be inserted into the database simultaneously. Each data entry consists of 12 columns, where each column can store up to 41 characters. The dataset contains 80 rows of sensor data. This simulates a scenario where the database experiences an exception due to the high volume of data being inserted at the same time.\n"
}
{
    "start_time": "1697331990",
    "end_time": "1697332062",
    "start_timestamp": "2023-10-15 09:06:30",
    "end_timestamp": "2023-10-15 09:07:42",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 84\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 22\n    \n    # Number of rows to insert\n    num_rows = 70\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 84 IoT devices simultaneously generate a large amount of data that needs to be inserted into the database. Each IoT device has 22 sensors, and each sensor generates data with a size of 66 characters. The database table has a total of 70 rows. This process simulates the database exception caused by the simultaneous insertion of large data from multiple devices.\n"
}
{
    "start_time": "1697332122",
    "end_time": "1697332182",
    "start_timestamp": "2023-10-15 09:08:42",
    "end_timestamp": "2023-10-15 09:09:42",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 83\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 96\n    \n    # Number of rows to insert\n    num_rows = 274\n    \n    # Size of each column (in characters)\n    column_size = 85\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, a scenario is simulated where 83 users simultaneously perform frequent update operations in a database table containing 96 columns and 274 rows of product records. Each column has a size of 85 characters, and multiple users compete with each other to lock the database table during the update operation.\n"
}
{
    "start_time": "1697332242",
    "end_time": "1697332305",
    "start_timestamp": "2023-10-15 09:10:42",
    "end_timestamp": "2023-10-15 09:11:45",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 131\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 3107663\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an e-commerce platform's database, users frequently search for products based on various criteria such as product name, category, and price range. However, after performing a large-scale data cleaning operation on a database table with 12 columns and 3,107,663 rows, each column containing 66 characters of product records, an exception occurs when 131 users simultaneously perform a search. This simulates the database's response to a high volume of search queries after a data cleaning operation.\n"
}
{
    "start_time": "1697332365",
    "end_time": "1697332478",
    "start_timestamp": "2023-10-15 09:12:45",
    "end_timestamp": "2023-10-15 09:14:38",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 76\n    \n    # Number of rows to insert\n    num_rows = 600440\n    \n    # Size of each column (in characters)\n    column_size = 64\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database with 76 columns and 600,440 rows, each with a column size of 64 characters, a large number of indexes are created for items such as product name, category, and price range at the beginning of the query. These indexes are deleted after the query operation. Simulate the additional storage footprint and performance overhead caused by this process with 8 users.\n"
}
{
    "start_time": "1697332538",
    "end_time": "1697332629",
    "start_timestamp": "2023-10-15 09:15:38",
    "end_timestamp": "2023-10-15 09:17:09",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, there is a high volume of file uploads, downloads, and edits happening simultaneously. This creates a competition for input/output (I/O) resources, leading to slower file transfer speeds.\n"
}
{
    "start_time": "1697332689",
    "end_time": "1697332749",
    "start_timestamp": "2023-10-15 09:18:09",
    "end_timestamp": "2023-10-15 09:19:09",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system used for customer sales analysis, multiple users are attempting to perform join operations on large tables simultaneously. This process puts a heavy load on the CPU and leads to contention, resulting in poor performance of the join operations.\n"
}
{
    "start_time": "1697332809",
    "end_time": "1697332958",
    "start_timestamp": "2023-10-15 09:20:09",
    "end_timestamp": "2023-10-15 09:22:38",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail system, when trying to retrieve a large amount of data, such as the inventory for each product, there is a risk of performance degradation if the query uses inefficient correlated subqueries. This can slow down the system's response time and impact overall user experience.\n"
}
{
    "start_time": "1697333019",
    "end_time": "1697333091",
    "start_timestamp": "2023-10-15 09:23:39",
    "end_timestamp": "2023-10-15 09:24:51",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 166\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 75\n    \n    # Size of each column (in characters)\n    column_size = 27\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home automation system, 166 sensors are sending a large amount of data simultaneously, which needs to be inserted into the database. Each data point has 5 attributes with a size of 27 characters, and there are a total of 75 data points. This process simulates a database exception due to the high volume of data being inserted.\n"
}
{
    "start_time": "1697333151",
    "end_time": "1697333223",
    "start_timestamp": "2023-10-15 09:25:51",
    "end_timestamp": "2023-10-15 09:27:03",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 166\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 32\n    \n    # Number of rows to insert\n    num_rows = 75\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, a large amount of data generated by 166 sensors needs to be inserted into the database simultaneously. This can simulate the database exception caused by the high volume and frequency of data insertion. The database table contains 32 columns and 75 rows, with each column having a size of 54 characters.\n"
}
{
    "start_time": "1697333283",
    "end_time": "1697333343",
    "start_timestamp": "2023-10-15 09:28:03",
    "end_timestamp": "2023-10-15 09:29:03",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 61\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 87\n    \n    # Number of rows to insert\n    num_rows = 284\n    \n    # Size of each column (in characters)\n    column_size = 85\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online marketplace, 61 users are simultaneously attempting to perform frequent update operations on a database table that contains 87 columns and 284 rows of product records. Each column has a size of 85 characters. These users are competing with each other to lock the database table in order to complete their update operations. As a result, the database system may encounter contention issues, resulting in slower performance and potential exceptions.\n"
}
{
    "start_time": "1697333403",
    "end_time": "1697333507",
    "start_timestamp": "2023-10-15 09:30:03",
    "end_timestamp": "2023-10-15 09:31:47",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 137\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 3959949\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online marketplace, if 137 users simultaneously perform a search after a large-scale data cleaning operation on a database table containing 5 columns, 3,959,949 rows, with each column having a size of 54 characters for product records, an exception will occur in the database.\n"
}
{
    "start_time": "1697333567",
    "end_time": "1697333681",
    "start_timestamp": "2023-10-15 09:32:47",
    "end_timestamp": "2023-10-15 09:34:41",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 53\n    \n    # Number of rows to insert\n    num_rows = 544087\n    \n    # Size of each column (in characters)\n    column_size = 61\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial institution's database with 53 columns and 544,087 rows, each with a column size of 61 characters, a large number of redundant indexes are created for different financial transaction attributes such as date, transaction type, and amount. These redundant indexes lead to additional storage consumption and performance overhead.\n"
}
{
    "start_time": "1697333741",
    "end_time": "1697333832",
    "start_timestamp": "2023-10-15 09:35:41",
    "end_timestamp": "2023-10-15 09:37:12",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a cloud storage system, multiple users are uploading, downloading, or editing files at the same time. Due to high I/O competition, the file transfer speed is significantly reduced.\n"
}
{
    "start_time": "1697333892",
    "end_time": "1697333952",
    "start_timestamp": "2023-10-15 09:38:12",
    "end_timestamp": "2023-10-15 09:39:12",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis system, multiple data tables are joined together based on a specific condition. However, the join operation is not optimized, and the system faces high CPU contention as multiple processes compete for computing resources. This leads to poor performance in executing the join operation.\n"
}
{
    "start_time": "1697334012",
    "end_time": "1697334151",
    "start_timestamp": "2023-10-15 09:40:12",
    "end_timestamp": "2023-10-15 09:42:31",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a retail platform's database, retrieving a large amount of data related to product inventory may involve executing multiple correlated subqueries. If these subqueries are not optimized efficiently, the performance of retrieving inventory information for each product may be negatively impacted.\n"
}
{
    "start_time": "1697334212",
    "end_time": "1697334283",
    "start_timestamp": "2023-10-15 09:43:32",
    "end_timestamp": "2023-10-15 09:44:43",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 62\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 6\n    \n    # Number of rows to insert\n    num_rows = 71\n    \n    # Size of each column (in characters)\n    column_size = 24\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, multiple sensors are collecting data and need to simultaneously insert a large amount of data into the database. This script simulates the database exception that can occur when 62 sensors attempt to insert data into a table containing 6 columns, 71 rows of data, with each column size being 24 characters.\n"
}
{
    "start_time": "1697334343",
    "end_time": "1697334414",
    "start_timestamp": "2023-10-15 09:45:43",
    "end_timestamp": "2023-10-15 09:46:54",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 62\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 22\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, 62 sensors generate a large amount of data to be inserted into a database simultaneously. The database table contains 22 columns and 58 rows, with each column having a size of 52 characters. This process simulates a database exception caused by the insertion of large data.\n"
}
{
    "start_time": "1697334474",
    "end_time": "1697334535",
    "start_timestamp": "2023-10-15 09:47:54",
    "end_timestamp": "2023-10-15 09:48:55",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 108\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 55\n    \n    # Number of rows to insert\n    num_rows = 400\n    \n    # Size of each column (in characters)\n    column_size = 93\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 108 users simultaneously attempt to perform frequent update operations in a database table containing 55 columns and 400 rows of product records, each with a column size of 93 characters. Multiple users compete with each other to lock the database table to perform the update operation.\n"
}
{
    "start_time": "1697334595",
    "end_time": "1697334640",
    "start_timestamp": "2023-10-15 09:49:55",
    "end_timestamp": "2023-10-15 09:50:40",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 181\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 6\n    \n    # Number of rows to insert\n    num_rows = 2583250\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online marketplace, there is a need to optimize the performance of search queries. In this case, there are 181 users simultaneously searching for products in a table with 6 columns and 2,583,250 rows of records, with each column containing 52 characters. By running the script provided, a simulation is performed to trigger an exception in the database due to the lack of proper indexing during the search process.\n"
}
{
    "start_time": "1697334700",
    "end_time": "1697334814",
    "start_timestamp": "2023-10-15 09:51:40",
    "end_timestamp": "2023-10-15 09:53:34",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 79\n    \n    # Number of rows to insert\n    num_rows = 811500\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a large-scale database of an e-commerce platform, 8 users query a table containing 79 columns and 811,500 rows of product records. Each column has a size of 78 characters. The script simulates the creation and deletion of redundant indexes for attributes such as product name, category, and price range, resulting in additional storage overhead and performance degradation.\n"
}
{
    "start_time": "1697334875",
    "end_time": "1697334965",
    "start_timestamp": "2023-10-15 09:54:35",
    "end_timestamp": "2023-10-15 09:56:05",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users share files, there is a concurrent uploading, downloading, or editing of files. This leads to I/O contention, which causes a slowdown in the file transfer process.\n"
}
{
    "start_time": "1697335025",
    "end_time": "1697335086",
    "start_timestamp": "2023-10-15 09:57:05",
    "end_timestamp": "2023-10-15 09:58:06",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis system, multiple users are executing join queries simultaneously on a database table with poor join performance. The system experiences contention for CPU resources, which slows down the execution of these queries.\n"
}
{
    "start_time": "1697335146",
    "end_time": "1697335295",
    "start_timestamp": "2023-10-15 09:59:06",
    "end_timestamp": "2023-10-15 10:01:35",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, when trying to fetch a large amount of data, specifically the inventory for each product, the process may involve executing related subqueries. If these subqueries are not optimized, the performance of querying inventory may deteriorate.\n"
}
{
    "start_time": "1697335355",
    "end_time": "1697335426",
    "start_timestamp": "2023-10-15 10:02:35",
    "end_timestamp": "2023-10-15 10:03:46",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 84\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 64\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data logging system, 84 sensors are simultaneously generating a large amount of data. This data needs to be inserted into a database table with 10 columns and 64 rows. Each column has a size of 51 characters. The goal is to simulate the database exception caused by this insert operation.\n"
}
{
    "start_time": "1697335487",
    "end_time": "1697335558",
    "start_timestamp": "2023-10-15 10:04:47",
    "end_timestamp": "2023-10-15 10:05:58",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 84\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 39\n    \n    # Number of rows to insert\n    num_rows = 60\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an industrial monitoring system, 84 sensors are generating a large amount of data simultaneously. This data needs to be inserted into a database table containing 39 columns and 60 rows. Each column can hold up to 66 characters. By simulating this scenario, we can trigger a database exception caused by the insertion of large data.\n"
}
{
    "start_time": "1697335618",
    "end_time": "1697335678",
    "start_timestamp": "2023-10-15 10:06:58",
    "end_timestamp": "2023-10-15 10:07:58",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 79\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 93\n    \n    # Number of rows to insert\n    num_rows = 384\n    \n    # Size of each column (in characters)\n    column_size = 62\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online store platform, there are 79 users concurrently attempting to perform frequent update operations on a database table. The table contains 93 columns and 384 rows of product records, with each column having a size of 62 characters. These users are competing for locks on the database table, causing contention and potential exceptions in the process.\n"
}
{
    "start_time": "1697335738",
    "end_time": "1697335852",
    "start_timestamp": "2023-10-15 10:08:58",
    "end_timestamp": "2023-10-15 10:10:52",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 140\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2402256\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online store database, if 140 users simultaneously perform a search after a large-scale data cleaning operation on a database table containing 10 columns, 2,402,256 rows, and each column size of 66 characters for product records, an exception is caused.\n"
}
{
    "start_time": "1697335912",
    "end_time": "1697336027",
    "start_timestamp": "2023-10-15 10:11:52",
    "end_timestamp": "2023-10-15 10:13:47",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 86\n    \n    # Number of rows to insert\n    num_rows = 595153\n    \n    # Size of each column (in characters)\n    column_size = 57\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used for an e-commerce website, 9 users perform a query operation on a table containing 86 columns and 595,153 rows, with each column containing data of size 57 characters. However, before the query operation, a large number of indexes are created for attributes like product name, category, and price range. This can lead to additional storage requirements and performance overhead.\n"
}
{
    "start_time": "1697336087",
    "end_time": "1697336178",
    "start_timestamp": "2023-10-15 10:14:47",
    "end_timestamp": "2023-10-15 10:16:18",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are simultaneously uploading, downloading, or editing files, there is a competition for input/output resources. This competition causes a slowdown in file transfer operations.\n"
}
{
    "start_time": "1697336238",
    "end_time": "1697336299",
    "start_timestamp": "2023-10-15 10:17:18",
    "end_timestamp": "2023-10-15 10:18:19",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database management system, when performing a join operation on multiple tables to retrieve data, if the tables are large and the join operation is not optimized, it can result in poor performance. Additionally, if there is contention for CPU resources due to multiple processes running simultaneously, it can further impact the join performance.\n"
}
{
    "start_time": "1697336359",
    "end_time": "1697336499",
    "start_timestamp": "2023-10-15 10:19:19",
    "end_timestamp": "2023-10-15 10:21:39",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online store's back-end system, the script will simulate a scenario where the performance of querying inventory for a large number of products is affected due to the execution of related subqueries. This can happen when trying to retrieve information about the available quantity of each product, and the system is not optimized to handle such queries efficiently.\n"
}
{
    "start_time": "1697336560",
    "end_time": "1697336632",
    "start_timestamp": "2023-10-15 10:22:40",
    "end_timestamp": "2023-10-15 10:23:52",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 162\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 13\n    \n    # Number of rows to insert\n    num_rows = 62\n    \n    # Size of each column (in characters)\n    column_size = 79\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data streaming system, 162 data sources are simultaneously inserting a large amount of data into a database table with 13 columns. Each column can store up to 79 characters, and the table has a total of 62 rows. This process is causing a database exception due to the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697336692",
    "end_time": "1697336764",
    "start_timestamp": "2023-10-15 10:24:52",
    "end_timestamp": "2023-10-15 10:26:04",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 162\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 34\n    \n    # Number of rows to insert\n    num_rows = 67\n    \n    # Size of each column (in characters)\n    column_size = 74\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analytics platform, there is a scenario where 162 data sources simultaneously generate a large amount of data. Each data source has 34 columns with a column size of 74 characters, and there are a total of 67 rows. The data needs to be inserted into the database, but the high volume of data and simultaneous insertion can cause exceptions or performance issues. The script simulates this scenario to test the system's resilience.\n"
}
{
    "start_time": "1697336824",
    "end_time": "1697336884",
    "start_timestamp": "2023-10-15 10:27:04",
    "end_timestamp": "2023-10-15 10:28:04",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 186\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 70\n    \n    # Number of rows to insert\n    num_rows = 394\n    \n    # Size of each column (in characters)\n    column_size = 65\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, there is a situation where 186 users simultaneously attempt to perform frequent update operations on a database table containing 70 columns and 394 rows of product records, with each column having a size of 65 characters. These users compete with each other to lock the database table and perform the update operations.\n"
}
{
    "start_time": "1697336944",
    "end_time": "1697336985",
    "start_timestamp": "2023-10-15 10:29:04",
    "end_timestamp": "2023-10-15 10:29:45",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 110\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 13\n    \n    # Number of rows to insert\n    num_rows = 3469806\n    \n    # Size of each column (in characters)\n    column_size = 74\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the inventory management system of a large retail store, 110 employees simultaneously perform a product search after a data cleaning operation on a database table containing 13 columns, 3,469,806 rows, with each column having a size of 74 characters. This may lead to a database exception due to the increased workload on the database server.\n"
}
{
    "start_time": "1697337045",
    "end_time": "1697337161",
    "start_timestamp": "2023-10-15 10:30:45",
    "end_timestamp": "2023-10-15 10:32:41",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 751014\n    \n    # Size of each column (in characters)\n    column_size = 91\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial banking database with 95 columns and 751,014 rows, each with a column size of 91 characters, multiple users try to create redundant indexes for different attributes like customer name, account number, transaction amount, etc., causing additional storage consumption and performance overhead.\n"
}
{
    "start_time": "1697337221",
    "end_time": "1697337312",
    "start_timestamp": "2023-10-15 10:33:41",
    "end_timestamp": "2023-10-15 10:35:12",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are simultaneously uploading, downloading, or editing files. This leads to competition for input/output resources, causing slower file transfers.\n"
}
{
    "start_time": "1697337372",
    "end_time": "1697337432",
    "start_timestamp": "2023-10-15 10:36:12",
    "end_timestamp": "2023-10-15 10:37:12",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a business intelligence system, multiple users perform join operations on a large table using complex conditions. These join operations place a heavy load on the CPU, causing contention and reducing the performance of the system.\n"
}
{
    "start_time": "1697337492",
    "end_time": "1697337642",
    "start_timestamp": "2023-10-15 10:38:12",
    "end_timestamp": "2023-10-15 10:40:42",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, imagine an e-commerce platform where a manager wants to analyze the inventory for each product. However, the database is not optimized and the query to fetch this large amount of data requires executing correlated subqueries. This leads to a slowdown in performance, making it difficult for the manager to efficiently analyze the inventory.\n"
}
{
    "start_time": "1697337702",
    "end_time": "1697337774",
    "start_timestamp": "2023-10-15 10:41:42",
    "end_timestamp": "2023-10-15 10:42:54",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 149\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 78\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a manufacturing facility, there are 149 machines producing data that needs to be inserted into a database table. Each machine has 7 data points with a column size of 63 characters. The database is set up to handle 78 rows of data. Running this script will simulate the exception that occurs when trying to insert a large amount of data from multiple machines into the database.\n"
}
{
    "start_time": "1697337834",
    "end_time": "1697337906",
    "start_timestamp": "2023-10-15 10:43:54",
    "end_timestamp": "2023-10-15 10:45:06",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 149\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 36\n    \n    # Number of rows to insert\n    num_rows = 73\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, 149 sensors generate a large amount of data that needs to be inserted simultaneously into a database table with 36 columns and 73 rows. Each column has a size of 72 characters. This scenario simulates a database exception caused by the high volume of data insertion.\n"
}
{
    "start_time": "1697337966",
    "end_time": "1697338027",
    "start_timestamp": "2023-10-15 10:46:06",
    "end_timestamp": "2023-10-15 10:47:07",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 149\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 87\n    \n    # Number of rows to insert\n    num_rows = 327\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online platform, when 149 users simultaneously compete to perform frequent update operations in a database table containing 87 columns and 327 rows of data, with each column having a size of 54 characters, there is contention for locking the database table. This can lead to a database exception.\n"
}
{
    "start_time": "1697338087",
    "end_time": "1697338148",
    "start_timestamp": "2023-10-15 10:48:07",
    "end_timestamp": "2023-10-15 10:49:08",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 196\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 3888722\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a social media platform, there are 196 users searching for posts in a table that contains 7 columns, 3,888,722 rows, with each column size being 72 characters. However, after performing a vacuum operation on the database table, an exception occurs when these users simultaneously search for posts.\n"
}
{
    "start_time": "1697338208",
    "end_time": "1697338322",
    "start_timestamp": "2023-10-15 10:50:08",
    "end_timestamp": "2023-10-15 10:52:02",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 90\n    \n    # Number of rows to insert\n    num_rows = 692354\n    \n    # Size of each column (in characters)\n    column_size = 57\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used for tracking customer orders in a restaurant, a large number of indexes are created for various attributes such as order number, customer name, and order date. However, these indexes are redundant and unnecessary, resulting in additional storage consumption and decreased query performance. This scenario simulates the impact of creating redundant indexes on a database table with 10 concurrent users performing various operations. The table contains 90 columns and 692,354 rows, with each column having a size of 57 characters.\n"
}
{
    "start_time": "1697338382",
    "end_time": "1697338473",
    "start_timestamp": "2023-10-15 10:53:02",
    "end_timestamp": "2023-10-15 10:54:33",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users upload, download, or edit files simultaneously, the file system experiences contention in input/output operations, resulting in slower file transfers.\n"
}
{
    "start_time": "1697338533",
    "end_time": "1697338594",
    "start_timestamp": "2023-10-15 10:55:33",
    "end_timestamp": "2023-10-15 10:56:34",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics platform, multiple users are performing join operations on a large dataset using multiple CPU cores simultaneously. Due to CPU contention, the performance of join operations is compromised, resulting in slower execution times.\n"
}
{
    "start_time": "1697338654",
    "end_time": "1697338803",
    "start_timestamp": "2023-10-15 10:57:34",
    "end_timestamp": "2023-10-15 11:00:03",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a scenario where a company is managing its inventory in a database, there is a need to fetch large amounts of data related to the inventory, specifically the amount of inventory for each product. This requires the execution of correlated subqueries, which can become inefficient when dealing with a large number of products. The provided script is a simulation of this scenario, where the \"FETCH_LARGE_DATA,CORRELATED_SUBQUERY\" anomaly is triggered to replicate the performance degradation caused by inefficient subqueries.\n"
}
{
    "start_time": "1697338863",
    "end_time": "1697338935",
    "start_timestamp": "2023-10-15 11:01:03",
    "end_timestamp": "2023-10-15 11:02:15",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 101\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 17\n    \n    # Number of rows to insert\n    num_rows = 71\n    \n    # Size of each column (in characters)\n    column_size = 46\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, there are 101 sensors generating a large amount of data that needs to be inserted into the database simultaneously. Each sensor generates data with 17 columns, each column containing 46 characters. There are a total of 71 rows of data being inserted. This process simulates the database exception caused by the large influx of data.\n"
}
{
    "start_time": "1697338995",
    "end_time": "1697339066",
    "start_timestamp": "2023-10-15 11:03:15",
    "end_timestamp": "2023-10-15 11:04:26",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 101\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 37\n    \n    # Number of rows to insert\n    num_rows = 53\n    \n    # Size of each column (in characters)\n    column_size = 62\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, 101 sensors are generating a large amount of data that needs to be simultaneously inserted into a database. The database table has 37 columns and 53 rows, with each column being 62 characters in size. This task simulates the database exception caused by the high volume data insertion process.\n"
}
{
    "start_time": "1697339126",
    "end_time": "1697339187",
    "start_timestamp": "2023-10-15 11:05:26",
    "end_timestamp": "2023-10-15 11:06:27",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 73\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 82\n    \n    # Number of rows to insert\n    num_rows = 208\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a social media platform, 73 users simultaneously attempt to perform frequent update operations in a database table containing 82 columns and 208 rows of user records, each with a column size of 56 characters. These users compete with each other to lock the database table and perform the update operation. The purpose of this simulation is to observe if any exceptions occur due to lock contention in the database.\n"
}
{
    "start_time": "1697339247",
    "end_time": "1697339316",
    "start_timestamp": "2023-10-15 11:07:27",
    "end_timestamp": "2023-10-15 11:08:36",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 158\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 17\n    \n    # Number of rows to insert\n    num_rows = 2580822\n    \n    # Size of each column (in characters)\n    column_size = 62\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database system of an online store, there is a system maintenance operation called VACUUM, which is used to clean up and reclaim the fragmented space in the database table. In this scenario, 158 users simultaneously perform searches in a table containing 17 columns and 2,580,822 rows. Each column has a size of 62 characters. The purpose of this simulation is to trigger a database exception caused by the VACUUM operation and the concurrent search operations.\n"
}
{
    "start_time": "1697339376",
    "end_time": "1697339491",
    "start_timestamp": "2023-10-15 11:09:36",
    "end_timestamp": "2023-10-15 11:11:31",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 81\n    \n    # Number of rows to insert\n    num_rows = 710858\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database for an online marketplace, there are 5 users executing a query that involves a large database table with 81 columns and 710,858 rows. Each column has a size of 51 characters. However, there are redundant indexes created for certain attributes, such as product name, category, and price range. The query performance may be affected due to the additional storage space and processing overhead caused by these redundant indexes.\n"
}
{
    "start_time": "1697339551",
    "end_time": "1697339642",
    "start_timestamp": "2023-10-15 11:12:31",
    "end_timestamp": "2023-10-15 11:14:02",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are uploading, downloading, or editing files simultaneously, the system is experiencing a contention issue with input/output operations (IO). This contention slows down the file transfer process.\n"
}
{
    "start_time": "1697339702",
    "end_time": "1697339762",
    "start_timestamp": "2023-10-15 11:15:02",
    "end_timestamp": "2023-10-15 11:16:02",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system with poor join performance, there is a high contention for CPU resources. This leads to slow query execution and degraded performance when performing join operations.\n"
}
{
    "start_time": "1697339823",
    "end_time": "1697339971",
    "start_timestamp": "2023-10-15 11:17:03",
    "end_timestamp": "2023-10-15 11:19:31",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, a performance issue may arise when querying the inventory of a large number of products due to the execution of related subqueries. This can be simulated by running the command \"python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY\".\n"
}
{
    "start_time": "1697340032",
    "end_time": "1697340104",
    "start_timestamp": "2023-10-15 11:20:32",
    "end_timestamp": "2023-10-15 11:21:44",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 163\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 8\n    \n    # Number of rows to insert\n    num_rows = 59\n    \n    # Size of each column (in characters)\n    column_size = 48\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home automation system, 163 devices are simultaneously generating a large amount of data that needs to be inserted into the database. Each device has 8 data fields, with each field containing up to 48 characters. There are a total of 59 records being inserted into the database. This process aims to simulate the database exception caused by the insertion of such large data from multiple devices.\n"
}
{
    "start_time": "1697340164",
    "end_time": "1697340236",
    "start_timestamp": "2023-10-15 11:22:44",
    "end_timestamp": "2023-10-15 11:23:56",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 163\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 24\n    \n    # Number of rows to insert\n    num_rows = 59\n    \n    # Size of each column (in characters)\n    column_size = 74\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, a large amount of data generated by 163 sensors needs to be inserted into the smart home database simultaneously. Simulate the database exception caused by this process, where the smart home database table contains 24 columns, 59 rows, and each column has a size of 74 characters.\n"
}
{
    "start_time": "1697340296",
    "end_time": "1697340357",
    "start_timestamp": "2023-10-15 11:24:56",
    "end_timestamp": "2023-10-15 11:25:57",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 177\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 89\n    \n    # Number of rows to insert\n    num_rows = 292\n    \n    # Size of each column (in characters)\n    column_size = 57\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 177 users simultaneously attempt to perform a frequent update operation in a database table containing 89 columns and 292 rows of product records each with a column size of 57 characters. Multiple users compete with each other to lock the database table to perform the update operation, which may lead to a database exception.\n"
}
{
    "start_time": "1697340417",
    "end_time": "1697340531",
    "start_timestamp": "2023-10-15 11:26:57",
    "end_timestamp": "2023-10-15 11:28:51",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 129\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 8\n    \n    # Number of rows to insert\n    num_rows = 2785449\n    \n    # Size of each column (in characters)\n    column_size = 74\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online store's database, there is a need to perform a large-scale data cleaning operation after which 129 users simultaneously search in a database table containing 8 columns and 2,785,449 rows of records, with each column having a size of 74 characters. This is done to simulate the scenario where, after a data cleaning operation, a large number of users search for products using different criteria such as product name, category, and price range, causing potential performance issues and possible exceptions in the database.\n"
}
{
    "start_time": "1697340591",
    "end_time": "1697340706",
    "start_timestamp": "2023-10-15 11:29:51",
    "end_timestamp": "2023-10-15 11:31:46",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 7\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 58\n    \n    # Number of rows to insert\n    num_rows = 614396\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database with 58 columns and 614,396 rows, each with a column size of 63 characters, a large number of indexes are created for items such as product name, category, and price range at the beginning of the query. Simulate the additional storage footprint and performance overhead caused by this process, with 7 users executing the query simultaneously.\n"
}
{
    "start_time": "1697340766",
    "end_time": "1697340857",
    "start_timestamp": "2023-10-15 11:32:46",
    "end_timestamp": "2023-10-15 11:34:17",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are sharing files, there is a scenario where multiple users upload, download, or edit files simultaneously. This leads to I/O contention, which causes a slowdown in file transfer. This scenario is simulated using the command \"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION\".\n"
}
{
    "start_time": "1697340917",
    "end_time": "1697340977",
    "start_timestamp": "2023-10-15 11:35:17",
    "end_timestamp": "2023-10-15 11:36:17",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis scenario for a company, when joining large datasets with poor performance optimization, multiple CPU cores compete for computing resources, resulting in slower execution times for the join operation.\n"
}
{
    "start_time": "1697341037",
    "end_time": "1697341177",
    "start_timestamp": "2023-10-15 11:37:17",
    "end_timestamp": "2023-10-15 11:39:37",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, imagine an e-commerce platform that needs to track the inventory levels for each product. However, the process of retrieving this information requires executing related subqueries. This can be quite time-consuming, especially if there are a large number of products in the database. Without optimizing these related subqueries, the performance of the inventory query operation may significantly deteriorate.\n"
}
{
    "start_time": "1697341238",
    "end_time": "1697341309",
    "start_timestamp": "2023-10-15 11:40:38",
    "end_timestamp": "2023-10-15 11:41:49",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 144\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 90\n    \n    # Size of each column (in characters)\n    column_size = 26\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a system where 144 devices are simultaneously inserting a large amount of data, each device is inserting 90 rows with 10 columns, where each column has a size of 26 characters. This scenario simulates the database exception caused by the high data insertion rate.\n"
}
{
    "start_time": "1697341369",
    "end_time": "1697341441",
    "start_timestamp": "2023-10-15 11:42:49",
    "end_timestamp": "2023-10-15 11:44:01",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 144\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 39\n    \n    # Number of rows to insert\n    num_rows = 65\n    \n    # Size of each column (in characters)\n    column_size = 85\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial system, 144 financial transactions are being inserted into a database table simultaneously. Each transaction has 39 columns with a size of 85 characters, and there are a total of 65 transactions. This process is simulating the database exception that can occur when inserting a large amount of data.\n"
}
{
    "start_time": "1697341501",
    "end_time": "1697341562",
    "start_timestamp": "2023-10-15 11:45:01",
    "end_timestamp": "2023-10-15 11:46:02",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 156\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 78\n    \n    # Number of rows to insert\n    num_rows = 218\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, if 156 users simultaneously attempt to perform frequent update operations in a database table with 78 columns and 218 rows of records, each with a column size of 99 characters, and compete to lock the table during the update process, it could lead to a database exception due to lock contention.\n"
}
{
    "start_time": "1697341622",
    "end_time": "1697341685",
    "start_timestamp": "2023-10-15 11:47:02",
    "end_timestamp": "2023-10-15 11:48:05",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 198\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2366643\n    \n    # Size of each column (in characters)\n    column_size = 85\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online store database, there is a need to optimize the performance of search queries made by 198 users. The database table contains 10 columns and 2,366,643 rows, with each column having a size of 85 characters. By simulating a VACUUM operation, which involves the reorganization and optimization of the database storage, we can trigger a database exception to evaluate the impact on search performance.\n"
}
{
    "start_time": "1697341745",
    "end_time": "1697341861",
    "start_timestamp": "2023-10-15 11:49:05",
    "end_timestamp": "2023-10-15 11:51:01",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 75\n    \n    # Number of rows to insert\n    num_rows = 471392\n    \n    # Size of each column (in characters)\n    column_size = 96\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database, 10 users simultaneously perform a query operation on a database table with 75 columns and 471,392 rows of product records. Each column has a size of 96 characters. The query operation involves creating redundant indexes for items such as product name, category, and price range before the query and deleting them after the query. This simulates the additional storage footprint and performance overhead caused by creating and deleting redundant indexes.\n"
}
{
    "start_time": "1697341921",
    "end_time": "1697342012",
    "start_timestamp": "2023-10-15 11:52:01",
    "end_timestamp": "2023-10-15 11:53:32",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing platform where multiple users are sharing files, there is a situation where multiple users are uploading, downloading, or editing files at the same time. This results in competition for input/output (I/O) resources, which leads to a slowdown in the file transfer process. The given command is used to simulate and trigger this scenario.\n"
}
{
    "start_time": "1697342072",
    "end_time": "1697342132",
    "start_timestamp": "2023-10-15 11:54:32",
    "end_timestamp": "2023-10-15 11:55:32",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis system, there is a poor performance of data joining due to high CPU contention. This can occur when multiple processes or tasks compete for CPU resources, causing delays in executing join operations.\n"
}
{
    "start_time": "1697342192",
    "end_time": "1697342341",
    "start_timestamp": "2023-10-15 11:56:32",
    "end_timestamp": "2023-10-15 11:59:01",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, when trying to fetch a large amount of product data, related subqueries are used to gather additional information about each product. However, if these subqueries are not optimized, the performance of the overall data retrieval process may be negatively impacted.\n"
}
{
    "start_time": "1697342402",
    "end_time": "1697342474",
    "start_timestamp": "2023-10-15 12:00:02",
    "end_timestamp": "2023-10-15 12:01:14",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 174\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 78\n    \n    # Size of each column (in characters)\n    column_size = 73\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, such as a financial system, a large number of transactions need to be processed simultaneously. This results in the insertion of a large amount of data into the database. Specifically, 174 threads are used to insert data into a table with 5 columns, each with a size of 73 characters, containing 78 rows. The purpose of this script is to simulate the database exception that may occur during this process.\n"
}
{
    "start_time": "1697342534",
    "end_time": "1697342606",
    "start_timestamp": "2023-10-15 12:02:14",
    "end_timestamp": "2023-10-15 12:03:26",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 174\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 38\n    \n    # Number of rows to insert\n    num_rows = 88\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an industrial manufacturing process, data from 174 sensors needs to be inserted into a database. Each sensor generates data with 38 attributes, each with a size of 50 characters. This process is simulated by running the script, causing database exceptions due to the large amount of data being inserted.\n"
}
{
    "start_time": "1697342666",
    "end_time": "1697342726",
    "start_timestamp": "2023-10-15 12:04:26",
    "end_timestamp": "2023-10-15 12:05:26",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 122\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 56\n    \n    # Number of rows to insert\n    num_rows = 221\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online platform's database, 122 users simultaneously attempt to perform frequent update operations in a database table containing 56 columns and 221 rows of records for a duration of time, with each column having a size of 99 characters. These users compete with each other to lock the database table, resulting in a contention for resources and potential exceptions in the database.\n"
}
{
    "start_time": "1697342786",
    "end_time": "1697342875",
    "start_timestamp": "2023-10-15 12:06:26",
    "end_timestamp": "2023-10-15 12:07:55",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 61\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2784402\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a social media platform, if 61 users simultaneously perform a search after a data cleaning operation on a database table containing 5 columns, 2,784,402 rows, each column size of 50 characters, an exception might occur due to the high demand on the database resources.\n"
}
{
    "start_time": "1697342935",
    "end_time": "1697343049",
    "start_timestamp": "2023-10-15 12:08:55",
    "end_timestamp": "2023-10-15 12:10:49",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 75\n    \n    # Number of rows to insert\n    num_rows = 868525\n    \n    # Size of each column (in characters)\n    column_size = 88\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a financial database with 75 columns and 868,525 rows, each with a column size of 88 characters, a large number of indexes are created for different financial metrics at the beginning of a query. Nine users then perform various search operations on the database, and the indexes are deleted after the queries are finished. This simulates the additional storage and performance overhead caused by redundant indexes.\n"
}
{
    "start_time": "1697343109",
    "end_time": "1697343200",
    "start_timestamp": "2023-10-15 12:11:49",
    "end_timestamp": "2023-10-15 12:13:20",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a shared file storage system, multiple users are simultaneously uploading, downloading, or editing files. This leads to contention for input/output (I/O) resources, resulting in slower file transfer speeds.\n"
}
{
    "start_time": "1697343260",
    "end_time": "1697343320",
    "start_timestamp": "2023-10-15 12:14:20",
    "end_timestamp": "2023-10-15 12:15:20",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis scenario, multiple threads attempt to perform a join operation on a large dataset, causing poor performance due to high CPU contention.\n"
}
{
    "start_time": "1697343380",
    "end_time": "1697343520",
    "start_timestamp": "2023-10-15 12:16:20",
    "end_timestamp": "2023-10-15 12:18:40",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, there is a scenario where the system needs to fetch a large amount of data and execute correlated subqueries to retrieve inventory information for each product. However, if these subqueries are not optimized, the performance of the system can degrade, causing delays in retrieving inventory data.\n"
}
{
    "start_time": "1697343581",
    "end_time": "1697343653",
    "start_timestamp": "2023-10-15 12:19:41",
    "end_timestamp": "2023-10-15 12:20:53",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 182\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 11\n    \n    # Number of rows to insert\n    num_rows = 50\n    \n    # Size of each column (in characters)\n    column_size = 26\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analytics platform, 182 users are simultaneously inserting a large amount of data into a database table. The table has 11 columns, each with a size of 26 characters, and contains 50 rows of data. This process is designed to simulate the database exception that can occur when multiple users are trying to insert a large volume of data at the same time.\n"
}
{
    "start_time": "1697343713",
    "end_time": "1697343786",
    "start_timestamp": "2023-10-15 12:21:53",
    "end_timestamp": "2023-10-15 12:23:06",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 182\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 30\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 94\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analysis scenario, there are 182 data analysts working simultaneously, trying to insert a large amount of data into a database table. The table has 30 columns, with each column size being 94 characters, and there are 68 rows of data. This scenario simulates the database exception that can occur when multiple users try to insert a large amount of data into the database at the same time.\n"
}
{
    "start_time": "1697343846",
    "end_time": "1697343906",
    "start_timestamp": "2023-10-15 12:24:06",
    "end_timestamp": "2023-10-15 12:25:06",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 134\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 66\n    \n    # Number of rows to insert\n    num_rows = 204\n    \n    # Size of each column (in characters)\n    column_size = 61\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online database for a blog platform, 134 users are simultaneously attempting to perform frequent update operations on a database table that contains 66 columns and 204 rows of blog records. Each blog record has a column size of 61 characters. These users are competing with each other to lock the database table and perform the update operations. The duration of this competition is not specified in the command. This scenario simulates a database exception caused by contention for locking the table during the update operations.\n"
}
{
    "start_time": "1697343966",
    "end_time": "1697344004",
    "start_timestamp": "2023-10-15 12:26:06",
    "end_timestamp": "2023-10-15 12:26:44",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 180\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 11\n    \n    # Number of rows to insert\n    num_rows = 3176852\n    \n    # Size of each column (in characters)\n    column_size = 94\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a website that sells various products, 180 users simultaneously perform a search operation after a large-scale data cleaning operation on a database table containing 11 columns, 3,176,852 rows. Each column has a size of 94 characters.\n"
}
{
    "start_time": "1697344064",
    "end_time": "1697344178",
    "start_timestamp": "2023-10-15 12:27:44",
    "end_timestamp": "2023-10-15 12:29:38",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 82\n    \n    # Number of rows to insert\n    num_rows = 705049\n    \n    # Size of each column (in characters)\n    column_size = 85\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online store's database with 82 columns and 705,049 rows of product records, each with a column size of 85 characters, a large number of redundant indexes are created for various product attributes. This can lead to additional storage space usage and performance overhead.\n"
}
{
    "start_time": "1697344238",
    "end_time": "1697344329",
    "start_timestamp": "2023-10-15 12:30:38",
    "end_timestamp": "2023-10-15 12:32:09",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are uploading, downloading, or editing files simultaneously, the system is experiencing contention for input/output (I/O) resources. This contention slows down the file transfer process.\n"
}
{
    "start_time": "1697344389",
    "end_time": "1697344449",
    "start_timestamp": "2023-10-15 12:33:09",
    "end_timestamp": "2023-10-15 12:34:09",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics platform, several users are simultaneously executing join operations on large datasets. Due to inefficient join algorithms and limited CPU resources, the system experiences poor performance and high CPU contention.\n"
}
{
    "start_time": "1697344509",
    "end_time": "1697344658",
    "start_timestamp": "2023-10-15 12:35:09",
    "end_timestamp": "2023-10-15 12:37:38",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, this command is simulating a situation in an e-commerce platform's database where there is a need to retrieve a large amount of data regarding inventory for each product. This retrieval requires the execution of related subqueries. However, if these subqueries are not optimized, the performance of the inventory query may suffer, leading to slower execution times.\n"
}
{
    "start_time": "1697344719",
    "end_time": "1697344791",
    "start_timestamp": "2023-10-15 12:38:39",
    "end_timestamp": "2023-10-15 12:39:51",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 158\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 51\n    \n    # Size of each column (in characters)\n    column_size = 41\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data logging system, 158 sensors are generating a large amount of data simultaneously. This data needs to be inserted into the database, which has a table with 7 columns and 51 rows of data for each sensor. Each column has a size of 41 characters. Simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697344851",
    "end_time": "1697344923",
    "start_timestamp": "2023-10-15 12:40:51",
    "end_timestamp": "2023-10-15 12:42:03",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 158\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 28\n    \n    # Number of rows to insert\n    num_rows = 56\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data processing system used for scientific research, 158 sensors are simultaneously collecting data and inserting it into a database. Each sensor generates data with 28 attributes, and each attribute has a size of 58 characters. The database table has a total of 56 rows. Simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697344983",
    "end_time": "1697345043",
    "start_timestamp": "2023-10-15 12:43:03",
    "end_timestamp": "2023-10-15 12:44:03",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 131\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 99\n    \n    # Number of rows to insert\n    num_rows = 305\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a system where multiple users are accessing a database, 131 users are competing to update a table with 99 columns and 305 rows of records, each column having a size of 72 characters. These users are trying to lock the database table in order to perform the update operation simultaneously, which may result in contention and lead to database exceptions.\n"
}
{
    "start_time": "1697345103",
    "end_time": "1697345209",
    "start_timestamp": "2023-10-15 12:45:03",
    "end_timestamp": "2023-10-15 12:46:49",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 136\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 3133084\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, a large-scale data cleaning operation is performed on a database table containing 7 columns, 3,133,084 rows, each column size of 58 characters of commodity records. 136 users simultaneously perform a search operation after the data cleaning process, causing an exception in the database.\n"
}
{
    "start_time": "1697345270",
    "end_time": "1697345383",
    "start_timestamp": "2023-10-15 12:47:50",
    "end_timestamp": "2023-10-15 12:49:43",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 89\n    \n    # Number of rows to insert\n    num_rows = 650796\n    \n    # Size of each column (in characters)\n    column_size = 77\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database system of an online marketplace, there is a situation where redundant indexes are created for certain item attributes such as product name, category, and price range. This leads to additional storage usage and performance overhead. To simulate this scenario, 10 users are querying the database table, which contains 89 columns and 650,796 rows of product records. Each column has a size of 77 characters.\n"
}
{
    "start_time": "1697345444",
    "end_time": "1697345534",
    "start_timestamp": "2023-10-15 12:50:44",
    "end_timestamp": "2023-10-15 12:52:14",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a collaborative document editing platform, when multiple users try to upload, download or edit files simultaneously, the file system encounters a high competition for input/output resources. As a result, file transfer becomes slower and less efficient.\n"
}
{
    "start_time": "1697345594",
    "end_time": "1697345655",
    "start_timestamp": "2023-10-15 12:53:14",
    "end_timestamp": "2023-10-15 12:54:15",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a finance management system, multiple users are running a query that involves joining large tables together. However, the query is not optimized and the system is experiencing contention issues with CPU resources, leading to poor performance.\n"
}
{
    "start_time": "1697345715",
    "end_time": "1697345855",
    "start_timestamp": "2023-10-15 12:55:15",
    "end_timestamp": "2023-10-15 12:57:35",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a database of an e-commerce platform, fetching a large amount of data involves executing related subqueries to determine the inventory levels for each product. If these subqueries are not optimized, querying the inventory can become slow and inefficient.\n"
}
{
    "start_time": "1697345915",
    "end_time": "1697345987",
    "start_timestamp": "2023-10-15 12:58:35",
    "end_timestamp": "2023-10-15 12:59:47",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 97\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 6\n    \n    # Number of rows to insert\n    num_rows = 67\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, 97 sensors are generating a large amount of data, which needs to be inserted into a database. Each row in the database table has 6 columns, with each column having a size of 56 characters. There are 67 rows in total. This simulation aims to trigger a database exception caused by the simultaneous insertion of the large data set.\n"
}
{
    "start_time": "1697346047",
    "end_time": "1697346118",
    "start_timestamp": "2023-10-15 13:00:47",
    "end_timestamp": "2023-10-15 13:01:58",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 97\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an internet of things (IoT) application, with 97 sensors generating a large amount of data, there is an attempt to simultaneously insert this data into a database table. The table has 20 columns and 68 rows, with each column having a size of 58 characters. This process aims to simulate the database exception caused by inserting a large volume of data.\n"
}
{
    "start_time": "1697346178",
    "end_time": "1697346239",
    "start_timestamp": "2023-10-15 13:02:58",
    "end_timestamp": "2023-10-15 13:03:59",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 56\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 67\n    \n    # Number of rows to insert\n    num_rows = 282\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, there is a high contention for accessing and updating the records of 282 products in a table with 67 columns. Each column can store up to 99 characters. This contention is caused by 56 users simultaneously trying to perform update operations on the database.\n"
}
{
    "start_time": "1697346299",
    "end_time": "1697346402",
    "start_timestamp": "2023-10-15 13:04:59",
    "end_timestamp": "2023-10-15 13:06:42",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 163\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 6\n    \n    # Number of rows to insert\n    num_rows = 3104782\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online shopping platform, a large-scale data cleaning operation is performed on a table containing 6 columns and 3,104,782 rows of product records. The database is then subjected to a search operation by 163 users simultaneously. The search includes terms such as product name, category, and price range. However, due to the high number of concurrent searches and the previous data cleaning operation, an exception occurs in the database.\n"
}
{
    "start_time": "1697346462",
    "end_time": "1697346576",
    "start_timestamp": "2023-10-15 13:07:42",
    "end_timestamp": "2023-10-15 13:09:36",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 82\n    \n    # Number of rows to insert\n    num_rows = 583072\n    \n    # Size of each column (in characters)\n    column_size = 51\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database for an e-commerce website, 10 users are performing a query operation on a table containing 82 columns and 583,072 rows. Each column has a size of 51 characters. However, the query involves creating redundant indexes for items such as product name, category, and price range, which can lead to additional storage footprint and performance overhead.\n"
}
{
    "start_time": "1697346636",
    "end_time": "1697346727",
    "start_timestamp": "2023-10-15 13:10:36",
    "end_timestamp": "2023-10-15 13:12:07",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users are simultaneously uploading, downloading, or editing files, the system is experiencing I/O contention. This means that the file transfer speed is being slowed down due to the competition for input/output resources.\n"
}
{
    "start_time": "1697346787",
    "end_time": "1697346847",
    "start_timestamp": "2023-10-15 13:13:07",
    "end_timestamp": "2023-10-15 13:14:07",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis scenario, multiple users are performing join operations on a dataset in a database that is experiencing high CPU contention. This leads to poor performance of the join operations, causing delays in data analysis tasks.\n"
}
{
    "start_time": "1697346907",
    "end_time": "1697347047",
    "start_timestamp": "2023-10-15 13:15:07",
    "end_timestamp": "2023-10-15 13:17:27",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a database for an e-commerce platform, there is a query that fetches a large amount of data and involves correlated subqueries. This query is used to retrieve the inventory quantity for each product. However, without optimizing the correlated subqueries, the performance of this query may degrade when there is a large number of products in the database.\n"
}
{
    "start_time": "1697347108",
    "end_time": "1697347179",
    "start_timestamp": "2023-10-15 13:18:28",
    "end_timestamp": "2023-10-15 13:19:39",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 107\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 60\n    \n    # Size of each column (in characters)\n    column_size = 33\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) application, there is a need to insert a large amount of data from 107 sensors into a database. Each row of data contains 18 columns, with each column having a size of 33 characters. There are a total of 60 rows to be inserted. This simulation aims to test the performance and resilience of the database system when faced with such a large-scale data insertion process.\n"
}
{
    "start_time": "1697347239",
    "end_time": "1697347311",
    "start_timestamp": "2023-10-15 13:20:39",
    "end_timestamp": "2023-10-15 13:21:51",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 107\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 39\n    \n    # Number of rows to insert\n    num_rows = 69\n    \n    # Size of each column (in characters)\n    column_size = 93\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, where various IoT devices are interconnected, there are 107 devices that generate a large amount of sensor data simultaneously. This data needs to be inserted into the database table, which contains 39 columns with a column size of 93 characters, and there are 69 rows in total. However, the database may experience exceptions due to the high volume of data being inserted at the same time.\n"
}
{
    "start_time": "1697347371",
    "end_time": "1697347431",
    "start_timestamp": "2023-10-15 13:22:51",
    "end_timestamp": "2023-10-15 13:23:51",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 195\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 62\n    \n    # Number of rows to insert\n    num_rows = 300\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for an online platform, there is contention for locking the database table when 195 users simultaneously try to perform update operations on a table containing 62 columns and 300 rows of records with a column size of 52 characters. This causes a database exception due to the competition for lock acquisition.\n"
}
{
    "start_time": "1697347491",
    "end_time": "1697347542",
    "start_timestamp": "2023-10-15 13:24:51",
    "end_timestamp": "2023-10-15 13:25:42",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 138\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 3056064\n    \n    # Size of each column (in characters)\n    column_size = 93\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, if there are 138 users searching in the database table containing 18 columns, 3,056,064 rows, each column size of 93 characters, and a vacuum operation is performed on the database table, resulting in an exception due to the search queries being executed while the vacuum operation is in progress.\n"
}
{
    "start_time": "1697347602",
    "end_time": "1697347717",
    "start_timestamp": "2023-10-15 13:26:42",
    "end_timestamp": "2023-10-15 13:28:37",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 77\n    \n    # Number of rows to insert\n    num_rows = 660192\n    \n    # Size of each column (in characters)\n    column_size = 62\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database with 77 columns and 660,192 rows, each with a column size of 62 characters, unnecessary indexes are created and then deleted after 6 users perform queries. This process simulates the additional storage and performance overhead caused by the creation and deletion of these indexes.\n"
}
{
    "start_time": "1697347777",
    "end_time": "1697347868",
    "start_timestamp": "2023-10-15 13:29:37",
    "end_timestamp": "2023-10-15 13:31:08",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a busy file sharing system, multiple users are uploading, downloading, or editing files concurrently. The system is experiencing contention issues with the input/output (I/O) operations, resulting in slower file transfers.\n"
}
{
    "start_time": "1697347928",
    "end_time": "1697347988",
    "start_timestamp": "2023-10-15 13:32:08",
    "end_timestamp": "2023-10-15 13:33:08",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis platform, multiple users are performing join operations on a large dataset using 100% of the available CPU resources. This intense CPU competition is causing poor performance in the join operations, resulting in slow data processing.\n"
}
{
    "start_time": "1697348048",
    "end_time": "1697348198",
    "start_timestamp": "2023-10-15 13:34:08",
    "end_timestamp": "2023-10-15 13:36:38",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce website, when querying the inventory for each product, the script will simulate a scenario where a large amount of data needs to be fetched and related subqueries are involved. If the subqueries are not optimized, the performance of the inventory query may be negatively impacted.\n"
}
{
    "start_time": "1697348259",
    "end_time": "1697348330",
    "start_timestamp": "2023-10-15 13:37:39",
    "end_timestamp": "2023-10-15 13:38:50",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 137\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 80\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a real-life scenario of an Internet of Things (IoT) application, 137 sensors are generating a large amount of data simultaneously. This data needs to be inserted into a database table that has 5 columns and 80 rows, with each column having a size of 67 characters. The purpose of running this script is to simulate a database exception that may occur due to the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697348390",
    "end_time": "1697348462",
    "start_timestamp": "2023-10-15 13:39:50",
    "end_timestamp": "2023-10-15 13:41:02",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 137\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 63\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 137 devices are simultaneously generating a large amount of data that needs to be inserted into the database. Each device has 20 data points, with each data point having a size of 99 characters. The data generated by these devices needs to be stored in the database efficiently and in a timely manner.\n"
}
{
    "start_time": "1697348522",
    "end_time": "1697348583",
    "start_timestamp": "2023-10-15 13:42:02",
    "end_timestamp": "2023-10-15 13:43:03",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 107\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 242\n    \n    # Size of each column (in characters)\n    column_size = 64\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a reservation system, 107 users simultaneously try to update a table with 95 columns and 242 rows of reservation records, each with a column size of 64 characters. Multiple users compete to lock the table for performing the update operation, leading to a database exception.\n"
}
{
    "start_time": "1697348643",
    "end_time": "1697348760",
    "start_timestamp": "2023-10-15 13:44:03",
    "end_timestamp": "2023-10-15 13:46:00",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 55\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 3955578\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database reservation system, 55 users are simultaneously searching for available rooms in a database table containing 5 columns and 3,955,578 rows. Each column has a maximum size of 99 characters. However, the search operation is causing a database exception due to a large-scale data cleaning operation that was performed prior to the search.\n"
}
{
    "start_time": "1697348820",
    "end_time": "1697348935",
    "start_timestamp": "2023-10-15 13:47:00",
    "end_timestamp": "2023-10-15 13:48:55",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 73\n    \n    # Number of rows to insert\n    num_rows = 747606\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database for a social media platform, when optimizing the search function, a large number of indexes are created for user profiles, posts, and comments. However, some of these indexes are redundant and do not significantly improve the search performance. This can lead to unnecessary storage consumption and a potential decrease in query performance.\n"
}
{
    "start_time": "1697348995",
    "end_time": "1697349086",
    "start_timestamp": "2023-10-15 13:49:55",
    "end_timestamp": "2023-10-15 13:51:26",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are simultaneously uploading, downloading, or editing files. This leads to competition for input/output resources, resulting in slower file transfers.\n"
}
{
    "start_time": "1697349146",
    "end_time": "1697349206",
    "start_timestamp": "2023-10-15 13:52:26",
    "end_timestamp": "2023-10-15 13:53:26",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system, when multiple users simultaneously execute a join operation on a large dataset, with each user competing for CPU resources, it can lead to contention and result in poor performance of the join operation.\n"
}
{
    "start_time": "1697349266",
    "end_time": "1697349407",
    "start_timestamp": "2023-10-15 13:54:26",
    "end_timestamp": "2023-10-15 13:56:47",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, when a large amount of inventory needs to be fetched for each product, the performance of the query may be affected because it requires executing related subqueries.\n"
}
{
    "start_time": "1697349468",
    "end_time": "1697349539",
    "start_timestamp": "2023-10-15 13:57:48",
    "end_timestamp": "2023-10-15 13:58:59",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 76\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 56\n    \n    # Size of each column (in characters)\n    column_size = 77\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, 76 sensors are generating a large amount of data simultaneously. This data needs to be inserted into a database table with 18 columns, where each column can hold up to 77 characters. There are a total of 56 rows in the table. The purpose of this simulation is to test the database's ability to handle the insertion of a large volume of data, which may result in a performance degradation or database exception if not properly optimized.\n"
}
{
    "start_time": "1697349599",
    "end_time": "1697349670",
    "start_timestamp": "2023-10-15 13:59:59",
    "end_timestamp": "2023-10-15 14:01:10",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 76\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 37\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, 76 sensors generate a large amount of data that needs to be inserted into a database simultaneously. This data consists of 37 columns, each with a size of 52 characters, and a total of 58 rows. The goal is to simulate the database exception that can occur during this process.\n"
}
{
    "start_time": "1697349730",
    "end_time": "1697349790",
    "start_timestamp": "2023-10-15 14:02:10",
    "end_timestamp": "2023-10-15 14:03:10",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 107\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 89\n    \n    # Number of rows to insert\n    num_rows = 230\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for managing employee records, multiple users are trying to update the database simultaneously. This creates conflict and contention as the users compete to lock the database table to perform the update operation. The database table contains 89 columns and 230 rows of employee records, with each column having a size of 54 characters. There are a total of 107 users involved in this process. The aim is to simulate the database exception caused by the contention and locking.\n"
}
{
    "start_time": "1697349851",
    "end_time": "1697349885",
    "start_timestamp": "2023-10-15 14:04:11",
    "end_timestamp": "2023-10-15 14:04:45",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 153\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 3976098\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an e-commerce database with 18 columns and 3,976,098 rows, each with a column size of 52 characters, the VACUUM command is used to clean up unused space and improve database performance. This command is executed by 153 users simultaneously, which may cause an exception or delay in the database.\n"
}
{
    "start_time": "1697349945",
    "end_time": "1697350059",
    "start_timestamp": "2023-10-15 14:05:45",
    "end_timestamp": "2023-10-15 14:07:39",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 94\n    \n    # Number of rows to insert\n    num_rows = 485731\n    \n    # Size of each column (in characters)\n    column_size = 93\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace database, a situation is simulated where there are redundant indexes created for various product attributes such as name, category, and price range. This causes an additional storage footprint and performance overhead. The simulation involves 5 users searching in the database table, which contains 94 columns and 485,731 rows, with each column having a size of 93 characters.\n"
}
{
    "start_time": "1697350120",
    "end_time": "1697350210",
    "start_timestamp": "2023-10-15 14:08:40",
    "end_timestamp": "2023-10-15 14:10:10",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users upload, download, or edit files simultaneously, the system experiences I/O contention. This leads to a slower file transfer process.\n"
}
{
    "start_time": "1697350270",
    "end_time": "1697350331",
    "start_timestamp": "2023-10-15 14:11:10",
    "end_timestamp": "2023-10-15 14:12:11",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a company's database system, there are multiple users simultaneously performing join operations on large datasets using the CPU. This causes contention, leading to poor performance in the database.\n"
}
{
    "start_time": "1697350391",
    "end_time": "1697350540",
    "start_timestamp": "2023-10-15 14:13:11",
    "end_timestamp": "2023-10-15 14:15:40",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail database, when fetching a large amount of data, the execution of related subqueries may be needed to retrieve inventory information for each product. If these subqueries are not optimized, it can lead to a deterioration in the performance of retrieving inventory data.\n"
}
{
    "start_time": "1697350600",
    "end_time": "1697350672",
    "start_timestamp": "2023-10-15 14:16:40",
    "end_timestamp": "2023-10-15 14:17:52",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 173\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 17\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, 173 sensors generate a large amount of data that needs to be inserted into a database. This process will simulate the database exception caused by inserting data from 173 sensors, where each sensor generates data with 17 columns, each column having a size of 66 characters, and a total of 68 rows of data.\n"
}
{
    "start_time": "1697350733",
    "end_time": "1697350805",
    "start_timestamp": "2023-10-15 14:18:53",
    "end_timestamp": "2023-10-15 14:20:05",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 173\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 31\n    \n    # Number of rows to insert\n    num_rows = 70\n    \n    # Size of each column (in characters)\n    column_size = 77\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a system where multiple users are simultaneously inserting large amounts of data into a database, the script is simulating the scenario where 173 users are inserting data. Each user is inserting data into a table with 31 columns, where each column can hold up to 77 characters. The table has a total of 70 rows. This script aims to trigger a database exception caused by the high volume of simultaneous data insertions.\n"
}
{
    "start_time": "1697350865",
    "end_time": "1697350925",
    "start_timestamp": "2023-10-15 14:21:05",
    "end_timestamp": "2023-10-15 14:22:05",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 181\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 82\n    \n    # Number of rows to insert\n    num_rows = 375\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for an online store, there are 181 users simultaneously competing for locks to perform frequent update operations on a database table containing 82 columns and 375 rows of product records. Each column has a size of 50 characters. This simulates a scenario where multiple users are trying to update the same database simultaneously, leading to lock contention and potential database exceptions.\n"
}
{
    "start_time": "1697350985",
    "end_time": "1697351024",
    "start_timestamp": "2023-10-15 14:23:05",
    "end_timestamp": "2023-10-15 14:23:44",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 98\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 17\n    \n    # Number of rows to insert\n    num_rows = 3560139\n    \n    # Size of each column (in characters)\n    column_size = 77\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for an online marketplace, if 98 users simultaneously search for products using terms like product name, category, and price range, after performing a large-scale data cleaning operation on a table containing 17 columns and 3,560,139 rows, each column consisting of 77 characters of product records, it may lead to a database exception.\n"
}
{
    "start_time": "1697351084",
    "end_time": "1697351199",
    "start_timestamp": "2023-10-15 14:24:44",
    "end_timestamp": "2023-10-15 14:26:39",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 63\n    \n    # Number of rows to insert\n    num_rows = 674052\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce platform's database, a large number of indexes are created for items such as product name, category, and price range at the beginning of the query, followed by a query of 10 users. The database table contains 63 columns and 674,052 rows of records, with each column having a size of 58 characters. This simulates the additional storage footprint and performance overhead caused by creating redundant indexes.\n"
}
{
    "start_time": "1697351259",
    "end_time": "1697351350",
    "start_timestamp": "2023-10-15 14:27:39",
    "end_timestamp": "2023-10-15 14:29:10",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing and accessing files simultaneously, there is a high volume of data being uploaded, downloaded, and edited. This creates contention for the I/O resources of the system, causing slower file transfers.\n"
}
{
    "start_time": "1697351410",
    "end_time": "1697351470",
    "start_timestamp": "2023-10-15 14:30:10",
    "end_timestamp": "2023-10-15 14:31:10",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics company, there is a process where multiple tables are joined together to extract meaningful insights from the data. However, if the join performance is poor and there is CPU contention, it can lead to slow query execution and high CPU usage, impacting the overall performance and efficiency of the data analytics tasks.\n"
}
{
    "start_time": "1697351530",
    "end_time": "1697351680",
    "start_timestamp": "2023-10-15 14:32:10",
    "end_timestamp": "2023-10-15 14:34:40",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a database for an e-commerce platform, when retrieving a large amount of data and executing related subqueries to find inventory information for each product, there may be a performance degradation if the subqueries are not optimized.\n"
}
{
    "start_time": "1697351740",
    "end_time": "1697351812",
    "start_timestamp": "2023-10-15 14:35:40",
    "end_timestamp": "2023-10-15 14:36:52",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 130\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 92\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a sensor data monitoring system, a large amount of data generated by 130 sensors needs to be inserted into the database simultaneously. This can result in a database exception due to the high volume of data being inserted. The database table contains 18 columns, each with a column size of 63 characters, and there are 92 rows of data.\n"
}
{
    "start_time": "1697351872",
    "end_time": "1697351944",
    "start_timestamp": "2023-10-15 14:37:52",
    "end_timestamp": "2023-10-15 14:39:04",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 130\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 33\n    \n    # Number of rows to insert\n    num_rows = 71\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, a large amount of data generated by 130 sensors needs to be inserted into the database simultaneously. This process may cause a database exception. The database table contains 33 columns and 71 rows of data, with each column size being 70 characters.\n"
}
{
    "start_time": "1697352004",
    "end_time": "1697352064",
    "start_timestamp": "2023-10-15 14:40:04",
    "end_timestamp": "2023-10-15 14:41:04",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 122\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 78\n    \n    # Number of rows to insert\n    num_rows = 295\n    \n    # Size of each column (in characters)\n    column_size = 76\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, 122 users are simultaneously attempting to update an inventory table with 78 columns and 295 rows, each containing data with a column size of 76 characters. These users are competing with each other to lock the table and perform the update operation. This scenario simulates a database exception caused by contention for the same resource.\n"
}
{
    "start_time": "1697352124",
    "end_time": "1697352167",
    "start_timestamp": "2023-10-15 14:42:04",
    "end_timestamp": "2023-10-15 14:42:47",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 106\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 2705074\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online store, there is a need for frequent data cleaning operations to optimize the system's performance. This script simulates a vacuum operation on a database table containing 18 columns and approximately 2,705,074 rows of product records. The search queries in this scenario are executed by 106 users simultaneously, each searching through columns with a column size of 70 characters. The purpose of this simulation is to trigger a database exception caused by the vacuum operation and the high number of concurrent search queries.\n"
}
{
    "start_time": "1697352227",
    "end_time": "1697352342",
    "start_timestamp": "2023-10-15 14:43:47",
    "end_timestamp": "2023-10-15 14:45:42",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 7\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 64\n    \n    # Number of rows to insert\n    num_rows = 505860\n    \n    # Size of each column (in characters)\n    column_size = 80\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database with 64 columns and 505,860 rows, each column size is 80 characters, a large number of indexes are created at the beginning of the query for items such as product name, category, and price range. Seven users execute queries, and after the query operation, these indexes are deleted. This simulates the additional storage footprint and performance overhead caused by this process.\n"
}
{
    "start_time": "1697352402",
    "end_time": "1697352493",
    "start_timestamp": "2023-10-15 14:46:42",
    "end_timestamp": "2023-10-15 14:48:13",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a collaborative document editing platform, multiple users are uploading, downloading, or editing files simultaneously, resulting in competition for input/output resources. This leads to a slowdown in file transfer operations.\n"
}
{
    "start_time": "1697352553",
    "end_time": "1697352613",
    "start_timestamp": "2023-10-15 14:49:13",
    "end_timestamp": "2023-10-15 14:50:13",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analysis task, multiple users are performing a join operation on a large dataset using a script. The join operation is not optimized, resulting in poor performance. Additionally, there is contention for CPU resources, leading to further degradation of the script's performance.\n"
}
{
    "start_time": "1697352673",
    "end_time": "1697352823",
    "start_timestamp": "2023-10-15 14:51:13",
    "end_timestamp": "2023-10-15 14:53:43",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail database, there is a need to fetch large amounts of data, specifically the inventory for each product. This requires the execution of correlated subqueries. If these subqueries are not properly optimized, the performance of querying inventory may deteriorate.\n"
}
{
    "start_time": "1697352883",
    "end_time": "1697352955",
    "start_timestamp": "2023-10-15 14:54:43",
    "end_timestamp": "2023-10-15 14:55:55",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 157\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 98\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial reporting system, 157 users simultaneously submit a large amount of data for processing. Each data entry contains 10 columns with a column size of 78 characters, and there are a total of 98 data entries. The system is overloaded and experiences a slowdown due to the high number of data submissions.\n"
}
{
    "start_time": "1697353015",
    "end_time": "1697353087",
    "start_timestamp": "2023-10-15 14:56:55",
    "end_timestamp": "2023-10-15 14:58:07",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 157\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 39\n    \n    # Number of rows to insert\n    num_rows = 97\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a manufacturing database, 157 machines simultaneously generate a large amount of data that needs to be inserted into the database. Each machine generates data for 39 columns, with each column size being 52 characters, and a total of 97 rows of data. This process may cause an exception in the database due to the high volume of data being inserted simultaneously.\n"
}
{
    "start_time": "1697353147",
    "end_time": "1697353208",
    "start_timestamp": "2023-10-15 14:59:07",
    "end_timestamp": "2023-10-15 15:00:08",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 89\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 78\n    \n    # Number of rows to insert\n    num_rows = 319\n    \n    # Size of each column (in characters)\n    column_size = 99\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 89 users simultaneously attempt to perform a frequent update operation in a database table containing 78 columns and 319 rows of product records, each with a column size of 99 characters. These users compete with each other to lock the database table to perform the update operation.\n"
}
{
    "start_time": "1697353268",
    "end_time": "1697353353",
    "start_timestamp": "2023-10-15 15:01:08",
    "end_timestamp": "2023-10-15 15:02:33",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 153\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 3276695\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by a popular social media platform, when 153 users simultaneously perform a search after a data cleaning operation on a database table with 10 columns and 3,276,695 rows, each with a column size of 52 characters, an exception is generated due to high search load and insufficient optimization.\n"
}
{
    "start_time": "1697353413",
    "end_time": "1697353528",
    "start_timestamp": "2023-10-15 15:03:33",
    "end_timestamp": "2023-10-15 15:05:28",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 87\n    \n    # Number of rows to insert\n    num_rows = 642046\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace database, 9 users are performing a query operation on a table containing 87 columns and 642,046 rows of product records, each column with a size of 63 characters. Redundant indexes are created before the query operation, which may result in additional storage usage and performance overhead.\n"
}
{
    "start_time": "1697353588",
    "end_time": "1697353678",
    "start_timestamp": "2023-10-15 15:06:28",
    "end_timestamp": "2023-10-15 15:07:58",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a busy file sharing system, multiple users are simultaneously uploading, downloading, or editing files, causing a competition for input/output (I/O) operations. This leads to slower file transfers as the file system struggles to handle the increased workload.\n"
}
{
    "start_time": "1697353738",
    "end_time": "1697353799",
    "start_timestamp": "2023-10-15 15:08:58",
    "end_timestamp": "2023-10-15 15:09:59",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a computer system where multiple tasks are running simultaneously, there is a performance issue with joining tables in a database query. This is causing the CPU to become overloaded, leading to contention and slowing down the overall system performance.\n"
}
{
    "start_time": "1697353859",
    "end_time": "1697354008",
    "start_timestamp": "2023-10-15 15:10:59",
    "end_timestamp": "2023-10-15 15:13:28",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail platform, when trying to retrieve a large amount of data related to inventory, such as the quantity of each product available, the database may encounter performance issues if the query involves correlated subqueries that are not optimized.\n"
}
{
    "start_time": "1697354069",
    "end_time": "1697354141",
    "start_timestamp": "2023-10-15 15:14:29",
    "end_timestamp": "2023-10-15 15:15:41",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 168\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 67\n    \n    # Size of each column (in characters)\n    column_size = 45\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a manufacturing facility, 168 machines are generating a large amount of data that needs to be inserted simultaneously into a database. Each machine has 9 sensors, and each sensor generates data of 45 characters. There are a total of 67 machine instances. This script simulates the database exception that may occur during this data insertion process.\n"
}
{
    "start_time": "1697354201",
    "end_time": "1697354273",
    "start_timestamp": "2023-10-15 15:16:41",
    "end_timestamp": "2023-10-15 15:17:53",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 168\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 40\n    \n    # Number of rows to insert\n    num_rows = 74\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analysis system, a large amount of data from 168 different sources needs to be inserted into the database simultaneously. This process involves 40 columns and 74 rows of data, where each column has a size of 56 characters.\n"
}
{
    "start_time": "1697354333",
    "end_time": "1697354393",
    "start_timestamp": "2023-10-15 15:18:53",
    "end_timestamp": "2023-10-15 15:19:53",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 104\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 69\n    \n    # Number of rows to insert\n    num_rows = 396\n    \n    # Size of each column (in characters)\n    column_size = 98\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online platform, 104 users simultaneously attempt a frequent update operation on a database table containing 69 columns and 396 rows of records. Each column has a size of 98 characters. The users compete with each other to lock the database table in order to perform the update operation. This simulates the database exception caused by contention for locking the table under heavy concurrent access.\n"
}
{
    "start_time": "1697354453",
    "end_time": "1697354554",
    "start_timestamp": "2023-10-15 15:20:53",
    "end_timestamp": "2023-10-15 15:22:34",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 119\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 2294463\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online marketplace, if 119 users simultaneously search the database table containing 9 columns, 2,294,463 rows, each column size of 56 characters, and this search occurs after a large-scale data cleaning operation, it may result in a database exception.\n"
}
{
    "start_time": "1697354614",
    "end_time": "1697354728",
    "start_timestamp": "2023-10-15 15:23:34",
    "end_timestamp": "2023-10-15 15:25:28",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 79\n    \n    # Number of rows to insert\n    num_rows = 536225\n    \n    # Size of each column (in characters)\n    column_size = 68\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an e-commerce database containing 79 columns and 536,225 rows, each with a column size of 68 characters, a large number of indexes are created for different product attributes such as name, category, and price range. This simulation involves 9 users performing queries and then deleting these indexes. The goal is to replicate the additional storage usage and performance impact caused by maintaining redundant indexes.\n"
}
{
    "start_time": "1697354788",
    "end_time": "1697354879",
    "start_timestamp": "2023-10-15 15:26:28",
    "end_timestamp": "2023-10-15 15:27:59",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, there is a situation where files are being uploaded, downloaded, or edited by many users at the same time. This creates competition for input/output (I/O) resources in the file system, resulting in slower file transfers.\n"
}
{
    "start_time": "1697354939",
    "end_time": "1697355000",
    "start_timestamp": "2023-10-15 15:28:59",
    "end_timestamp": "2023-10-15 15:30:00",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system used by a company's HR department, multiple users are attempting to perform a join operation on two large tables containing employee data. The join operation is not optimized, causing poor performance and creating CPU contention as multiple users compete for processing power.\n"
}
{
    "start_time": "1697355060",
    "end_time": "1697355200",
    "start_timestamp": "2023-10-15 15:31:00",
    "end_timestamp": "2023-10-15 15:33:20",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, this statement could be used to simulate a situation in an e-commerce platform's database where there is a large number of product inventory. The performance of querying the inventory might deteriorate because of the execution of related subqueries, which are not optimized.\n"
}
{
    "start_time": "1697355261",
    "end_time": "1697355332",
    "start_timestamp": "2023-10-15 15:34:21",
    "end_timestamp": "2023-10-15 15:35:32",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 55\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a manufacturing plant, there are 53 machines that are generating a large amount of data that needs to be inserted into a database. Each machine has 5 attributes, with each attribute consisting of 63 characters. There are a total of 55 records that need to be inserted. This process may cause a database exception due to the large volume of data being inserted simultaneously.\n"
}
{
    "start_time": "1697355392",
    "end_time": "1697355463",
    "start_timestamp": "2023-10-15 15:36:32",
    "end_timestamp": "2023-10-15 15:37:43",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 32\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 96\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, such as a scientific research project, 53 sensors are generating a large amount of data that needs to be inserted into a database table. The table consists of 32 columns, each with a size of 96 characters, and there are 58 rows of data. Simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697355523",
    "end_time": "1697355583",
    "start_timestamp": "2023-10-15 15:38:43",
    "end_timestamp": "2023-10-15 15:39:43",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 133\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 58\n    \n    # Number of rows to insert\n    num_rows = 247\n    \n    # Size of each column (in characters)\n    column_size = 73\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, multiple users simultaneously attempt to perform frequent update operations in a database table containing 58 columns and 247 rows of product records, each with a column size of 73 characters. These users compete with each other to lock the database table, potentially causing a database exception. The simulation runs with 133 threads.\n"
}
{
    "start_time": "1697355643",
    "end_time": "1697355692",
    "start_timestamp": "2023-10-15 15:40:43",
    "end_timestamp": "2023-10-15 15:41:32",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 168\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2236952\n    \n    # Size of each column (in characters)\n    column_size = 96\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a social media platform, after a large-scale data cleaning operation on a database table containing 5 columns, 2,236,952 rows, each column size of 96 characters of user records, 168 users simultaneously perform a search. Exception during the search process.\n"
}
{
    "start_time": "1697355752",
    "end_time": "1697355866",
    "start_timestamp": "2023-10-15 15:42:32",
    "end_timestamp": "2023-10-15 15:44:26",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 94\n    \n    # Number of rows to insert\n    num_rows = 620427\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a large-scale e-commerce database, when 5 users simultaneously perform a query operation on a database table containing 94 columns and 620,427 rows, each column with a size of 63 characters, redundantly created indexes for product information such as name, category, and price range will cause additional storage overhead and potentially impact the query performance.\n"
}
{
    "start_time": "1697355926",
    "end_time": "1697356017",
    "start_timestamp": "2023-10-15 15:45:26",
    "end_timestamp": "2023-10-15 15:46:57",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are uploading, downloading, or editing files simultaneously. As a result, there is contention for input/output (I/O) resources, leading to a slowdown in file transfer.\n"
}
{
    "start_time": "1697356077",
    "end_time": "1697356138",
    "start_timestamp": "2023-10-15 15:47:57",
    "end_timestamp": "2023-10-15 15:48:58",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a business scenario where a large amount of data is being processed, multiple users are executing join operations on a database table with poor performance. The join operations involve data from different tables, and the CPU becomes overloaded with the processing tasks, leading to a decrease in overall system performance.\n"
}
{
    "start_time": "1697356198",
    "end_time": "1697356347",
    "start_timestamp": "2023-10-15 15:49:58",
    "end_timestamp": "2023-10-15 15:52:27",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform's database, there is a scenario where a large amount of data needs to be fetched, specifically the inventory for each product. In order to find the inventory, correlated subqueries are being executed. However, if these subqueries are not optimized, the performance of the inventory query may be negatively affected.\n"
}
{
    "start_time": "1697356408",
    "end_time": "1697356480",
    "start_timestamp": "2023-10-15 15:53:28",
    "end_timestamp": "2023-10-15 15:54:40",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 182\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 69\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, if a large amount of data needs to be inserted into a database simultaneously by 182 threads, where each row contains 16 columns of data with a column size of 70 characters, there is a possibility of encountering a database exception due to the high workload.\n"
}
{
    "start_time": "1697356540",
    "end_time": "1697356612",
    "start_timestamp": "2023-10-15 15:55:40",
    "end_timestamp": "2023-10-15 15:56:52",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 182\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 94\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data logging system, a large amount of data generated by 182 sensors needs to be inserted into the database simultaneously. The database table contains 20 columns and 94 rows, with each column size of 50 characters. This simulates the database exception caused by the high volume of data being inserted.\n"
}
{
    "start_time": "1697356673",
    "end_time": "1697356733",
    "start_timestamp": "2023-10-15 15:57:53",
    "end_timestamp": "2023-10-15 15:58:53",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 186\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 52\n    \n    # Number of rows to insert\n    num_rows = 253\n    \n    # Size of each column (in characters)\n    column_size = 95\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a multi-user environment, 186 users are simultaneously trying to update a database table containing 52 columns and 253 rows of data, with each column having a size of 95 characters. These users are competing with each other to lock the table for performing the update operation, which may cause a database exception due to lock contention.\n"
}
{
    "start_time": "1697356793",
    "end_time": "1697356855",
    "start_timestamp": "2023-10-15 15:59:53",
    "end_timestamp": "2023-10-15 16:00:55",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 122\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 3185326\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, there is a need for a large-scale data cleaning operation. This script simulates the scenario where 122 users search the database table, which contains 16 columns, 3,185,326 rows, and each column has a size of 50 characters. However, if the database lacks proper index optimization, this can result in an exception occurring during the search process.\n"
}
{
    "start_time": "1697356915",
    "end_time": "1697357029",
    "start_timestamp": "2023-10-15 16:01:55",
    "end_timestamp": "2023-10-15 16:03:49",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 76\n    \n    # Number of rows to insert\n    num_rows = 942481\n    \n    # Size of each column (in characters)\n    column_size = 87\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a large database with 76 columns and 942,481 rows, each with a column size of 87 characters, an excessive number of indexes are created for various attributes such as product name, category, and price range. This leads to extra storage usage and performance overhead.\n"
}
{
    "start_time": "1697357089",
    "end_time": "1697357180",
    "start_timestamp": "2023-10-15 16:04:49",
    "end_timestamp": "2023-10-15 16:06:20",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are uploading, downloading, or editing files at the same time. This creates a scenario where there is contention for input/output (I/O) resources, which slows down the file transfer process.\n"
}
{
    "start_time": "1697357240",
    "end_time": "1697357300",
    "start_timestamp": "2023-10-15 16:07:20",
    "end_timestamp": "2023-10-15 16:08:20",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a large-scale data analysis system, multiple parallel join operations are performed on a database table with poor join performance. This results in CPU contention, where multiple users compete for system resources, causing a slowdown in overall performance.\n"
}
{
    "start_time": "1697357360",
    "end_time": "1697357510",
    "start_timestamp": "2023-10-15 16:09:20",
    "end_timestamp": "2023-10-15 16:11:50",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, a database in an e-commerce platform is experiencing performance issues when trying to fetch large amounts of data by executing correlated subqueries. This particularly impacts the process of retrieving inventory information for each product, causing delays and slowing down the overall system performance.\n"
}
{
    "start_time": "1697357570",
    "end_time": "1697357642",
    "start_timestamp": "2023-10-15 16:12:50",
    "end_timestamp": "2023-10-15 16:14:02",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 152\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 55\n    \n    # Size of each column (in characters)\n    column_size = 38\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT application, a large amount of data generated by 152 sensors needs to be inserted into the database simultaneously. This process simulates a database exception caused by the insertion of data into a database table containing 12 columns, 55 rows, and each column having a size of 38 characters.\n"
}
{
    "start_time": "1697357702",
    "end_time": "1697357774",
    "start_timestamp": "2023-10-15 16:15:02",
    "end_timestamp": "2023-10-15 16:16:14",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 152\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 33\n    \n    # Number of rows to insert\n    num_rows = 95\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) system, 152 sensors generate a large amount of data that needs to be inserted into a database simultaneously. This data insertion process causes contention and can result in exceptions in the database. The database table has 33 columns with a column size of 67 characters and a total of 95 rows.\n"
}
{
    "start_time": "1697357834",
    "end_time": "1697357895",
    "start_timestamp": "2023-10-15 16:17:14",
    "end_timestamp": "2023-10-15 16:18:15",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 123\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 62\n    \n    # Number of rows to insert\n    num_rows = 303\n    \n    # Size of each column (in characters)\n    column_size = 73\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online database, 123 users simultaneously try to update a table containing 62 columns and 303 rows of data. Each column has a size of 73 characters. These users compete with each other to lock the table for performing update operations, resulting in potential database exceptions.\n"
}
{
    "start_time": "1697357955",
    "end_time": "1697358071",
    "start_timestamp": "2023-10-15 16:19:15",
    "end_timestamp": "2023-10-15 16:21:11",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 61\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 2467885\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a large online platform, 61 users are simultaneously performing searches after a database cleaning operation. The database table contains 12 columns and 2,467,885 rows of product records, with each column having a size of 67 characters. This scenario simulates the exception that can occur when there is a high concurrency of users searching the database after a data cleaning process.\n"
}
{
    "start_time": "1697358131",
    "end_time": "1697358247",
    "start_timestamp": "2023-10-15 16:22:11",
    "end_timestamp": "2023-10-15 16:24:07",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 86\n    \n    # Number of rows to insert\n    num_rows = 779666\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used for managing inventory in an online store, there is an issue where redundant indexes are created for various attributes of products such as name, category, and price range. This can lead to increased storage usage and slower performance. The simulation involves 6 users searching in the database table which has 86 columns and 779,666 rows. Each column is of size 78 characters.\n"
}
{
    "start_time": "1697358307",
    "end_time": "1697358398",
    "start_timestamp": "2023-10-15 16:25:07",
    "end_timestamp": "2023-10-15 16:26:38",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are uploading, downloading, or editing files simultaneously, there is contention for I/O resources. This results in sluggish file transfers and a slow file sharing process.\n"
}
{
    "start_time": "1697358458",
    "end_time": "1697358518",
    "start_timestamp": "2023-10-15 16:27:38",
    "end_timestamp": "2023-10-15 16:28:38",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a cloud-based data analytics platform, multiple users are performing join operations on large datasets using CPU-intensive algorithms. The high CPU usage and inefficient join algorithms lead to poor performance in terms of query execution time and resource contention.\n"
}
{
    "start_time": "1697358578",
    "end_time": "1697358719",
    "start_timestamp": "2023-10-15 16:29:38",
    "end_timestamp": "2023-10-15 16:31:59",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online retail system, when retrieving information about the inventory of each product, there is a need to execute related subqueries. If the optimization of these subqueries is not done properly, the performance of the query may be affected when dealing with a large number of products.\n"
}
{
    "start_time": "1697358780",
    "end_time": "1697358851",
    "start_timestamp": "2023-10-15 16:33:00",
    "end_timestamp": "2023-10-15 16:34:11",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 106\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 85\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an Internet of Things (IoT) application, when there are 106 sensors generating a large amount of data that needs to be inserted into a database simultaneously, with each data entry having 9 columns and a column size of 66 characters, and a total of 85 rows, an exception is simulated due to the high workload and potentially insufficient resources to handle the insertion process.\n"
}
{
    "start_time": "1697358911",
    "end_time": "1697358983",
    "start_timestamp": "2023-10-15 16:35:11",
    "end_timestamp": "2023-10-15 16:36:23",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 106\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 26\n    \n    # Number of rows to insert\n    num_rows = 92\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an IoT system, there are 106 devices generating a large amount of data that needs to be inserted into a database simultaneously. The database table has 26 columns with each column having a size of 67 characters, and there are 92 rows. This simulation represents the database exception that can occur when inserting such a large amount of data.\n"
}
{
    "start_time": "1697359043",
    "end_time": "1697359103",
    "start_timestamp": "2023-10-15 16:37:23",
    "end_timestamp": "2023-10-15 16:38:23",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 197\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 52\n    \n    # Number of rows to insert\n    num_rows = 267\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, 197 users simultaneously attempt to perform frequent update operations in a database table containing 52 columns and 267 rows of product records, each with a column size of 67 characters. These users compete with each other to lock the database table to perform the update operation, potentially causing contention and leading to a database exception.\n"
}
{
    "start_time": "1697359163",
    "end_time": "1697359225",
    "start_timestamp": "2023-10-15 16:39:23",
    "end_timestamp": "2023-10-15 16:40:25",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 163\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 9\n    \n    # Number of rows to insert\n    num_rows = 3296063\n    \n    # Size of each column (in characters)\n    column_size = 67\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online platform, a large number of users are searching for products using various criteria such as product name, category, and price range. This script simulates a scenario where 163 users perform simultaneous searches on a database table containing 9 columns and 3,296,063 rows, with each column having a size of 67 characters. The purpose is to trigger a database exception by compressing and reclaiming wasted space in the table through the VACUUM operation and observing the impact on the search performance.\n"
}
{
    "start_time": "1697359286",
    "end_time": "1697359400",
    "start_timestamp": "2023-10-15 16:41:26",
    "end_timestamp": "2023-10-15 16:43:20",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 63\n    \n    # Number of rows to insert\n    num_rows = 421698\n    \n    # Size of each column (in characters)\n    column_size = 88\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database used for an online marketplace, there are 10 users performing a query operation on a table containing 63 columns and 421,698 rows of data. Each column has a size of 88 characters. These users are creating redundant indexes for various attributes like product name, category, and price range. This process can result in additional storage usage and performance overhead.\n"
}
{
    "start_time": "1697359460",
    "end_time": "1697359551",
    "start_timestamp": "2023-10-15 16:44:20",
    "end_timestamp": "2023-10-15 16:45:51",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a system where multiple users are sharing files, there is a high competition for input/output (I/O) resources. This leads to a slowdown in file transfer when multiple users are simultaneously uploading, downloading, or editing files.\n"
}
{
    "start_time": "1697359611",
    "end_time": "1697359672",
    "start_timestamp": "2023-10-15 16:46:51",
    "end_timestamp": "2023-10-15 16:47:52",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database used for customer relationship management, there is a poor performance issue when joining two large tables due to a lack of optimization. This results in high CPU contention, where multiple users compete for CPU resources, leading to processing delays.\n"
}
{
    "start_time": "1697359732",
    "end_time": "1697359881",
    "start_timestamp": "2023-10-15 16:48:52",
    "end_timestamp": "2023-10-15 16:51:21",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In a real-life scenario, this script is simulating a situation in an e-commerce platform's database where a large amount of data needs to be fetched and correlated subqueries are used. This could be similar to a situation where an online retailer needs to retrieve information about product inventory and sales, but the queries are not optimized for efficiency. As a result, the performance of querying inventory data is negatively affected due to the use of correlated subqueries.\n"
}
{
    "start_time": "1697359942",
    "end_time": "1697360014",
    "start_timestamp": "2023-10-15 16:52:22",
    "end_timestamp": "2023-10-15 16:53:34",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 114\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 59\n    \n    # Size of each column (in characters)\n    column_size = 79\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a smart home system, 114 IoT devices are generating a large amount of data that needs to be inserted into the database simultaneously. Each device has 12 columns of data, with each column containing 79 characters. There are a total of 59 records being inserted. Simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697360074",
    "end_time": "1697360145",
    "start_timestamp": "2023-10-15 16:54:34",
    "end_timestamp": "2023-10-15 16:55:45",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 114\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 30\n    \n    # Number of rows to insert\n    num_rows = 100\n    \n    # Size of each column (in characters)\n    column_size = 95\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive system, 114 sensors are generating a large amount of data simultaneously, and this data needs to be inserted into the database. The data consists of 30 columns, with each column having a size of 95 characters, and there are 100 records in total. This process may cause an exception in the database due to the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697360205",
    "end_time": "1697360266",
    "start_timestamp": "2023-10-15 16:56:45",
    "end_timestamp": "2023-10-15 16:57:46",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 200\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 57\n    \n    # Number of rows to insert\n    num_rows = 398\n    \n    # Size of each column (in characters)\n    column_size = 97\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, simulate a scenario where 200 users simultaneously try to update a table containing 57 columns and 398 rows, with each column having a size of 97 characters. These users compete with each other to lock the table, resulting in contention and potentially causing an exception in the database.\n"
}
{
    "start_time": "1697360326",
    "end_time": "1697360377",
    "start_timestamp": "2023-10-15 16:58:46",
    "end_timestamp": "2023-10-15 16:59:37",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 124\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 12\n    \n    # Number of rows to insert\n    num_rows = 3238784\n    \n    # Size of each column (in characters)\n    column_size = 95\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online marketplace, there is a process command that triggers the \"VACUUM\" anomaly scenario. This scenario involves simulating a situation where 124 users are actively performing searches on a table with 12 columns and 3,238,784 rows of product records. Each column has a size of 95 characters. The purpose of this simulation is to expose any exceptions or issues that may arise when multiple users search the database after a database cleanup process.\n"
}
{
    "start_time": "1697360437",
    "end_time": "1697360552",
    "start_timestamp": "2023-10-15 17:00:37",
    "end_timestamp": "2023-10-15 17:02:32",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 7\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 90\n    \n    # Number of rows to insert\n    num_rows = 440562\n    \n    # Size of each column (in characters)\n    column_size = 87\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In an online marketplace database with 90 columns and 440,562 rows, each with a column size of 87 characters, a large number of indexes are created for various attributes such as product name, category, and price range. However, these indexes are redundant and do not significantly improve the performance of queries. Running 7 threads, this script simulates the storage overhead and potential performance degradation caused by maintaining the redundant indexes.\n"
}
{
    "start_time": "1697360612",
    "end_time": "1697360703",
    "start_timestamp": "2023-10-15 17:03:32",
    "end_timestamp": "2023-10-15 17:05:03",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are uploading, downloading, or editing files simultaneously. The system is facing I/O contention, resulting in slower file transfer speeds.\n"
}
{
    "start_time": "1697360763",
    "end_time": "1697360823",
    "start_timestamp": "2023-10-15 17:06:03",
    "end_timestamp": "2023-10-15 17:07:03",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database system used by a financial organization, a poor join performance is simulated wherein multiple users attempt to join large tables without proper indexing or optimization. This process causes contention for CPU resources and severely affects system performance and response time.\n"
}
{
    "start_time": "1697360883",
    "end_time": "1697361033",
    "start_timestamp": "2023-10-15 17:08:03",
    "end_timestamp": "2023-10-15 17:10:33",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an e-commerce platform, when retrieving a large amount of data that requires correlated subqueries, such as finding the inventory levels for each product, there could be a performance degradation if the subqueries are not optimized. This can be simulated using the script \"python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY\".\n"
}
{
    "start_time": "1697361093",
    "end_time": "1697361165",
    "start_timestamp": "2023-10-15 17:11:33",
    "end_timestamp": "2023-10-15 17:12:45",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 86\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 15\n    \n    # Number of rows to insert\n    num_rows = 60\n    \n    # Size of each column (in characters)\n    column_size = 22\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application where 86 concurrent users are inserting a large amount of data into a database table with 15 columns and 60 rows, each column having a size of 22 characters, simulate the database exception caused by this process.\n"
}
{
    "start_time": "1697361225",
    "end_time": "1697361296",
    "start_timestamp": "2023-10-15 17:13:45",
    "end_timestamp": "2023-10-15 17:14:56",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 86\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 35\n    \n    # Number of rows to insert\n    num_rows = 92\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data collection system, 86 sensors are generating a large amount of data simultaneously. This data needs to be inserted into the database. Each data entry contains 35 columns, with each column having a maximum size of 54 characters. There are a total of 92 entries to be inserted. This process might simulate a database exception caused by the high volume of data being inserted at once.\n"
}
{
    "start_time": "1697361356",
    "end_time": "1697361416",
    "start_timestamp": "2023-10-15 17:15:56",
    "end_timestamp": "2023-10-15 17:16:56",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 107\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 92\n    \n    # Number of rows to insert\n    num_rows = 325\n    \n    # Size of each column (in characters)\n    column_size = 66\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for an online platform, 107 users simultaneously attempt to perform a frequent update operation in a database table containing 92 columns and 325 rows of data, with each column having a size of 66 characters. These users compete with each other to lock the database table and perform the update operation, potentially causing a database exception.\n"
}
{
    "start_time": "1697361476",
    "end_time": "1697361539",
    "start_timestamp": "2023-10-15 17:17:56",
    "end_timestamp": "2023-10-15 17:18:59",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 135\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 15\n    \n    # Number of rows to insert\n    num_rows = 2954088\n    \n    # Size of each column (in characters)\n    column_size = 54\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online marketplace, 135 users simultaneously perform a search operation after a large-scale data cleaning process on a database table containing 15 columns and 2,954,088 rows of product records. Each column has a size of 54 characters. This may result in an exception in the database due to the increased workload and cleaning process.\n"
}
{
    "start_time": "1697361599",
    "end_time": "1697361713",
    "start_timestamp": "2023-10-15 17:19:59",
    "end_timestamp": "2023-10-15 17:21:53",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 8\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 69\n    \n    # Number of rows to insert\n    num_rows = 659977\n    \n    # Size of each column (in characters)\n    column_size = 63\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database system for an online marketplace, multiple users are performing queries on a table with 69 columns and 659,977 rows of data. Each column has a size of 63 characters. However, there are redundant indexes created for attributes such as product name, category, and price range. These indexes have an additional storage footprint and cause performance overhead during the query process.\n"
}
{
    "start_time": "1697361774",
    "end_time": "1697361864",
    "start_timestamp": "2023-10-15 17:22:54",
    "end_timestamp": "2023-10-15 17:24:24",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system, multiple users are simultaneously uploading, downloading, or editing files, causing a competition for input/output operations. This results in slower file transfers.\n"
}
{
    "start_time": "1697361924",
    "end_time": "1697361985",
    "start_timestamp": "2023-10-15 17:25:24",
    "end_timestamp": "2023-10-15 17:26:25",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a database used by an online marketplace, there is a performance issue when joining multiple tables due to poor join performance. This, combined with high CPU contention, leads to slower processing and response times.\n"
}
{
    "start_time": "1697362045",
    "end_time": "1697362195",
    "start_timestamp": "2023-10-15 17:27:25",
    "end_timestamp": "2023-10-15 17:29:55",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n",
    "description": "In an online marketplace, if there is a need to fetch a large amount of data from the database, such as inventory information for each product, and the query involves executing correlated subqueries, it may cause a performance issue. Therefore, using the script \"anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY\" would simulate this scenario and trigger the corresponding exception in the database.\n"
}
{
    "start_time": "1697362255",
    "end_time": "1697362327",
    "start_timestamp": "2023-10-15 17:30:55",
    "end_timestamp": "2023-10-15 17:32:07",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 104\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 19\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 21\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a real-life scenario, this script simulates a situation where a system is trying to insert a large amount of data into a database. The script is triggered by running the command \"python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA --threads 104 --ncolumn 19 --colsize 21 --nrow 68\". The system is inserting data from 104 sensors into the database table, which has 19 columns and 68 rows. Each column can hold up to 21 characters. The purpose of this simulation is to test the system's ability to handle the simultaneous insertion of a large amount of data and to detect any exceptions that may occur during this process.\n"
}
{
    "start_time": "1697362387",
    "end_time": "1697362458",
    "start_timestamp": "2023-10-15 17:33:07",
    "end_timestamp": "2023-10-15 17:34:18",
    "alerts": [],
    "labels": [
        "highly concurrent commits or highly concurrent inserts"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 104\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 25\n    \n    # Number of rows to insert\n    num_rows = 65\n    \n    # Size of each column (in characters)\n    column_size = 61\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data-intensive application, 104 sensors are generating a large amount of data that needs to be inserted into a database. Each data record contains 25 columns, with each column having a size of 61 characters. There are a total of 65 data records to be inserted. This simulates the scenario where a high number of sensor inputs overwhelm the database insertion process, potentially causing exceptions or delays in data processing.\n"
}
{
    "start_time": "1697362518",
    "end_time": "1697362578",
    "start_timestamp": "2023-10-15 17:35:18",
    "end_timestamp": "2023-10-15 17:36:18",
    "alerts": [],
    "labels": [
        "highly concurrent updates"
    ],
    "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 78\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 50\n    \n    # Number of rows to insert\n    num_rows = 259\n    \n    # Size of each column (in characters)\n    column_size = 86\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for managing a large inventory, 78 users simultaneously try to update a table that contains 50 columns and 259 rows of product records. Each column has a size of 86 characters. These users compete with each other to lock the database table for the update operation. This simulation aims to trigger a database exception caused by the contention for locking the table.\n"
}
{
    "start_time": "1697362638",
    "end_time": "1697362679",
    "start_timestamp": "2023-10-15 17:37:18",
    "end_timestamp": "2023-10-15 17:37:59",
    "alerts": [],
    "labels": [
        "highly deletes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly VACUUM",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 77\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 19\n    \n    # Number of rows to insert\n    num_rows = 3888305\n    \n    # Size of each column (in characters)\n    column_size = 61\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online store, during a period of heavy usage, 77 users simultaneously perform searches on a database table containing 19 columns and 3,888,305 rows of product records, each with a column size of 61 characters. This process occurs after a large-scale data cleaning operation, which may result in a database exception.\n"
}
{
    "start_time": "1697362739",
    "end_time": "1697362854",
    "start_timestamp": "2023-10-15 17:38:59",
    "end_timestamp": "2023-10-15 17:40:54",
    "alerts": [],
    "labels": [
        "too many indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 100\n    \n    # Number of rows to insert\n    num_rows = 925504\n    \n    # Size of each column (in characters)\n    column_size = 77\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n",
    "description": "In a database for a large online marketplace, a redundant index scenario is being simulated. This scenario involves running a Python script called \"main.py\" with the specified anomaly \"REDUNDANT_INDEX\", along with the parameters \"--threads 9\", \"--ncolumn 100\", \"--colsize 77\", and \"--nrow 925504\". This means that there will be 9 users performing operations on a database table with 100 columns and 925,504 rows, where each column can hold up to 77 characters. The purpose of this simulation is to assess the potential impact of having redundant indexes on the database's storage footprint and performance.\n"
}
{
    "start_time": "1697362914",
    "end_time": "1697363005",
    "start_timestamp": "2023-10-15 17:41:54",
    "end_timestamp": "2023-10-15 17:43:25",
    "alerts": [],
    "labels": [
        "INSERT_LARGE_DATA",
        "IO_CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION",
    "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n",
    "description": "In a file sharing system where multiple users upload, download, or edit files at the same time, the system experiences I/O contention. This leads to a slowdown in file transfer operations.\n"
}
{
    "start_time": "1697363065",
    "end_time": "1697363125",
    "start_timestamp": "2023-10-15 17:44:25",
    "end_timestamp": "2023-10-15 17:45:25",
    "alerts": [],
    "labels": [
        "POOR JOIN PERFORMANCE",
        "CPU CONTENTION"
    ],
    "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n",
    "description": "In a data analytics platform, multiple users are performing join operations on a database table containing a large amount of data. These join operations require a significant amount of CPU resources, leading to contention. As a result, the join performance deteriorates, causing delays in data processing and analysis.\n"
}
{
    "start_time": "1697363185",
    "end_time": "1697363185",
    "start_timestamp": "2023-10-15 17:46:25",
    "end_timestamp": "2023-10-15 17:46:25",
    "alerts": [],
    "labels": [
        "FETCH_LARGE_DATA",
        "CORRELATED SUBQUERY"
    ],
    "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY",
    "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n\n\n",
    "description": "In an online store's database, when trying to retrieve a large amount of data related to products and perform correlated subqueries, there may be a performance issue. This can occur when querying the inventory of each product, especially if the subqueries are not optimized properly.\n"
}
{
    "start_time": "1697440521",
    "end_time": "1697440591",
    "start_timestamp": "2023-10-16 15:15:21",
    "end_timestamp": "2023-10-16 15:16:31",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import import psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online shopping platform, there are 5 users searching in a database table containing 5 columns, 2,000,000 rows, each column size of 50 characters. However, the search lacks the necessary index, which may result in an exception in the database.\n"
}
{
    "start_time": "1697440653",
    "end_time": "1697440724",
    "start_timestamp": "2023-10-16 15:17:33",
    "end_timestamp": "2023-10-16 15:18:44",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analysis scenario, there is a database with 5 columns and 2,000,000 rows. Each column has a size of 50 characters. However, there is a missing index in the database. This can result in an exception when 5 users search the database simultaneously.\n"
}
{
    "start_time": "1697440784",
    "end_time": "1697440854",
    "start_timestamp": "2023-10-16 15:19:44",
    "end_timestamp": "2023-10-16 15:20:54",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a social media platform, if there are 5 users searching in a database table containing 5 columns, 2,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697440915",
    "end_time": "1697440985",
    "start_timestamp": "2023-10-16 15:21:55",
    "end_timestamp": "2023-10-16 15:23:05",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a financial system, if there are 10 concurrent users searching in a table with 5 columns, 2,000,000 rows, each column containing 50 characters, and there is a lack of necessary indexes, an exception will occur in the database.\n"
}
{
    "start_time": "1697441046",
    "end_time": "1697441117",
    "start_timestamp": "2023-10-16 15:24:06",
    "end_timestamp": "2023-10-16 15:25:17",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a social media platform, if there are 10 users simultaneously searching in a database table containing 5 columns, with each column size of 50 characters, and a total of 2,000,000 rows, but the necessary indexes for efficient searching are missing, it can result in a database exception.\n"
}
{
    "start_time": "1697441192",
    "end_time": "1697441263",
    "start_timestamp": "2023-10-16 15:26:32",
    "end_timestamp": "2023-10-16 15:27:43",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a social media platform, if there are 5 users searching in a database table containing 5 columns, 4,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, it can result in a database exception.\n"
}
{
    "start_time": "1697441323",
    "end_time": "1697441394",
    "start_timestamp": "2023-10-16 15:28:43",
    "end_timestamp": "2023-10-16 15:29:54",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial analysis system with 5 users, each user attempts to search through a database table containing 5 columns and 4 million rows of data. Each column has a size of 50 characters. However, the search operations are not optimized with the necessary indexes, causing a database exception.\n"
}
{
    "start_time": "1697441454",
    "end_time": "1697441526",
    "start_timestamp": "2023-10-16 15:30:54",
    "end_timestamp": "2023-10-16 15:32:06",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online database used by a media streaming service, there are 10 users searching for data in a table with 5 columns and 4,000,000 rows. Each column has a size of 50 characters. However, there is no index present to optimize the search process, resulting in reduced performance and potential exceptions in the database.\n"
}
{
    "start_time": "1697441586",
    "end_time": "1697441658",
    "start_timestamp": "2023-10-16 15:33:06",
    "end_timestamp": "2023-10-16 15:34:18",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by a financial company, 10 users are simultaneously searching for data in a table with 5 columns and 4,000,000 rows. Each column has a size of 50 characters. However, the search is missing the necessary indexes, resulting in a potential exception in the database.\n"
}
{
    "start_time": "1697441731",
    "end_time": "1697441802",
    "start_timestamp": "2023-10-16 15:35:31",
    "end_timestamp": "2023-10-16 15:36:42",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial data analysis system, five analysts simultaneously search for information in a database table that contains five columns of data. Each column has a size of 100 characters, and there are 2,000,000 rows of data. However, the search operation does not utilize the necessary indexes, leading to a database exception.\n"
}
{
    "start_time": "1697441862",
    "end_time": "1697441933",
    "start_timestamp": "2023-10-16 15:37:42",
    "end_timestamp": "2023-10-16 15:38:53",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online shopping platform, if there are 5 users simultaneously searching in the database table containing 5 columns, 2,000,000 rows, each column size 100 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697441993",
    "end_time": "1697442064",
    "start_timestamp": "2023-10-16 15:39:53",
    "end_timestamp": "2023-10-16 15:41:04",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a social media platform, if there are 10 users searching in a database table with 5 columns, 2,000,000 rows, each column consisting of 100 characters, but the search lacks the necessary index, it could lead to performance issues and a slower search process.\n"
}
{
    "start_time": "1697442124",
    "end_time": "1697442195",
    "start_timestamp": "2023-10-16 15:42:04",
    "end_timestamp": "2023-10-16 15:43:15",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online shopping platform, if there are 10 users searching in a database table containing 5 columns, 2,000,000 rows, each column size of 100 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697442281",
    "end_time": "1697442352",
    "start_timestamp": "2023-10-16 15:44:41",
    "end_timestamp": "2023-10-16 15:45:52",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, if there are 5 users searching in a database table containing 5 columns, 4,000,000 rows, each column with a size of 100 characters, but the search lacks the necessary index, it may cause an exception in the database.\n"
}
{
    "start_time": "1697442413",
    "end_time": "1697442484",
    "start_timestamp": "2023-10-16 15:46:53",
    "end_timestamp": "2023-10-16 15:48:04",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for an online store, if there are 5 users searching in a table containing 5 columns, 4,000,000 rows, with each column size of 100 characters, but the search lacks the necessary index, it can result in an exception in the database.\n"
}
{
    "start_time": "1697442544",
    "end_time": "1697442616",
    "start_timestamp": "2023-10-16 15:49:04",
    "end_timestamp": "2023-10-16 15:50:16",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial database system, 10 users are simultaneously searching for data in a table with 5 columns, containing 4,000,000 rows. Each column has a size of 100 characters. However, the search operation lacks the necessary indexes, resulting in a delay or exception in retrieving the desired information.\n"
}
{
    "start_time": "1697442676",
    "end_time": "1697442748",
    "start_timestamp": "2023-10-16 15:51:16",
    "end_timestamp": "2023-10-16 15:52:28",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of a social media platform, if there are 10 users searching in a database table containing 5 columns, 4,000,000 rows, each column size of 100 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697442822",
    "end_time": "1697442893",
    "start_timestamp": "2023-10-16 15:53:42",
    "end_timestamp": "2023-10-16 15:54:53",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online gaming platform's database, if there are 5 players searching for game statistics in a table with 10 columns and 2 million rows, each column containing data of size 50 characters, but the search operation lacks the necessary index, it may result in an exception in the database.\n"
}
{
    "start_time": "1697442953",
    "end_time": "1697443024",
    "start_timestamp": "2023-10-16 15:55:53",
    "end_timestamp": "2023-10-16 15:57:04",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, there is a scenario where 5 users are searching in a table with 10 columns and 2,000,000 rows. Each column size is 50 characters. However, the search is missing the necessary indexes, which can lead to a database exception. This can occur in situations where the platform's search functionality is not optimized with the appropriate indexes.\n"
}
{
    "start_time": "1697443084",
    "end_time": "1697443155",
    "start_timestamp": "2023-10-16 15:58:04",
    "end_timestamp": "2023-10-16 15:59:15",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a sales management system, there are 10 concurrent users searching in a table with 10 columns. This table contains 2,000,000 rows, with each column having a size of 50 characters. The search operation lacks the necessary index, resulting in a database exception.\n"
}
{
    "start_time": "1697443215",
    "end_time": "1697443287",
    "start_timestamp": "2023-10-16 16:00:15",
    "end_timestamp": "2023-10-16 16:01:27",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online platform, there are threads of 10 users searching for data in a table with 10 columns. The table contains 2,000,000 rows, with each column having a size of 50 characters. However, the search operation lacks the necessary indexes, leading to a database exception.\n"
}
{
    "start_time": "1697443374",
    "end_time": "1697443446",
    "start_timestamp": "2023-10-16 16:02:54",
    "end_timestamp": "2023-10-16 16:04:06",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial database with 10 columns and 4,000,000 rows, each with a column size of 50 characters, there is a search operation being performed by 5 users simultaneously. However, there is no index on the columns being searched, resulting in a database exception.\n"
}
{
    "start_time": "1697443506",
    "end_time": "1697443577",
    "start_timestamp": "2023-10-16 16:05:06",
    "end_timestamp": "2023-10-16 16:06:17",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a social media platform, if there are 5 users searching in the database table containing 10 columns, 4,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697443637",
    "end_time": "1697443710",
    "start_timestamp": "2023-10-16 16:07:17",
    "end_timestamp": "2023-10-16 16:08:30",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial data analysis system, 10 analysts simultaneously query a database table containing 10 columns and 4,000,000 rows. Each column has a size of 50 characters. However, there is no index in place for efficient searching, resulting in poor performance and longer query execution times.\n"
}
{
    "start_time": "1697443770",
    "end_time": "1697443843",
    "start_timestamp": "2023-10-16 16:09:30",
    "end_timestamp": "2023-10-16 16:10:43",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of a financial institution, 10 customers are simultaneously searching for their transactions in a table with 10 columns and 4,000,000 rows. Each column has a size of 50 characters. However, there is no index present for the search, resulting in a database exception.\n"
}
{
    "start_time": "1697443927",
    "end_time": "1697443998",
    "start_timestamp": "2023-10-16 16:12:07",
    "end_timestamp": "2023-10-16 16:13:18",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, there are 5 users searching for information in a table with 10 columns and 2 million rows. Each column contains 100 characters. However, there is an issue with missing indexes, which may result in slower search performance and potentially cause database exceptions.\n"
}
{
    "start_time": "1697444058",
    "end_time": "1697444128",
    "start_timestamp": "2023-10-16 16:14:18",
    "end_timestamp": "2023-10-16 16:15:28",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by a travel booking website, when 5 users simultaneously search for flights, hotels, and car rentals in a table with 10 columns and 2,000,000 rows, each column being 100 characters long, there is an exception caused by the absence of necessary indexes in the database.\n"
}
{
    "start_time": "1697444189",
    "end_time": "1697444260",
    "start_timestamp": "2023-10-16 16:16:29",
    "end_timestamp": "2023-10-16 16:17:40",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by a financial institution, if there are 10 parallel transactions running in a table with 10 columns, each column capable of storing 100 characters, and a total of 2 million rows, but the necessary indexes are missing, this can lead to a performance degradation in the database.\n"
}
{
    "start_time": "1697444320",
    "end_time": "1697444392",
    "start_timestamp": "2023-10-16 16:18:40",
    "end_timestamp": "2023-10-16 16:19:52",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial database, if there are 10 users searching in the database table containing 10 columns, 2,000,000 rows, each column size of 100 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697444504",
    "end_time": "1697444576",
    "start_timestamp": "2023-10-16 16:21:44",
    "end_timestamp": "2023-10-16 16:22:56",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of a financial institution, if 5 employees are searching in a database table containing 10 columns and 4,000,000 rows, each column size being 100 characters, but the search lacks the necessary index, an exception will occur in the database.\n"
}
{
    "start_time": "1697444636",
    "end_time": "1697444708",
    "start_timestamp": "2023-10-16 16:23:56",
    "end_timestamp": "2023-10-16 16:25:08",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used for a financial analysis application, if 5 users search for data in a table containing 10 columns and 4,000,000 rows, each column storing data up to 100 characters long, but the necessary indexes are missing, it may result in a database exception.\n"
}
{
    "start_time": "1697444769",
    "end_time": "1697444842",
    "start_timestamp": "2023-10-16 16:26:09",
    "end_timestamp": "2023-10-16 16:27:22",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large-scale database of an online platform, 10 users are searching for information in a table with 10 columns, containing 4,000,000 rows, where each column has a size of 100 characters. However, the search is missing the necessary indexes, resulting in a database exception.\n"
}
{
    "start_time": "1697444903",
    "end_time": "1697444976",
    "start_timestamp": "2023-10-16 16:28:23",
    "end_timestamp": "2023-10-16 16:29:36",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a business database system, if there are 10 users concurrently searching a table with 10 columns, each containing 100 characters, and a total of 4,000,000 rows of data, but the necessary indexes are missing, it may result in exceptions or slower performance in the database.\n"
}
{
    "start_time": "1697445061",
    "end_time": "1697445132",
    "start_timestamp": "2023-10-16 16:31:01",
    "end_timestamp": "2023-10-16 16:32:12",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a social media platform, if there are 5 users searching in the database table containing 20 columns, 2,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697445192",
    "end_time": "1697445263",
    "start_timestamp": "2023-10-16 16:33:12",
    "end_timestamp": "2023-10-16 16:34:23",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online shopping platform, there are 5 users searching in the database table containing 20 columns, 2,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697445323",
    "end_time": "1697445395",
    "start_timestamp": "2023-10-16 16:35:23",
    "end_timestamp": "2023-10-16 16:36:35",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online shopping platform, when 10 users search the database table containing 20 columns and 2,000,000 rows, with each column size of 50 characters, but the search lacks the necessary index, an exception occurs in the database.\n"
}
{
    "start_time": "1697445455",
    "end_time": "1697445527",
    "start_timestamp": "2023-10-16 16:37:35",
    "end_timestamp": "2023-10-16 16:38:47",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online shopping platform, if there are 10 users searching in the database table containing 20 columns, 2,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697445639",
    "end_time": "1697445711",
    "start_timestamp": "2023-10-16 16:40:39",
    "end_timestamp": "2023-10-16 16:41:51",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online banking system, if 5 users search the database table containing 20 columns and 4,000,000 rows, where each column has a size of 50 characters, but the search does not have the necessary index, it could result in a database exception.\n"
}
{
    "start_time": "1697445771",
    "end_time": "1697445843",
    "start_timestamp": "2023-10-16 16:42:51",
    "end_timestamp": "2023-10-16 16:44:03",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online shopping platform, there is an issue with missing indexes. The script is being used to simulate the scenario where 5 users are searching in a table with 20 columns, each having a size of 50 characters, and 4 million rows. This lack of indexes can cause performance issues and potentially lead to exceptions in the database.\n"
}
{
    "start_time": "1697445903",
    "end_time": "1697445976",
    "start_timestamp": "2023-10-16 16:45:03",
    "end_timestamp": "2023-10-16 16:46:16",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the context of a database used by an online shopping platform, there is a situation where 10 users are performing searches on a database table with 20 columns and 4,000,000 rows. Each column has a size of 50 characters. However, there is a lack of necessary indexes in the database, leading to a potential exception or error.\n"
}
{
    "start_time": "1697446036",
    "end_time": "1697446109",
    "start_timestamp": "2023-10-16 16:47:16",
    "end_timestamp": "2023-10-16 16:48:29",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online shopping platform's database, when 10 users simultaneously search for information in a table with 20 columns and 4,000,000 rows, and each column is limited to a size of 50 characters, an exception will occur if there is no index created for the search operation.\n"
}
{
    "start_time": "1697446221",
    "end_time": "1697446293",
    "start_timestamp": "2023-10-16 16:50:21",
    "end_timestamp": "2023-10-16 16:51:33",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of an online shopping platform, 5 users are simultaneously searching in a table with 20 columns and 2 million rows of data. Each column has a size of 100 characters. However, the search is lacking the necessary indexes, resulting in a database exception.\n"
}
{
    "start_time": "1697446353",
    "end_time": "1697446425",
    "start_timestamp": "2023-10-16 16:52:33",
    "end_timestamp": "2023-10-16 16:53:45",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online shopping platform, 5 users perform searches on a database table with 20 columns and 2,000,000 rows. Each column can hold up to 100 characters. However, the search queries lack the necessary indexes, which causes a database exception to occur.\n"
}
{
    "start_time": "1697446485",
    "end_time": "1697446557",
    "start_timestamp": "2023-10-16 16:54:45",
    "end_timestamp": "2023-10-16 16:55:57",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online shopping platform, there are 10 users simultaneously searching in a table with 20 columns and 2,000,000 rows. Each column has a size of 100 characters. However, an exception occurs because there is no index available for the search operation, causing performance issues in retrieving the required data.\n"
}
{
    "start_time": "1697446617",
    "end_time": "1697446690",
    "start_timestamp": "2023-10-16 16:56:57",
    "end_timestamp": "2023-10-16 16:58:10",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large database used by an e-commerce platform, 10 users are simultaneously searching for information in a table with 20 columns. The table contains 2 million rows, with each column having a size of 100 characters. However, there is a missing index on the database table, which leads to an exception being raised in the system.\n"
}
{
    "start_time": "1697446896",
    "end_time": "1697446967",
    "start_timestamp": "2023-10-16 17:01:36",
    "end_timestamp": "2023-10-16 17:02:47",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "This statement is simulating a scenario in an online shopping platform where 5 users are searching the database table for information. The table contains 20 columns with data size of 100 characters and 4,000,000 rows. However, there is a missing index, which can cause a database exception or error when trying to perform the search operation.\n"
}
{
    "start_time": "1697447027",
    "end_time": "1697447101",
    "start_timestamp": "2023-10-16 17:03:47",
    "end_timestamp": "2023-10-16 17:05:01",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for an online shopping platform, 10 users are performing searches in a database table with 20 columns, 4,000,000 rows, and each column has a size of 100 characters. However, there is no index configured for the search operation, resulting in a potential database exception.\n"
}
{
    "start_time": "1697447162",
    "end_time": "1697447235",
    "start_timestamp": "2023-10-16 17:06:02",
    "end_timestamp": "2023-10-16 17:07:15",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online shopping platform with a database table containing 20 columns and 4,000,000 rows, if there are 10 users searching for products using terms like product name, category, and price range, but the necessary indexes are missing, the search operation may result in a database exception.\n"
}
{
    "start_time": "1697447302",
    "end_time": "1697447373",
    "start_timestamp": "2023-10-16 17:08:22",
    "end_timestamp": "2023-10-16 17:09:33",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online platform, if there are 5 users searching in the database table containing 10 columns, 2,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697447433",
    "end_time": "1697447504",
    "start_timestamp": "2023-10-16 17:10:33",
    "end_timestamp": "2023-10-16 17:11:44",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online platform, there is an issue with missing indexes. This causes exceptions when 5 users perform searches simultaneously in a table with 5 columns and 2,000,000 rows. Each column has a size of 50 characters. The lack of necessary indexes hampers the efficiency of the search process.\n"
}
{
    "start_time": "1697447564",
    "end_time": "1697447635",
    "start_timestamp": "2023-10-16 17:12:44",
    "end_timestamp": "2023-10-16 17:13:55",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a data analytics platform, there are 10 users searching in a database table with 5 columns, 2,000,000 rows, each column having a size of 50 characters. However, the search query lacks the necessary indexes, resulting in a database exception.\n"
}
{
    "start_time": "1697447695",
    "end_time": "1697447766",
    "start_timestamp": "2023-10-16 17:14:55",
    "end_timestamp": "2023-10-16 17:16:06",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a financial institution, there are 10 clients simultaneously searching for information in a table containing 5 columns and 2,000,000 rows. Each column has a size of 50 characters. However, the necessary indexes for efficient searching are missing, resulting in slower database performance.\n"
}
{
    "start_time": "1697447840",
    "end_time": "1697447911",
    "start_timestamp": "2023-10-16 17:17:20",
    "end_timestamp": "2023-10-16 17:18:31",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial management system, if there are 5 users searching in a database table containing 5 columns and 4,000,000 rows, each column with a size of 50 characters, but the search lacks the necessary index, an exception will occur in the system.\n"
}
{
    "start_time": "1697447971",
    "end_time": "1697448042",
    "start_timestamp": "2023-10-16 17:19:31",
    "end_timestamp": "2023-10-16 17:20:42",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a healthcare database, if there are 5 doctors searching for patient records in a database table containing 5 columns, 4,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, it may result in slower performance and longer response times.\n"
}
{
    "start_time": "1697448102",
    "end_time": "1697448174",
    "start_timestamp": "2023-10-16 17:21:42",
    "end_timestamp": "2023-10-16 17:22:54",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial transaction database, if there are 10 concurrent transactions being processed and searching through a database table with 5 columns, 4,000,000 rows, where each column has a size of 50 characters, but the necessary index is missing, it can result in delays and inefficiencies in retrieving transaction data.\n"
}
{
    "start_time": "1697448234",
    "end_time": "1697448306",
    "start_timestamp": "2023-10-16 17:23:54",
    "end_timestamp": "2023-10-16 17:25:06",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online bookstore with 10 concurrent users, a database table containing 5 columns and 4,000,000 rows, each column having a size of 50 characters, is searched without the necessary indexes. This results in a database exception due to the lack of optimized indexing for efficient search operations.\n"
}
{
    "start_time": "1697448378",
    "end_time": "1697448449",
    "start_timestamp": "2023-10-16 17:26:18",
    "end_timestamp": "2023-10-16 17:27:29",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a media content platform, if there are 5 users searching in a table containing 5 columns, with each column size of 100 characters, and a total of 2,000,000 rows, but the search operation lacks the necessary indexes, it can result in an exception being raised in the database.\n"
}
{
    "start_time": "1697448509",
    "end_time": "1697448580",
    "start_timestamp": "2023-10-16 17:28:29",
    "end_timestamp": "2023-10-16 17:29:40",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an online store, when 5 users simultaneously search for products using only 5 columns, but with a large table of 2,000,000 rows, each with a column size of 100 characters, the lack of proper indexing causes a database exception.\n"
}
{
    "start_time": "1697448640",
    "end_time": "1697448711",
    "start_timestamp": "2023-10-16 17:30:40",
    "end_timestamp": "2023-10-16 17:31:51",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database of an online platform, 10 users are searching in a table with 5 columns and 2,000,000 rows. Each column has a size of 100 characters. However, the search is lacking the necessary index, resulting in a database exception.\n"
}
{
    "start_time": "1697448771",
    "end_time": "1697448842",
    "start_timestamp": "2023-10-16 17:32:51",
    "end_timestamp": "2023-10-16 17:34:02",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a social media platform, if there are 10 users searching in the database table containing 5 columns, 2,000,000 rows, each column size of 100 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697448928",
    "end_time": "1697448999",
    "start_timestamp": "2023-10-16 17:35:28",
    "end_timestamp": "2023-10-16 17:36:39",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large-scale database of a financial institution, if 5 users search the database table containing 5 columns, 4 million rows, each column size of 100 characters, but the search lacks the necessary index, it may cause a performance issue due to the absence of the required optimization.\n"
}
{
    "start_time": "1697449059",
    "end_time": "1697449131",
    "start_timestamp": "2023-10-16 17:37:39",
    "end_timestamp": "2023-10-16 17:38:51",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a financial analytics system, when performing data analysis on a large dataset containing 4,000,000 rows and 5 columns, with each column having a size of 100 characters, if there are 5 concurrent users searching for specific data, but the necessary indexes on the database table are missing, it may result in a database exception.\n"
}
{
    "start_time": "1697449191",
    "end_time": "1697449263",
    "start_timestamp": "2023-10-16 17:39:51",
    "end_timestamp": "2023-10-16 17:41:03",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large database with 10 users performing searches on a table containing 5 columns and 4 million rows, each column having a size of 100 characters, a database exception occurs due to the lack of necessary indexes for efficient searching.\n"
}
{
    "start_time": "1697449323",
    "end_time": "1697449395",
    "start_timestamp": "2023-10-16 17:42:03",
    "end_timestamp": "2023-10-16 17:43:15",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system for an online marketplace, when 10 users search for products in a database table with 5 columns, containing 4,000,000 rows, and each column has a size of 100 characters, an exception occurs due to the lack of necessary indexes on the search columns.\n"
}
{
    "start_time": "1697449469",
    "end_time": "1697449540",
    "start_timestamp": "2023-10-16 17:44:29",
    "end_timestamp": "2023-10-16 17:45:40",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for a social media platform, if there are 5 users searching in a database table containing 10 columns, 2,000,000 rows, with each column size of 50 characters, but the search lacks the necessary index, it could result in an exception in the database.\n"
}
{
    "start_time": "1697449600",
    "end_time": "1697449671",
    "start_timestamp": "2023-10-16 17:46:40",
    "end_timestamp": "2023-10-16 17:47:51",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a company's database for managing employee records, if 5 users simultaneously search for information in a database table containing 10 columns and 2,000,000 rows, each column's size being 50 characters, but the necessary index is missing, it may lead to performance issues and a potential exception in the database.\n"
}
{
    "start_time": "1697449731",
    "end_time": "1697449803",
    "start_timestamp": "2023-10-16 17:48:51",
    "end_timestamp": "2023-10-16 17:50:03",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a social media platform, if there are 10 users searching in a database table with 10 columns and 2,000,000 rows, each column having a size of 50 characters, but the search lacks the necessary index, it may lead to a database exception.\n"
}
{
    "start_time": "1697449863",
    "end_time": "1697449934",
    "start_timestamp": "2023-10-16 17:51:03",
    "end_timestamp": "2023-10-16 17:52:14",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database scenario, there are 10 users searching for data in a table with 10 columns and 2 million rows. Each column has a size of 50 characters. However, this search operation is missing the necessary indexes, which can result in a database exception.\n"
}
{
    "start_time": "1697450021",
    "end_time": "1697450092",
    "start_timestamp": "2023-10-16 17:53:41",
    "end_timestamp": "2023-10-16 17:54:52",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by an online marketplace, there are 5 simultaneous search requests being made by users. The search is being performed on a table with 10 columns and 4,000,000 rows, where each column can hold up to 50 characters. However, the search queries are not utilizing the necessary indexes, which can result in slower performance or even database exceptions.\n"
}
{
    "start_time": "1697450152",
    "end_time": "1697450223",
    "start_timestamp": "2023-10-16 17:55:52",
    "end_timestamp": "2023-10-16 17:57:03",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an online banking system, if there are 5 users searching in the database table containing 10 columns, 4,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697450283",
    "end_time": "1697450356",
    "start_timestamp": "2023-10-16 17:58:03",
    "end_timestamp": "2023-10-16 17:59:16",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by a social media platform, when 10 users are searching a database table with 10 columns, 4,000,000 rows, each column having a size of 50 characters, an exception occurs due to the lack of necessary indexes for efficient searching.\n"
}
{
    "start_time": "1697450416",
    "end_time": "1697450490",
    "start_timestamp": "2023-10-16 18:00:16",
    "end_timestamp": "2023-10-16 18:01:30",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large database of a financial organization, 10 employees are simultaneously searching for information in a database table with 10 columns and 4,000,000 rows. Each column has a size of 50 characters. However, there is no index created for the search, resulting in slow query performance and possible database exceptions.\n"
}
{
    "start_time": "1697450573",
    "end_time": "1697450644",
    "start_timestamp": "2023-10-16 18:02:53",
    "end_timestamp": "2023-10-16 18:04:04",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a large-scale data processing scenario, where there are 5 concurrent users searching in a database table with 10 columns and 2 million rows, each column containing 100 characters, the absence of necessary indexes in the search operation leads to a database exception.\n"
}
{
    "start_time": "1697450704",
    "end_time": "1697450775",
    "start_timestamp": "2023-10-16 18:05:04",
    "end_timestamp": "2023-10-16 18:06:15",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database for an online shopping platform, if 5 users search in a table with 10 columns, each column containing 100 characters, and a total of 2,000,000 rows, but the necessary index is missing, it may result in a database exception.\n"
}
{
    "start_time": "1697450835",
    "end_time": "1697450907",
    "start_timestamp": "2023-10-16 18:07:15",
    "end_timestamp": "2023-10-16 18:08:27",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In an analytics system, 10 users simultaneously search a database table with 10 columns, 2,000,000 rows, each column containing 100 characters. However, the search query lacks the necessary indexes, resulting in an exception being thrown in the database.\n"
}
{
    "start_time": "1697450968",
    "end_time": "1697451039",
    "start_timestamp": "2023-10-16 18:09:28",
    "end_timestamp": "2023-10-16 18:10:39",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database system used by a large online platform, 10 users simultaneously search for data in a table with 10 columns and 2,000,000 rows. Each column contains 100 characters. However, the search operation does not utilize the necessary indexes, resulting in a performance issue and potential exceptions in the database.\n"
}
{
    "start_time": "1697451151",
    "end_time": "1697451224",
    "start_timestamp": "2023-10-16 18:12:31",
    "end_timestamp": "2023-10-16 18:13:44",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by a social media platform, 5 users simultaneously perform searches on a database table containing 10 columns, 4,000,000 rows, each column with a size of 100 characters. However, the search operation lacks the necessary indexes, resulting in an exception in the database.\n"
}
{
    "start_time": "1697451284",
    "end_time": "1697451356",
    "start_timestamp": "2023-10-16 18:14:44",
    "end_timestamp": "2023-10-16 18:15:56",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In a database used by an e-commerce platform, 5 users are searching for information in a table with 10 columns and 4,000,000 rows, where each column can store up to 100 characters. However, the search queries are not optimized with proper indexes, resulting in poor query performance and potential database exceptions.\n"
}
{
    "start_time": "1697451416",
    "end_time": "1697451490",
    "start_timestamp": "2023-10-16 18:16:56",
    "end_timestamp": "2023-10-16 18:18:10",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online platform, 10 users perform searches on a table containing 10 columns and 4,000,000 rows, with each column having a size of 100 characters. However, the search operation lacks the necessary index, leading to a database exception.\n"
}
{
    "start_time": "1697451550",
    "end_time": "1697451623",
    "start_timestamp": "2023-10-16 18:19:10",
    "end_timestamp": "2023-10-16 18:20:23",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 10\n    \n    # Number of rows to insert\n    num_rows = 4000000\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of a content management system, 10 users are searching in a database table containing 10 columns and 4,000,000 rows. Each column has a size of 100 characters. However, the search operation does not have the necessary index, resulting in a database exception.\n"
}
{
    "start_time": "1697451708",
    "end_time": "1697451778",
    "start_timestamp": "2023-10-16 18:21:48",
    "end_timestamp": "2023-10-16 18:22:58",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the database of an online shopping platform, if there are 5 users searching in the database table containing 20 columns, 2,000,000 rows, each column size of 50 characters, but the search lacks the necessary index, resulting in an exception in the database.\n"
}
{
    "start_time": "1697451838",
    "end_time": "1697451909",
    "start_timestamp": "2023-10-16 18:23:58",
    "end_timestamp": "2023-10-16 18:25:09",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n",
    "description": "In the case of a database for an online shopping platform, if 5 users simultaneously search for data in a database table with 20 columns, containing 2,000,000 rows, where each column has a size of 50 characters, and the necessary indexes are missing, it may lead to a database exception.\n"
}
{
    "start_time": "1697451969",
    "end_time": "1697452041",
    "start_timestamp": "2023-10-16 18:26:09",
    "end_timestamp": "2023-10-16 18:27:21",
    "alerts": [],
    "labels": [
        "missing indexes"
    ],
    "command": "python anomaly_trigger/main.py --anomaly MISSING_INDEXES",
    "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef missing_index(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    db=Database(init())\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be selected \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.execute_sqls(insert_data)  \n\n    #select without the index\n    missing_index='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,missing_index,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    #print the end time\n    print_time()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 10\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 2000000\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    missing_index(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n\n\n\n",
    "description": "In a database used by an online shopping platform, if there are 10 users simultaneously searching in a database table containing 20 columns, 2 million rows, each column with a size of 50 characters, and the search operation lacks the necessary index, it can lead to a database exception.\n"
}
