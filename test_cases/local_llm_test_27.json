{"0": {"start_time": "1697297150", "end_time": "1697297328", "start_timestamp": "2023-10-14 23:25:50", "end_timestamp": "2023-10-14 23:28:48", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:cpu_usage[ins=] = 0.99 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.99"}, "startsAt": "2023-10-14T23:26:49.467858611Z", "endsAt": "2023-10-14T23:27:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Acpu_usage_1m+%3E+0.7&g0.tab=1", "fingerprint": "b97dc2cd4ed607f5"}], "groupLabels": {"alertname": "NodeCpuHigh"}, "commonLabels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:cpu_usage[ins=] = 0.99 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.99"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeCpuHigh\"}", "truncatedAlerts": 0}], "labels": ["highly deletes"], "command": "python anomaly_trigger/main.py --anomaly VACUUM", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 176\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 18\n    \n    # Number of rows to insert\n    num_rows = 3167807\n    \n    # Size of each column (in characters)\n    column_size = 70\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In an online marketplace, 176 users perform simultaneous searches on a database table containing 18 columns and 3,167,807 rows of product records. Each column has a size of 70 characters. The searches are conducted after a large-scale data cleaning operation, which causes a database exception due to increased workload and decreased performance.\n", "workload": {"delete from table1 where id < 2534245;": 1}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 3.0, 9.0, 2.0, 4.0, 8.0, 4.0, 11.0, 1.0, 5.0, 4.0, 4.0, 2.0, 1.0, 2.0, 4.0, 3.0, 5.0, 6.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 136.0, 186.0, 186.0, 186.0, 188.0, 186.0, 188.0, 191.0, 185.0, 182.0, 183.0, 186.0, 187.0, 194.0, 186.0, 188.0, 189.0, 185.0, 187.0, 187.0, 184.0, 183.0, 182.0, 182.0, 171.0, 168.0, 104.0, 53.0, 1.0], "node_procs_blocked": [0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 3.0, 2.0, 0.0, 0.0, 2.0, 1.0, 1.0, 3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 2.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 3.0, 0.0, 0.0, 2.0, 4.0, 0.0, 0.0, 1.0, 0.0], "node_entropy_available_bits": [3501.0, 3502.0, 3513.0, 3539.0, 3559.0, 3578.0, 3597.0, 3619.0, 3639.0, 3653.0, 3680.0, 3700.0, 3716.0, 3741.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0], "node_load1": [2.13, 1.96, 1.88, 1.88, 1.89, 1.89, 1.82, 1.91, 1.91, 1.84, 1.84, 1.93, 2.02, 2.02, 2.1, 2.1, 2.01, 1.93, 1.93, 1.93, 1.93, 1.86, 1.87, 1.87, 1.8, 1.8, 1.82, 1.75, 1.75, 1.69, 1.69, 1.56, 16.25, 16.25, 29.84, 29.84, 42.18, 53.7, 53.7, 64.06, 64.06, 73.42, 82.44, 82.44, 90.81, 90.81, 98.43, 105.6, 105.6, 111.88, 111.88, 117.66, 122.89, 122.89, 127.54, 127.54, 131.02, 129.18, 129.18, 118.83]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 36.333333333333336, 30.333333333333332, 1.0, 0.6666666666666666, 1.0, 109.66666666666667, 47.333333333333336, 14.333333333333334, 47.333333333333336, 205.66666666666666, 27.333333333333332, 49.666666666666664, 1.3333333333333333, 135.0, 37.666666666666664, 78.0, 59.666666666666664, 229.0, 2.0, 46.666666666666664, 195.33333333333334, 390.0, 50.666666666666664, 62.333333333333336, 17.0, 29.666666666666668, 0.3333333333333333, 0.0, 41.0, 9.333333333333334, 1.0, 0.3333333333333333, 2.0, 12.0, 15.0, 2.6666666666666665, 22.0, 0.0, 15.0, 0.0, 0.0, 59.666666666666664, 3.3333333333333335, 0.0, 0.6666666666666666, 26.0, 38.0, 21.666666666666668, 17.333333333333332, 12.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 0.0, 0.0, 0.0, 2730.6666666666665, 0.0, 1365.3333333333333, 0.0, 2730.6666666666665, 660821.3333333334, 703146.6666666666, 4096.0, 2730.6666666666665, 4096.0, 2323797.3333333335, 946176.0, 88746.66666666667, 233472.0, 6063445.333333333, 897024.0, 795989.3333333334, 13653.333333333334, 2514944.0, 516096.0, 2609152.0, 1981098.6666666667, 6425258.666666667, 87381.33333333333, 2348373.3333333335, 954368.0, 5540522.666666667, 1325738.6666666667, 2715648.0, 420522.6666666667, 1077248.0, 1365.3333333333333, 0.0, 1078613.3333333333, 518826.6666666667, 4096.0, 1365.3333333333333, 53248.0, 98304.0, 578901.3333333334, 98304.0, 253952.0, 0.0, 595285.3333333334, 0.0, 0.0, 248490.66666666666, 13653.333333333334, 0.0, 2730.6666666666665, 830122.6666666666, 1008981.3333333334, 1396736.0, 398677.3333333333, 49152.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.012999999996585151, 0.0, 0.0, 0.0, 0.0016666666682188709, 0.11533333330104749, 0.09500000001086543, 0.018000000001241762, 0.007666666681567828, 0.029333333329608042, 0.9933333333271245, 1.3566666666495923, 0.15666666669615856, 0.10333333331315468, 1.1923333333494763, 0.4236666666499029, 0.9610000000102445, 0.050000000007761024, 1.0239999999757856, 1.172000000020489, 2.8009999999776483, 2.021999999997206, 4.542333333364998, 0.0803333333072563, 0.2616666666775321, 0.0636666666638727, 0.78666666666201, 0.05100000001645336, 0.11700000000807147, 0.02999999998913457, 0.1083333333178113, 0.0, 0.0, 0.06300000000434618, 0.029333333329608042, 0.0, 0.0003333333491658171, 0.0, 0.006000000013348957, 0.045999999972991645, 0.0006666666595265269, 0.021000000027318794, 0.0, 0.03899999998975545, 0.0, 0.0, 0.04233333332619319, 0.0006666666595265269, 0.0, 0.0, 0.06900000001769513, 0.03733333332153658, 0.0613333333361273, 0.034999999993791185, 0.03200000000651926]}, "memory": {"node_memory_Inactive_anon_bytes": [233639936.0, 233639936.0, 297340928.0, 645787648.0, 858333184.0, 994586624.0, 1410961408.0, 1609715712.0, 1681817600.0, 2072154112.0, 2288951296.0, 2443890688.0, 2763046912.0, 2976579584.0, 3170316288.0, 3441963008.0, 3714469888.0, 3863355392.0, 4073107456.0, 4132630528.0, 4132626432.0, 4132626432.0, 4132626432.0, 4132626432.0, 4132626432.0, 4132626432.0, 4132626432.0, 4132626432.0, 4132626432.0, 212324352.0, 212324352.0, 280133632.0, 280129536.0, 280129536.0, 280129536.0, 280129536.0, 280137728.0, 280137728.0, 280039424.0, 279678976.0, 279707648.0, 279703552.0, 279965696.0, 279883776.0, 279932928.0, 280076288.0, 280084480.0, 280084480.0, 280072192.0, 280080384.0, 279777280.0, 279707648.0, 279719936.0, 278269952.0, 274472960.0, 267939840.0, 255156224.0, 241369088.0, 220807168.0, 212353024.0]}, "network": {"node_sockstat_TCP_tw": [9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 7.0, 7.0, 5.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 5.0, 8.0, 8.0, 7.0, 7.0, 10.0, 10.0, 10.0, 9.0, 10.0, 10.0, 10.0, 10.0, 12.0, 14.0, 14.0, 14.0, 14.0, 13.0, 13.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 12.0, 12.0, 11.0, 12.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 59.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 1.0, 1.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 21.0, 197.0, 197.0, 197.0, 197.0, 197.0, 197.0, 197.0, 197.0, 199.0, 200.0, 201.0, 202.0, 200.0, 197.0, 197.0, 198.0, 198.0, 198.0, 198.0, 198.0, 200.0, 202.0, 201.0, 202.0, 202.0, 176.0, 122.0, 70.0, 21.0], "node_sockstat_TCP_inuse": [12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 188.0, 188.0, 188.0, 188.0, 188.0, 188.0, 188.0, 188.0, 190.0, 190.0, 190.0, 190.0, 188.0, 188.0, 188.0, 189.0, 189.0, 189.0, 189.0, 189.0, 191.0, 192.0, 190.0, 190.0, 190.0, 167.0, 113.0, 61.0, 12.0]}}}, "1": {"start_time": "1697304127", "end_time": "1697304187", "start_timestamp": "2023-10-15 01:22:07", "end_timestamp": "2023-10-15 01:23:07", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:cpu_usage[ins=] = 0.98 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.98"}, "startsAt": "2023-10-15T01:22:49.467858611Z", "endsAt": "2023-10-15T01:23:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Acpu_usage_1m+%3E+0.7&g0.tab=1", "fingerprint": "b97dc2cd4ed607f5"}], "groupLabels": {"alertname": "NodeCpuHigh"}, "commonLabels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:cpu_usage[ins=] = 0.98 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.98"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeCpuHigh\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent updates"], "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 161\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 90\n    \n    # Number of rows to insert\n    num_rows = 364\n    \n    # Size of each column (in characters)\n    column_size = 68\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In a database for an online store, 161 users simultaneously attempt to perform frequent update operations in a database table containing 90 columns and 364 rows of product records. Each product record has a column size of 68 characters. These users compete with each other to lock the database table for updates, causing contention and potentially triggering a database exception.\n", "workload": {"update table1 set name[i]=(SELECT substr(md5(random()::text), 1, 68)) where id =[j]` (where i is a random number between 0 and 89, both inclusive, and j is a random number between 1 and 363, both inclusive)": 161}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 4.0, 9.0, 10.0, 8.0, 4.0, 6.0, 8.0, 5.0, 5.0, 8.0, 7.0, 7.0, 8.0, 2.0, 10.0, 10.0, 8.0, 3.0, 6.0, 8.0], "node_procs_blocked": [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0], "node_entropy_available_bits": [3498.0, 3542.0, 3629.0, 3716.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 0.0, 1365.3333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003333333491658171, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [1489420288.0, 1489428480.0, 1489428480.0, 1489444864.0, 1489444864.0, 1489436672.0, 1489453056.0, 1489453056.0, 1489453056.0, 1489453056.0, 1489469440.0, 1489469440.0, 1489469440.0, 1489485824.0, 1489485824.0, 1489485824.0, 1489502208.0, 1489502208.0, 1489502208.0, 1489502208.0, 1489518592.0]}, "network": {"node_sockstat_TCP_tw": [8.0, 9.0, 9.0, 4.0, 4.0, 8.0, 9.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 7.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 2.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 27.0, 27.0, 27.0, 27.0, 25.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0], "node_sockstat_TCP_inuse": [13.0, 18.0, 18.0, 18.0, 18.0, 16.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0]}}}, "2": {"start_time": "1697297388", "end_time": "1697297503", "start_timestamp": "2023-10-14 23:29:48", "end_timestamp": "2023-10-14 23:31:43", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "startsAt": "2023-10-14T23:29:49.467858611Z", "endsAt": "2023-10-15T00:26:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 57\n    \n    # Number of rows to insert\n    num_rows = 799006\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In a digital marketing company, the database contains 57 columns and 799,006 rows of customer information. Each column has a size of 56 characters. Initially, a large number of indexes are created for customer attributes such as name, age, and gender. Then, nine users simultaneously perform queries on the customer data, and the indexes are deleted after the queries are completed. This simulates the additional storage and performance impact caused by redundant indexes.\n", "workload": {"CREATE INDEX index_table1_0 ON table1(name0);": 1, "CREATE INDEX index_table1_1 ON table1(name1);": 1, "CREATE INDEX index_table1_2 ON table1(name2);": 1, "CREATE INDEX index_table1_3 ON table1(name3);": 1, "CREATE INDEX index_table1_4 ON table1(name4);": 1, "CREATE INDEX index_table1_5 ON table1(name5);": 1, "CREATE INDEX index_table1_id ON table1(id);": 1, "INSERT INTO table1 SELECT generate_series(1,799006),(SELECT substr(md5(random()::text), 1, 56), ... , now();": 1, "UPDATE table1 SET nameN=(SELECT substr(md5(random()::text), 1, 56) WHERE id =M;": 9}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 2.0, 7.0, 1.0, 3.0, 4.0, 5.0, 5.0, 7.0, 9.0, 6.0, 3.0, 4.0, 9.0, 3.0, 6.0, 5.0, 1.0, 16.0, 8.0, 7.0, 8.0, 12.0, 2.0, 3.0, 8.0, 2.0, 1.0, 17.0, 1.0, 7.0, 3.0, 1.0, 6.0, 6.0, 5.0, 8.0, 15.0], "node_procs_blocked": [0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 7.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3500.0, 3505.0, 3512.0, 3517.0, 3523.0, 3528.0, 3532.0, 3537.0, 3612.0, 3663.0, 3727.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [12.666666666666666, 0.0, 8.0, 2.0, 13.333333333333334, 5.333333333333333, 304.0, 0.6666666666666666, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 3.0, 0.0, 599.3333333333334, 10178.666666666666, 2960.3333333333335, 978.0, 538.0, 364.3333333333333, 90.33333333333333, 92.33333333333333, 955.3333333333334, 1650.3333333333333, 2002.0, 136.33333333333334, 79.66666666666667, 231.33333333333334, 18.666666666666668, 153.66666666666666, 124.0, 49.0, 13.666666666666666, 45.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [132437.33333333334, 0.0, 32768.0, 8192.0, 72362.66666666667, 62805.333333333336, 4501504.0, 2730.6666666666665, 1365.3333333333333, 1365.3333333333333, 1365.3333333333333, 1365.3333333333333, 1365.3333333333333, 0.0, 2730.6666666666665, 0.0, 4096.0, 12288.0, 0.0, 8385877.333333333, 133707093.33333333, 25123498.666666668, 8075946.666666667, 5025792.0, 3840682.6666666665, 756394.6666666666, 686762.6666666666, 8050005.333333333, 14834346.666666666, 16657066.666666666, 1148245.3333333333, 670378.6666666666, 5040810.666666667, 109226.66666666667, 802816.0, 1241088.0, 1033557.3333333334, 107861.33333333333, 705877.3333333334], "irate(node_disk_read_time_seconds_total": [0.045999999972991645, 0.0, 0.0016666666682188709, 0.0066666666728754835, 0.033000000015211604, 0.023000000005898375, 0.4609999999714394, 0.004666666694295903, 0.0, 0.004666666655490796, 0.0, 0.004666666655490796, 0.0, 0.0, 0.003999999995964269, 0.0, 0.004666666694295903, 0.0, 0.0, 0.30633333333147067, 15.914333333338922, 3.248666666642142, 2.3263333333500973, 1.8656666666502133, 0.5650000000217309, 0.6509999999931703, 0.2989999999990687, 6.897666666656733, 9.868333333327124, 1.5496666666585952, 0.34300000003228587, 0.20833333333333334, 0.4693333333125338, 0.07466666668187827, 0.7996666666585952, 0.13333333334109435, 0.4600000000015522, 0.01866666666076829, 0.04833333333954215]}, "memory": {"node_memory_Inactive_anon_bytes": [211820544.0, 211820544.0, 211824640.0, 211824640.0, 209211392.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209301504.0, 209088512.0, 209088512.0, 209104896.0, 209104896.0, 209104896.0, 209104896.0, 209104896.0, 209121280.0, 209108992.0, 209342464.0, 209342464.0, 209342464.0, 209526784.0, 209526784.0, 209526784.0, 209592320.0, 209592320.0, 209592320.0, 209596416.0, 209596416.0, 209596416.0]}, "network": {"node_sockstat_TCP_tw": [9.0, 10.0, 10.0, 10.0, 8.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0, 4.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 10.0, 10.0, 11.0, 11.0, 11.0, 9.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 9.0, 9.0, 8.0, 9.0, 10.0, 10.0, 10.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 3.3333333333333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 21.0, 22.0, 22.0, 21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 13.0, 13.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0]}}}, "3": {"start_time": "1697300907", "end_time": "1697301022", "start_timestamp": "2023-10-15 00:28:27", "end_timestamp": "2023-10-15 00:30:22", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.59 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.59"}, "startsAt": "2023-10-15T00:28:49.467858611Z", "endsAt": "2023-10-15T00:45:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.59 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.59"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 76\n    \n    # Number of rows to insert\n    num_rows = 733097\n    \n    # Size of each column (in characters)\n    column_size = 68\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In a large online database, where there are 5 users searching for information in a table with 76 columns and 733,097 rows, each column having a size of 68 characters, the database has created redundant indexes for various attributes such as product name, category, and price range. This can lead to unnecessary storage and performance overhead.\n", "workload": {"CREATE INDEX index_table1_0 ON table1(name0);": 1, "CREATE INDEX index_table1_1 ON table1(name1);": 1, "CREATE INDEX index_table1_2 ON table1(name2);": 1, "CREATE INDEX index_table1_3 ON table1(name3);": 1, "CREATE INDEX index_table1_4 ON table1(name4);": 1, "CREATE INDEX index_table1_5 ON table1(name5);": 1, "CREATE INDEX index_table1_id ON table1(id);": 1, "INSERT INTO table1 SELECT generate_series(1,799006),(SELECT substr(md5(random()::text), 1, 56), ... , now();": 1, "UPDATE table1 SET nameN=(SELECT substr(md5(random()::text), 1, 56) WHERE id =M;": 9}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 1.0, 5.0, 5.0, 5.0, 4.0, 1.0, 4.0, 6.0, 5.0, 6.0, 4.0, 3.0, 8.0, 4.0, 4.0, 6.0, 1.0, 2.0, 9.0, 9.0, 8.0, 15.0, 5.0, 2.0, 1.0, 8.0, 12.0, 9.0, 6.0, 2.0, 1.0, 13.0, 2.0, 2.0, 3.0, 13.0, 11.0], "node_procs_blocked": [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [41.333333333333336, 3.3333333333333335, 58.666666666666664, 0.0, 1.0, 3.3333333333333335, 0.0, 0.0, 0.3333333333333333, 150.66666666666666, 1.3333333333333333, 0.6666666666666666, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 24.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 87.66666666666667, 2.6666666666666665, 0.0, 0.6666666666666666, 3.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [173397.33333333334, 13653.333333333334, 973482.6666666666, 0.0, 4096.0, 34133.333333333336, 0.0, 0.0, 2730.6666666666665, 682666.6666666666, 86016.0, 2730.6666666666665, 0.0, 0.0, 12288.0, 0.0, 0.0, 0.0, 5461.333333333333, 0.0, 125610.66666666667, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 1365.3333333333333, 0.0, 1365.3333333333333, 0.0, 0.0, 0.0, 0.0, 1310720.0, 10922.666666666666, 0.0, 13653.333333333334, 53248.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.010000000009313226, 0.001000000008692344, 0.026333333342336118, 0.0, 0.0033333333364377418, 0.0009999999698872368, 0.0, 0.0, 0.0, 0.03333333336437742, 0.0006666666595265269, 0.0, 0.0, 0.0, 0.0006666666595265269, 0.0, 0.0, 0.0, 0.0003333333491658171, 0.0, 0.00599999997454385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0006666666595265269, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06300000000434618, 0.007000000022041301, 0.0, 0.00033333331036070984, 0.001000000008692344, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [2703360000.0, 2703360000.0, 2704273408.0, 2704273408.0, 2704273408.0, 1864830976.0, 1864732672.0, 1864167424.0, 1864155136.0, 1864118272.0, 1864093696.0, 1864048640.0, 1864028160.0, 1863999488.0, 1863942144.0, 1863925760.0, 1863827456.0, 1863798784.0, 1846599680.0, 1846599680.0, 1846607872.0, 1846616064.0, 1846624256.0, 1846640640.0, 1846648832.0, 1846673408.0, 1846673408.0, 1843068928.0, 1843068928.0, 1843068928.0, 1843068928.0, 1843068928.0, 1843068928.0, 1843068928.0, 1843085312.0, 1843085312.0, 1843085312.0, 1843085312.0, 1843085312.0]}, "network": {"node_sockstat_TCP_tw": [148.0, 148.0, 10.0, 10.0, 11.0, 11.0, 11.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, 5.0, 6.0, 6.0, 6.0, 5.0, 5.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 3.3333333333333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 21.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0]}}}, "4": {"start_time": "1697298427", "end_time": "1697298498", "start_timestamp": "2023-10-14 23:47:07", "end_timestamp": "2023-10-14 23:48:18", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.01 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.01"}, "startsAt": "2023-10-14T23:47:49.467858611Z", "endsAt": "2023-10-15T00:15:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.01 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.01"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["highly deletes"], "command": "python anomaly_trigger/main.py --anomaly VACUUM", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 177\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 11\n    \n    # Number of rows to insert\n    num_rows = 3025995\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In the database of an online store, 177 users are simultaneously searching a database table that contains 11 columns, 3,025,995 rows, and each column size is 78 characters. The search operation is followed by a vacuum operation on the database table, which aims to optimize the table's performance and storage space utilization. This scenario simulates the potential exception that could occur during this process.\n", "workload": {"delete from table1 where id < 2420796;": 1, "select * from table1 where id=?": 177}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 4.0, 8.0, 1.0, 8.0, 1.0, 3.0, 4.0, 3.0, 4.0, 2.0, 7.0, 1.0, 3.0, 4.0, 2.0, 4.0, 4.0, 2.0, 1.0, 1.0, 76.0, 186.0, 183.0], "node_procs_blocked": [0.0, 0.0, 0.0, 3.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 1.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [196456448.0, 196456448.0, 196456448.0, 196456448.0, 196456448.0, 359624704.0, 602722304.0, 774520832.0, 1033433088.0, 1261199360.0, 1426411520.0, 1669079040.0, 1833459712.0, 1833607168.0, 1833615360.0, 1833615360.0, 1612111872.0, 1612111872.0, 1612111872.0, 188690432.0, 188551168.0, 208396288.0, 256581632.0, 256684032.0]}, "network": {"node_sockstat_TCP_tw": [11.0, 11.0, 11.0, 6.0, 7.0, 10.0, 10.0, 10.0, 9.0, 10.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 9.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 3.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 59.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 22.0, 23.0, 23.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 21.0, 198.0, 198.0, 198.0], "node_sockstat_TCP_inuse": [12.0, 13.0, 14.0, 14.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 189.0, 189.0, 189.0]}}}, "5": {"start_time": "1697300773", "end_time": "1697300847", "start_timestamp": "2023-10-15 00:26:13", "end_timestamp": "2023-10-15 00:27:27", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "startsAt": "2023-10-15T00:26:49.467858611Z", "endsAt": "2023-10-15T00:34:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["highly deletes"], "command": "python anomaly_trigger/main.py --anomaly VACUUM", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 136\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 16\n    \n    # Number of rows to insert\n    num_rows = 2211862\n    \n    # Size of each column (in characters)\n    column_size = 83\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In an online store's database with a table containing 16 columns and 2,211,862 rows of product records, each with a column size of 83 characters, 136 users simultaneously perform a search after a large-scale data cleaning operation. This simulates the scenario of users searching for products using various filters like product name, category, and price range, after performing a vacuum operation on the database table.\n", "workload": {"delete from table1 where id < 1769489;": 1, "select * from table1 where id=?;": 136}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [2.0, 3.0, 5.0, 9.0, 2.0, 5.0, 4.0, 4.0, 1.0, 2.0, 7.0, 1.0, 5.0, 1.0, 2.0, 5.0, 5.0, 2.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 147.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 5.0, 4.0, 0.0, 2.0, 2.0, 5.0, 1.0, 5.0, 1.0, 0.0, 0.0, 1.0], "node_entropy_available_bits": [3499.0, 3504.0, 3528.0, 3549.0, 3569.0, 3588.0, 3609.0, 3630.0, 3650.0, 3671.0, 3692.0, 3712.0, 3735.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 66.0, 1.0, 241.66666666666666, 94.33333333333333, 463.0, 175.0, 267.3333333333333, 562.0, 423.0, 384.6666666666667, 338.6666666666667, 211.0, 366.3333333333333, 212.0, 478.0, 345.0, 367.3333333333333, 353.6666666666667, 1344.3333333333333], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 1365.3333333333333, 0.0, 0.0, 0.0, 1365.3333333333333, 936618.6666666666, 4096.0, 8645290.666666666, 1881429.3333333333, 15874730.666666666, 4143786.6666666665, 8496469.333333334, 14613162.666666666, 10410666.666666666, 8163328.0, 10627754.666666666, 5839530.666666667, 15365461.333333334, 5524138.666666667, 17485824.0, 10017450.666666666, 11672234.666666666, 4591616.0, 51630080.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0003333333491658171, 0.0, 0.0, 0.0, 0.0, 0.8576666666582847, 0.02999999998913457, 3.6903333333320916, 2.0873333333292976, 8.070333333336748, 3.3299999999968954, 4.805000000012417, 7.695333333336748, 9.246666666663563, 2.5569999999909974, 8.7390000000208, 5.407666666666046, 12.65266666666139, 4.514666666664804, 13.077333333319984, 7.5626666666939855, 2.922999999990376, 0.311666666646488, 3.35333333335196]}, "memory": {}, "network": {"node_sockstat_TCP_tw": [10.0, 11.0, 11.0, 11.0, 11.0, 10.0, 11.0, 11.0, 10.0, 10.0, 11.0, 11.0, 11.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 9.0, 9.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 45.333333333333336], "node_sockstat_TCP_alloc": [21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 21.0, 157.0], "node_sockstat_TCP_inuse": [12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 148.0]}}}, "6": {"start_time": "1697321865", "end_time": "1697321978", "start_timestamp": "2023-10-15 06:17:45", "end_timestamp": "2023-10-15 06:19:38", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.16 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.16"}, "startsAt": "2023-10-15T06:17:49.467858611Z", "endsAt": "2023-10-15T06:24:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.16 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.16"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}, {"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.49 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.49"}, "startsAt": "2023-10-15T06:18:49.467858611Z", "endsAt": "2023-10-15T06:35:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.49 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.49"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 6\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 79\n    \n    # Number of rows to insert\n    num_rows = 990412\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In an online platform database, a search operation is performed by 6 users simultaneously on a table with 79 columns and 990,412 rows, with each column containing 58 characters. This dataset has a large number of redundant indexes, which can result in additional storage space and performance overhead.\n", "workload": {"CREATE INDEX index_table1_0 ON table1(name0);": 1, "CREATE INDEX index_table1_1 ON table1(name1);": 1, "CREATE INDEX index_table1_2 ON table1(name2);": 1, "CREATE INDEX index_table1_3 ON table1(name3);": 1, "CREATE INDEX index_table1_4 ON table1(name4);": 1, "CREATE INDEX index_table1_5 ON table1(name5);": 1, "CREATE INDEX index_table1_id ON table1(id);": 1, "INSERT INTO table1 SELECT generate_series(1,799006),(SELECT substr(md5(random()::text), 1, 56), ... , now();": 1, "UPDATE table1 SET nameN=(SELECT substr(md5(random()::text), 1, 56) WHERE id =M;": 9}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 3.0, 4.0, 5.0, 4.0, 6.0, 5.0, 2.0, 4.0, 5.0, 6.0, 8.0, 7.0, 3.0, 6.0, 6.0, 1.0, 1.0, 6.0, 12.0, 11.0, 8.0, 3.0, 16.0, 3.0, 1.0, 13.0, 11.0, 16.0, 15.0, 1.0, 14.0, 8.0, 1.0, 12.0, 3.0, 5.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0], "node_load1": [14.32, 13.18, 13.18, 12.2, 11.47, 11.47, 10.71, 10.71, 10.01, 9.69, 9.69, 9.47, 9.47, 9.28, 8.85, 8.85, 8.7, 8.7, 8.01, 7.93, 7.93, 7.93, 7.93, 7.86, 8.03, 8.03, 7.71, 7.71, 7.81, 8.07, 8.07, 8.38, 8.38, 7.79, 7.49, 7.49, 7.13, 7.13]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 52.0, 0.3333333333333333, 14.333333333333334, 0.0, 1.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.6666666666666666, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 38.0, 0.6666666666666666, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.6666666666666667, 0.0, 0.0, 3.3333333333333335, 3.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.6666666666666666, 0.6666666666666666], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 947541.3333333334, 1365.3333333333333, 79189.33333333333, 0.0, 46421.333333333336, 1365.3333333333333, 2730.6666666666665, 1365.3333333333333, 1365.3333333333333, 2730.6666666666665, 0.0, 1365.3333333333333, 0.0, 1365.3333333333333, 159744.0, 5461.333333333333, 0.0, 35498.666666666664, 0.0, 0.0, 0.0, 0.0, 4096.0, 0.0, 0.0, 6826.666666666667, 0.0, 0.0, 13653.333333333334, 12288.0, 1365.3333333333333, 0.0, 0.0, 2730.6666666666665, 13653.333333333334, 2730.6666666666665], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.016666666643383603, 0.0, 0.01266666668622444, 0.0, 0.049333333309429385, 0.002000000017384688, 0.0, 0.004333333345130086, 0.0, 0.004666666655490796, 0.0, 0.003999999995964269, 0.0, 0.003999999995964269, 0.011666666677532097, 0.0, 0.0, 0.002333333327745398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003333333491658171, 0.0, 0.0, 0.0013333333190530539, 0.019666666669460636, 0.0, 0.0, 0.0, 0.006000000013348957, 0.00033333331036070984, 0.0003333333491658171]}, "memory": {"node_memory_Inactive_anon_bytes": [2612658176.0, 2612658176.0, 2613563392.0, 2613563392.0, 1776087040.0, 1774546944.0, 1773932544.0, 1773903872.0, 1773883392.0, 1773854720.0, 1773817856.0, 1773776896.0, 1773699072.0, 1773686784.0, 1773670400.0, 1773621248.0, 1773531136.0, 1756471296.0, 1756344320.0, 1756352512.0, 1756368896.0, 1756368896.0, 1756377088.0, 1756393472.0, 1756401664.0, 1756401664.0, 1752829952.0, 1752829952.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752846336.0, 1752862720.0]}, "network": {"node_sockstat_TCP_tw": [15.0, 16.0, 10.0, 10.0, 9.0, 9.0, 11.0, 11.0, 11.0, 10.0, 10.0, 11.0, 7.0, 11.0, 10.0, 10.0, 11.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 7.0, 7.0, 8.0, 8.0, 8.0, 7.0, 7.0, 8.0, 4.0, 6.0, 5.0, 5.0, 6.0, 6.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 3.3333333333333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 22.0, 22.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0], "node_sockstat_TCP_inuse": [13.0, 13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 13.0, 13.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0]}}}, "7": {"start_time": "1697334343", "end_time": "1697334414", "start_timestamp": "2023-10-15 09:45:43", "end_timestamp": "2023-10-15 09:46:54", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "PostgresDown", "category": "pgsql", "instance": "172.27.58.65:9187", "job": "postgres", "level": "0", "severity": "CRIT"}, "annotations": {"description": "pg_up[ins=, instance=172.27.58.65:9187] = 0 < 1\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "CRIT PostgresDown @172.27.58.65:9187"}, "startsAt": "2023-10-15T09:46:43.277542164Z", "endsAt": "2023-10-15T09:49:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_up+%3C+1&g0.tab=1", "fingerprint": "a64caa7394533ecc"}, {"status": "resolved", "labels": {"alertname": "PostgresDown", "category": "pgsql", "instance": "172.27.58.65:9630", "job": "postgres", "level": "0", "severity": "CRIT"}, "annotations": {"description": "pg_up[ins=, instance=172.27.58.65:9630] = 0 < 1\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "CRIT PostgresDown @172.27.58.65:9630"}, "startsAt": "2023-10-15T09:46:43.277542164Z", "endsAt": "2023-10-15T09:49:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_up+%3C+1&g0.tab=1", "fingerprint": "4bc8049bdbeb685b"}], "groupLabels": {"alertname": "PostgresDown"}, "commonLabels": {"alertname": "PostgresDown", "category": "pgsql", "job": "postgres", "level": "0", "severity": "CRIT"}, "commonAnnotations": {}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"PostgresDown\"}", "truncatedAlerts": 0}, {"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "PostgresDown", "category": "pgsql", "instance": "172.27.58.65:9187", "job": "postgres", "level": "0", "severity": "CRIT"}, "annotations": {"description": "pg_up[ins=, instance=172.27.58.65:9187] = 0 < 1\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "CRIT PostgresDown @172.27.58.65:9187"}, "startsAt": "2023-10-15T09:46:43.277542164Z", "endsAt": "2023-10-15T09:49:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_up+%3C+1&g0.tab=1", "fingerprint": "a64caa7394533ecc"}, {"status": "resolved", "labels": {"alertname": "PostgresDown", "category": "pgsql", "instance": "172.27.58.65:9630", "job": "postgres", "level": "0", "severity": "CRIT"}, "annotations": {"description": "pg_up[ins=, instance=172.27.58.65:9630] = 0 < 1\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "CRIT PostgresDown @172.27.58.65:9630"}, "startsAt": "2023-10-15T09:46:43.277542164Z", "endsAt": "2023-10-15T09:49:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_up+%3C+1&g0.tab=1", "fingerprint": "4bc8049bdbeb685b"}], "groupLabels": {"alertname": "PostgresDown"}, "commonLabels": {"alertname": "PostgresDown", "category": "pgsql", "job": "postgres", "level": "0", "severity": "CRIT"}, "commonAnnotations": {}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"PostgresDown\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent commits or highly concurrent inserts"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 62\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 22\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 52\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In an IoT application, 62 sensors generate a large amount of data to be inserted into a database simultaneously. The database table contains 22 columns and 58 rows, with each column having a size of 52 characters. This process simulates a database exception caused by the insertion of large data.\n", "workload": {"insert into table1 select generate_series(1,58),(SELECT substr(md5(random()::text), 1, 52)), now();": 62}, "slow_queries": ["insert into aa select generate_series(1,400000), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), now();"], "exceptions": {"cpu": {"node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 18.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 10.666666666666666, 0.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 0.0, 0.0, 12288.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 0.0, 727722.6666666666, 0.0, 0.0, 1365.3333333333333, 0.0, 0.0, 1365.3333333333333, 0.0, 686762.6666666666, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0016666666294137638, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333333364377418, 0.0, 0.09133333340287209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0026666666381061077, 0.0, 0.03133333334699273, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [248946688.0, 248946688.0, 248946688.0, 248946688.0, 248963072.0, 248963072.0, 248963072.0, 248979456.0, 248979456.0, 248979456.0, 248995840.0, 248995840.0, 248995840.0, 249012224.0, 249012224.0, 249028608.0, 249028608.0, 249028608.0, 249044992.0, 249044992.0, 249044992.0, 249036800.0, 249053184.0, 248950784.0]}, "network": {"node_sockstat_TCP_tw": [7.0, 8.0, 8.0, 7.0, 7.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 9.0, 9.0, 8.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 5.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 20.666666666666668, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 22.0, 22.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 84.0, 22.0], "node_sockstat_TCP_inuse": [13.0, 13.0, 13.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 13.0]}}}, "8": {"start_time": "1697343581", "end_time": "1697343653", "start_timestamp": "2023-10-15 12:19:41", "end_timestamp": "2023-10-15 12:20:53", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "PostgresIdleInXact", "category": "pgsql", "datname": "tpcc", "instance": "172.27.58.65:9630", "job": "postgres", "level": "2", "severity": "INFO"}, "annotations": {"description": "pg:db:ixact_backends[ins=, instance=172.27.58.65:9630, datname=tpcc] = 24 > 1\n", "summary": "Info PostgresIdleInXact: @172.27.58.65:9630 [tpcc]"}, "startsAt": "2023-10-15T12:19:43.277542164Z", "endsAt": "2023-10-15T12:20:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg%3Adb%3Aixact_backends+%3E+1&g0.tab=1", "fingerprint": "c95ba0f99eaf983c"}], "groupLabels": {"alertname": "PostgresIdleInXact"}, "commonLabels": {"alertname": "PostgresIdleInXact", "category": "pgsql", "datname": "tpcc", "instance": "172.27.58.65:9630", "job": "postgres", "level": "2", "severity": "INFO"}, "commonAnnotations": {"description": "pg:db:ixact_backends[ins=, instance=172.27.58.65:9630, datname=tpcc] = 24 > 1\n", "summary": "Info PostgresIdleInXact: @172.27.58.65:9630 [tpcc]"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"PostgresIdleInXact\"}", "truncatedAlerts": 0}, {"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:cpu_usage[ins=] = 0.98 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.98"}, "startsAt": "2023-10-15T12:19:49.467858611Z", "endsAt": "2023-10-15T12:21:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Acpu_usage_1m+%3E+0.7&g0.tab=1", "fingerprint": "b97dc2cd4ed607f5"}], "groupLabels": {"alertname": "NodeCpuHigh"}, "commonLabels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:cpu_usage[ins=] = 0.98 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.98"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeCpuHigh\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent commits or highly concurrent inserts"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 182\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 11\n    \n    # Number of rows to insert\n    num_rows = 50\n    \n    # Size of each column (in characters)\n    column_size = 26\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In a data analytics platform, 182 users are simultaneously inserting a large amount of data into a database table. The table has 11 columns, each with a size of 26 characters, and contains 50 rows of data. This process is designed to simulate the database exception that can occur when multiple users are trying to insert a large volume of data at the same time.\n", "workload": {"insert into table1 select generate_series(1,50),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)),(SELECT substr(md5(random()::text), 1, 26)), now();": 182}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], "node_entropy_available_bits": [3501.0, 3516.0, 3516.0, 3548.0, 3634.0, 3721.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 166.33333333333334, 0.0, 3.6666666666666665, 0.0, 0.0, 1.3333333333333333, 0.0, 289.3333333333333, 0.0, 0.0, 0.0, 13.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 2.3333333333333335, 0.0, 0.0, 0.0, 41.666666666666664, 0.0, 21.666666666666668], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 3967658.6666666665, 0.0, 15018.666666666666, 0.0, 0.0, 46421.333333333336, 0.0, 9961472.0, 0.0, 0.0, 0.0, 402773.3333333333, 1365.3333333333333, 0.0, 0.0, 0.0, 2730.6666666666665, 54613.333333333336, 0.0, 0.0, 0.0, 898389.3333333334, 0.0, 1268394.6666666667], "irate(node_disk_read_time_seconds_total": [0.0, 0.26633333329421777, 0.0, 0.0003333333879709244, 0.0, 0.0, 0.0006666666207214197, 0.0, 1.251666666707024, 0.0, 0.0, 0.0, 0.0283333333209157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016666666294137638, 0.0, 0.0, 0.0, 0.09633333336872359, 0.0, 0.12099999996523063]}, "memory": {}, "network": {"node_sockstat_TCP_tw": [12.0, 12.0, 11.0, 11.0, 12.0, 12.0, 12.0, 11.0, 11.0, 11.0, 11.0, 11.0, 3.0, 4.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 9.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 1.3333333333333333, 0.0, 60.666666666666664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 21.0, 21.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 203.0, 139.0, 21.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 12.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 194.0, 130.0, 12.0]}}}, "9": {"start_time": "1697346047", "end_time": "1697346118", "start_timestamp": "2023-10-15 13:00:47", "end_timestamp": "2023-10-15 13:01:58", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:cpu_usage[ins=] = 0.71 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.71"}, "startsAt": "2023-10-15T13:01:49.467858611Z", "endsAt": "2023-10-15T13:02:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Acpu_usage_1m+%3E+0.7&g0.tab=1", "fingerprint": "b97dc2cd4ed607f5"}], "groupLabels": {"alertname": "NodeCpuHigh"}, "commonLabels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:cpu_usage[ins=] = 0.71 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.71"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeCpuHigh\"}", "truncatedAlerts": 0}, {"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 2.57 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 2.57"}, "startsAt": "2023-10-15T13:00:49.467858611Z", "endsAt": "2023-10-15T13:03:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 2.57 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 2.57"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent commits or highly concurrent inserts"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 97\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 20\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 58\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In an internet of things (IoT) application, with 97 sensors generating a large amount of data, there is an attempt to simultaneously insert this data into a database table. The table has 20 columns and 68 rows, with each column having a size of 58 characters. This process aims to simulate the database exception caused by inserting a large volume of data.\n", "workload": {"INSERT INTO table1 SELECT generate_series(1,68), (SELECT substr(md5(random()::text), 1, 70)), now();": 97}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0], "node_entropy_available_bits": [3504.0, 3504.0, 3505.0, 3533.0, 3599.0, 3664.0, 3729.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.666666666666664, 0.0, 0.0, 0.3333333333333333, 10.333333333333334, 0.0, 3.3333333333333335, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 15.666666666666666, 2.6666666666666665, 40.666666666666664, 2.0, 10.666666666666666], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 653994.6666666666, 0.0, 0.0, 1365.3333333333333, 42325.333333333336, 0.0, 83285.33333333333, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 0.0, 204800.0, 10922.666666666666, 888832.0, 53248.0, 43690.666666666664], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03966666664928198, 0.0, 0.0, 0.002000000017384688, 0.009333333310981592, 0.0, 0.0016666666294137638, 0.0, 0.0, 0.0, 0.0, 0.0006666666983316342, 0.0, 0.02200000003601114, 0.0019999999397744737, 0.04033333334761361, 0.001000000008692344, 0.012000000026697913]}, "memory": {"node_memory_Inactive_anon_bytes": [234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234328064.0, 234323968.0]}, "network": {"node_sockstat_TCP_tw": [7.0, 7.0, 6.0, 7.0, 7.0, 6.0, 6.0, 6.0, 8.0, 8.0, 6.0, 6.0, 10.0, 11.0, 11.0, 11.0, 11.0, 9.0, 9.0, 10.0, 10.0, 10.0, 9.0, 10.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 32.333333333333336, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [23.0, 23.0, 23.0, 120.0, 120.0, 121.0, 121.0, 121.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 23.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 12.0, 109.0, 109.0, 110.0, 110.0, 110.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 109.0, 12.0]}}}, "10": {"start_time": "1697355392", "end_time": "1697355463", "start_timestamp": "2023-10-15 15:36:32", "end_timestamp": "2023-10-15 15:37:43", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeFsSpaceFull", "category": "node", "device": "/dev/vda1", "fstype": "ext4", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "mountpoint": "/", "severity": "WARN"}, "annotations": {"description": "node:fs:space_usage[ins=] = 1.00 > 90%\n", "summary": "WARN NodeFsSpaceFull @172.27.58.65:9100 1.00"}, "startsAt": "2023-10-15T15:36:49.467858611Z", "endsAt": "2023-10-15T15:56:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Afs%3Aspace_usage+%3E+0.9&g0.tab=1", "fingerprint": "ae3e75825050fe68"}], "groupLabels": {"alertname": "NodeFsSpaceFull"}, "commonLabels": {"alertname": "NodeFsSpaceFull", "category": "node", "device": "/dev/vda1", "fstype": "ext4", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "mountpoint": "/", "severity": "WARN"}, "commonAnnotations": {"description": "node:fs:space_usage[ins=] = 1.00 > 90%\n", "summary": "WARN NodeFsSpaceFull @172.27.58.65:9100 1.00"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeFsSpaceFull\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent commits or highly concurrent inserts"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 53\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 32\n    \n    # Number of rows to insert\n    num_rows = 58\n    \n    # Size of each column (in characters)\n    column_size = 96\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In a data-intensive application, such as a scientific research project, 53 sensors are generating a large amount of data that needs to be inserted into a database table. The table consists of 32 columns, each with a size of 96 characters, and there are 58 rows of data. Simulate the database exception caused by this process.\n", "workload": {"insert into table1 select generate_series(1, 58), (SELECT substr(md5(random()::text), 1, 96)), (SELECT substr(md5(random()::text), 1, 96)), ..., (SELECT substr(md5(random()::text), 1, 96)), now();": 53}, "slow_queries": [], "exceptions": {"cpu": {"node_procs_blocked": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3538.0, 3588.0, 3638.0, 3688.0, 3737.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [80.33333333333333, 0.0, 10.333333333333334, 0.0, 0.0, 20.0, 12.333333333333334, 0.0, 0.0, 0.0, 0.0, 116.33333333333333, 0.0, 8.333333333333334, 27.333333333333332, 0.0, 6.666666666666667, 5.333333333333333, 3.0, 1.6666666666666667, 3.0, 88.66666666666667, 22.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [329045.3333333333, 0.0, 667648.0, 0.0, 0.0, 316757.3333333333, 755029.3333333334, 0.0, 0.0, 0.0, 0.0, 476501.3333333333, 0.0, 176128.0, 1471829.3333333333, 0.0, 27306.666666666668, 21845.333333333332, 12288.0, 6826.666666666667, 57344.0, 641706.6666666666, 337237.3333333333, 0.0], "irate(node_disk_read_time_seconds_total": [0.018333333311602473, 0.0, 0.04566666670143604, 0.0, 0.0, 0.034333333295459546, 0.0400000000372529, 0.0, 0.0, 0.0, 0.0, 0.029000000019247334, 0.0, 0.007333333293596904, 0.14500000001862645, 0.0, 0.0033333333364377418, 0.009000000000620881, 0.006999999983236194, 0.004000000034769376, 0.001000000008692344, 0.15499999995032945, 0.03433333337306976, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [239902720.0, 239902720.0, 239902720.0, 239902720.0, 239902720.0, 239919104.0, 239919104.0, 239919104.0, 239919104.0, 239935488.0, 239935488.0, 239935488.0, 239951872.0, 239951872.0, 239951872.0, 239951872.0, 239968256.0, 239968256.0, 239968256.0, 239968256.0, 239984640.0, 239984640.0, 239976448.0, 239906816.0]}, "network": {"node_sockstat_TCP_tw": [15.0, 12.0, 12.0, 13.0, 16.0, 16.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 15.0, 14.0, 14.0, 14.0, 9.0, 13.0, 10.0, 10.0, 10.0, 9.0, 9.0, 7.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 17.666666666666668, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 22.0, 22.0, 76.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 75.0, 22.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 12.0, 66.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 65.0, 12.0]}}}, "11": {"start_time": "1697356198", "end_time": "1697356347", "start_timestamp": "2023-10-15 15:49:58", "end_timestamp": "2023-10-15 15:52:27", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "PostgresDown", "category": "pgsql", "instance": "172.27.58.65:9187", "job": "postgres", "level": "0", "severity": "CRIT"}, "annotations": {"description": "pg_up[ins=, instance=172.27.58.65:9187] = 0 < 1\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "CRIT PostgresDown @172.27.58.65:9187"}, "startsAt": "2023-10-15T15:51:43.277542164Z", "endsAt": "2023-10-15T15:57:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_up+%3C+1&g0.tab=1", "fingerprint": "a64caa7394533ecc"}, {"status": "resolved", "labels": {"alertname": "PostgresDown", "category": "pgsql", "instance": "172.27.58.65:9630", "job": "postgres", "level": "0", "severity": "CRIT"}, "annotations": {"description": "pg_up[ins=, instance=172.27.58.65:9630] = 0 < 1\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "CRIT PostgresDown @172.27.58.65:9630"}, "startsAt": "2023-10-15T15:49:43.277542164Z", "endsAt": "2023-10-15T15:57:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_up+%3C+1&g0.tab=1", "fingerprint": "4bc8049bdbeb685b"}], "groupLabels": {"alertname": "PostgresDown"}, "commonLabels": {"alertname": "PostgresDown", "category": "pgsql", "job": "postgres", "level": "0", "severity": "CRIT"}, "commonAnnotations": {}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"PostgresDown\"}", "truncatedAlerts": 0}], "labels": ["FETCH_LARGE_DATA", "CORRELATED SUBQUERY"], "command": "python anomaly_trigger/main.py --anomaly FETCH_LARGE_DATA,CORRELATED_SUBQUERY", "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_tpch_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='tpch',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '4.explain.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files):\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/tpch-queries/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1.explain.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n", "description": "In an e-commerce platform's database, there is a scenario where a large amount of data needs to be fetched, specifically the inventory for each product. In order to find the inventory, correlated subqueries are being executed. However, if these subqueries are not optimized, the performance of the inventory query may be negatively affected.\n", "workload": {"select o_orderpriority, count(*) as order_count from orders where o_orderdate >= date ':1' and o_orderdate < date ':1' + interval '3' month and exists ( select * from lineitem where l_orderkey = o_orderkey and l_commitdate < l_receiptdate ) group by o_orderpriority order by o_orderpriority;": 1}, "slow_queries": ["select o_orderpriority, count(*) as order_count from orders where o_orderdate >= date ':1' and o_orderdate < date ':1' + interval '3' month and exists ( select * from lineitem where l_orderkey = o_orderkey and l_commitdate < l_receiptdate ) group by o_orderpriority order by o_orderpriority;"], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 7.0, 1.0, 2.0, 6.0, 1.0, 1.0, 6.0, 1.0, 7.0, 4.0, 1.0, 1.0, 4.0, 2.0, 1.0, 5.0, 4.0, 2.0, 1.0, 4.0, 4.0, 4.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 7.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 4.0, 7.0, 3.0], "node_procs_blocked": [0.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 5.0, 3.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 1.0, 2.0, 3.0], "node_entropy_available_bits": [3521.0, 3542.0, 3563.0, 3584.0, 3605.0, 3626.0, 3646.0, 3667.0, 3688.0, 3709.0, 3730.0, 3750.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "1-(node_filesystem_free_bytes": [0.44185116479954456, 0.4418518786553881, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418518979487892, 0.4418519172421904, 0.4418519172421904, 0.4418519172421904, 0.4418519172421904, 0.4418519172421904, 0.4418522259366092, 0.4418522259366092, 0.44185255392442924, 0.4420300146284497, 0.4422998135505004, 0.44267101858913416, 0.44297384781399884, 0.44321462946068013, 0.4435009435341378, 0.4438855767799904, 0.4441504365913399, 0.4443885943354613, 0.4444977178125149, 0.44484191208950175, 0.445108161025736, 0.4453805838503466, 0.445701645339323, 0.4462682539450713, 0.4462682539450713, 0.4462682539450713, 0.4462682732384724, 0.4465053505521279], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_written_bytes_total": [6826.666666666667, 40960.0, 5461.333333333333, 32768.0, 42325.333333333336, 0.0, 101034.66666666667, 91477.33333333333, 6826.666666666667, 30037.333333333332, 9557.333333333334, 1365.3333333333333, 83285.33333333333, 0.0, 58709.333333333336, 0.0, 32768.0, 0.0, 43690.666666666664, 73728.0, 13653.333333333334, 51882.666666666664, 6826.666666666667, 0.0, 32768.0, 0.0, 38229.333333333336, 0.0, 1997482.6666666667, 5330261.333333333, 5380778.666666667, 5465429.333333333, 5484544.0, 5495466.666666667, 5252437.333333333, 5487274.666666667, 5337088.0, 5496832.0, 24174592.0, 48452949.333333336, 118699349.33333333, 5330261.333333333, 5520042.666666667, 5458602.666666667, 5337088.0, 5487274.666666667, 26356394.666666668, 43864064.0, 83974826.66666667, 5330261.333333333], "irate(node_disk_write_time_seconds_total": [0.0, 0.00033333331036070984, 0.02633333330353101, 0.02733333338983357, 0.05799999996088445, 0.0, 0.5220000000360111, 0.35766666661947966, 0.027333333312223356, 0.03233333335568508, 0.051999999986340605, 0.007000000060846408, 0.0826666666350017, 0.0, 0.10266666665362816, 0.0, 0.0400000000372529, 0.0, 0.07200000000496705, 0.4183333332960804, 0.06199999999565383, 0.251666666707024, 0.030666666648661096, 0.0, 0.060999999986961484, 0.0, 0.06999999998758237, 0.0, 2.014666666664804, 3.564333333323399, 3.9210000000117966, 3.9490000000223517, 7.160333333304152, 2.399000000053396, 4.998999999991308, 2.8493333333171904, 3.103999999972681, 2.771000000027319, 27.06966666667722, 63.058333333348855, 171.3739999999913, 2.936333333297322, 4.16366666664059, 3.983666666705782, 3.881999999983236, 4.2746666666741175, 34.55533333332278, 60.744666666646175, 131.72533333340348, 2.5826666666350016]}, "memory": {"irate(node_disk_write_time_seconds_total": [0.0, 0.00033333331036070984, 0.02633333330353101, 0.02733333338983357, 0.05799999996088445, 0.0, 0.5220000000360111, 0.35766666661947966, 0.027333333312223356, 0.03233333335568508, 0.051999999986340605, 0.007000000060846408, 0.0826666666350017, 0.0, 0.10266666665362816, 0.0, 0.0400000000372529, 0.0, 0.07200000000496705, 0.4183333332960804, 0.06199999999565383, 0.251666666707024, 0.030666666648661096, 0.0, 0.060999999986961484, 0.0, 0.06999999998758237, 0.0, 2.014666666664804, 3.564333333323399, 3.9210000000117966, 3.9490000000223517, 7.160333333304152, 2.399000000053396, 4.998999999991308, 2.8493333333171904, 3.103999999972681, 2.771000000027319, 27.06966666667722, 63.058333333348855, 171.3739999999913, 2.936333333297322, 4.16366666664059, 3.983666666705782, 3.881999999983236, 4.2746666666741175, 34.55533333332278, 60.744666666646175, 131.72533333340348, 2.5826666666350016], "node_memory_Inactive_anon_bytes": [232189952.0, 237711360.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237731840.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 237502464.0, 239599616.0, 239599616.0, 239599616.0, 239599616.0, 239599616.0, 239599616.0, 339472384.0, 962711552.0, 1518239744.0, 1957490688.0, 2504261632.0]}, "network": {"node_sockstat_TCP_tw": [7.0, 7.0, 6.0, 6.0, 6.0, 9.0, 9.0, 6.0, 9.0, 9.0, 9.0, 9.0, 8.0, 9.0, 10.0, 10.0, 10.0, 8.0, 9.0, 9.0, 12.0, 12.0, 12.0, 13.0, 13.0, 10.0, 10.0, 7.0, 10.0, 10.0, 10.0, 11.0, 9.0, 10.0, 10.0, 11.0, 11.0, 7.0, 8.0, 8.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0], "node_sockstat_TCP_alloc": [22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 24.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 21.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0], "node_sockstat_TCP_inuse": [12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 14.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 11.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0]}}}, "12": {"start_time": "1697356673", "end_time": "1697356733", "start_timestamp": "2023-10-15 15:57:53", "end_timestamp": "2023-10-15 15:58:53", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "PostgresRestart", "category": "pgsql", "instance": "172.27.58.65:9630", "job": "postgres", "level": "1", "severity": "WARN"}, "annotations": {"description": "pg_uptime[ins=, instance=172.27.58.65:9630] = 299.5 < 300\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "WARN PostgresRestart @172.27.58.65:9630"}, "startsAt": "2023-10-15T15:58:43.277542164Z", "endsAt": "2023-10-15T16:02:43.277542164Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=pg_uptime+%3C+300&g0.tab=1", "fingerprint": "c5d7d02ea90b96bd"}], "groupLabels": {"alertname": "PostgresRestart"}, "commonLabels": {"alertname": "PostgresRestart", "category": "pgsql", "instance": "172.27.58.65:9630", "job": "postgres", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "pg_uptime[ins=, instance=172.27.58.65:9630] = 299.5 < 300\nhttp://g.pigsty/d/pgsql-instance?var-ins=\n", "summary": "WARN PostgresRestart @172.27.58.65:9630"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"PostgresRestart\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent updates"], "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 186\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 52\n    \n    # Number of rows to insert\n    num_rows = 253\n    \n    # Size of each column (in characters)\n    column_size = 95\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In a multi-user online database environment, 186 users are simultaneously trying to update a database table containing 52 columns and 253 rows of data, with each column having a size of 95 characters. These users are competing with each other to lock the table for performing the update operation, which may cause a database exception due to lock contention.\n", "workload": "1. SQL Query: `insert into table1 select generate_series(1,253), (SELECT substr(md5(random()::text), 1, 95)), (SELECT substr(md5(random()::text), 1, 95)), ..., (SELECT substr(md5(random()::text), 1, 95)), now();`\n    Frequency Number (threads): 1 (Executed in the `lock_contention` function, not within any loop controlled by the thread count)\n\n2. SQL Query: `update table1 set nameX=(SELECT substr(md5(random()::text), 1, 95)) where id = Y`\n    Frequency Number (threads): 186 (Executed within a time-bounded loop in the `lock` function, which is called asynchronously by each of the 186 threads created in the `lock_contention` function; here X is a randomly selected column number between 0 and 51, and Y is a randomly selected row id between 1 and 252)\n", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 8.0, 5.0, 6.0, 7.0, 3.0, 5.0, 7.0, 6.0, 5.0, 2.0, 4.0, 1.0, 7.0, 9.0, 5.0, 3.0, 5.0, 13.0, 7.0, 7.0], "node_procs_blocked": [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], "node_entropy_available_bits": [3501.0, 3531.0, 3628.0, 3724.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3333333333333335, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [1365.3333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 9557.333333333334, 0.0, 0.0, 1365.3333333333333, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.00033333331036070984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001000000008692344, 0.0, 0.0, 0.00033333331036070984, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [1653927936.0, 1653927936.0, 1653927936.0, 1653927936.0, 1653944320.0, 1653944320.0, 1653940224.0, 1653956608.0, 1653956608.0, 1653956608.0, 1653972992.0, 1653972992.0, 1653972992.0, 1653989376.0, 1653989376.0, 1653989376.0, 1654005760.0, 1654005760.0, 1654005760.0, 1654022144.0, 1654022144.0]}, "network": {"node_sockstat_TCP_tw": [13.0, 13.0, 12.0, 12.0, 12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 11.0, 11.0, 11.0, 11.0, 12.0, 10.0, 10.0, 10.0, 10.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.3333333333333333, 2.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0], "node_sockstat_TCP_inuse": [12.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0, 17.0]}}}, "13": {"start_time": "1697298175", "end_time": "1697298246", "start_timestamp": "2023-10-14 23:42:55", "end_timestamp": "2023-10-14 23:44:06", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.59 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.59"}, "startsAt": "2023-10-15T01:17:49.467858611Z", "endsAt": "2023-10-15T01:23:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.59 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.59"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent commits or highly concurrent inserts"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 117\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 26\n    \n    # Number of rows to insert\n    num_rows = 68\n    \n    # Size of each column (in characters)\n    column_size = 78\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In an IoT system, 117 sensors generate a large amount of data that needs to be inserted into the database simultaneously. This process involves inserting data into a table containing 26 columns and 68 rows, with each column having a size of 78 characters. The purpose is to simulate the database exception that can occur due to the insertion of such a large volume of data.\n", "workload": "- SQL Query: \"insert into table1 select generate_series(1,3167807), (SELECT substr(md5(random()::text), 1, 70)),..., now();\", Frequency: 2\n", "slow_queries": ["insert into aa select generate_series(1,400000), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), repeat(round(random()*999)::text,6), now();"], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 1.0, 116.0, 103.0, 109.0, 123.0, 110.0, 117.0, 115.0, 97.0, 42.0, 120.0, 115.0, 119.0, 120.0, 114.0, 116.0, 123.0, 127.0, 74.0, 114.0, 74.0, 1.0], "node_procs_blocked": [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6666666666666667, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 8.666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6826.666666666667, 0.0, 0.0, 0.0, 1365.3333333333333, 0.0, 0.0, 35498.666666666664, 0.0, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003333333491658171, 0.0, 0.0, 0.0, 0.001999999978579581, 0.0, 0.0, 0.002333333327745398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016666666682188709, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [196501504.0, 196501504.0, 196501504.0, 196501504.0, 196517888.0, 196517888.0, 196517888.0, 196534272.0, 196534272.0, 196534272.0, 196550656.0, 196550656.0, 196542464.0, 196542464.0, 196558848.0, 196558848.0, 196558848.0, 196575232.0, 196575232.0, 196567040.0, 196567040.0, 196583424.0, 196583424.0, 196497408.0]}, "network": {"node_sockstat_TCP_tw": [9.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 4.0, 4.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 39.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 21.0, 21.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 139.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 138.0, 21.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 12.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 130.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 129.0, 12.0]}}}, "14": {"start_time": "1697299345", "end_time": "1697299417", "start_timestamp": "2023-10-15 00:02:25", "end_timestamp": "2023-10-15 00:03:37", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.59 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.59"}, "startsAt": "2023-10-15T01:17:49.467858611Z", "endsAt": "2023-10-15T01:23:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.59 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.59"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent commits or highly concurrent inserts"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n\n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef insert_large_data(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n\n    print_time()\n    #Delete undeleted tables\n    delete_table(table_name)\n    #create a new table\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    #insert the data\n    #insert_definitions = ', '.join(f'repeat(round(random()*999)::text,{(colsize//3)})' for i in range(ncolumns))\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();'\n    db.concurrent_execute_sql(threads,duration,insert_data,commit_interval=1)\n\n    #delete the table\n    delete_table(table_name)\n    \n    #print the end time\n    print_time()\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 152\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 34\n    \n    # Number of rows to insert\n    num_rows = 86\n    \n    # Size of each column (in characters)\n    column_size = 65\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    insert_large_data(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In an internet of things (IoT) system, 152 sensors generate a large amount of data that needs to be inserted into a database simultaneously. Each data entry consists of 34 columns, with each column having a size of 65 characters. There are a total of 86 data entries. This simulation aims to trigger a database exception due to the overwhelming amount of data being inserted at once.\n", "workload": "- SQL Query: \"insert into table1 select generate_series(1,3167807), (SELECT substr(md5(random()::text), 1, 70)),..., now();\", Frequency: 152\n", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 1.0, 112.0, 148.0, 148.0, 106.0, 152.0, 113.0, 136.0, 134.0, 139.0, 115.0, 153.0, 153.0, 139.0, 99.0, 149.0, 112.0, 156.0, 41.0, 147.0, 141.0, 1.0, 1.0], "node_procs_blocked": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.6666666666666667, 0.0, 0.0, 2.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8192.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10922.666666666666, 0.0, 0.0, 8192.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.018333333311602473, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.010000000009313226, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [1327775744.0, 1327779840.0, 1327779840.0, 1327779840.0, 1327779840.0, 1327779840.0, 1327796224.0, 1327796224.0, 1327796224.0, 1327796224.0, 1327812608.0, 1327812608.0, 1327812608.0, 1327812608.0, 1327824896.0, 1327824896.0, 1327824896.0, 1327824896.0, 1327841280.0, 1327841280.0, 1327841280.0, 1327841280.0, 1327857664.0, 1326788608.0, 1326788608.0]}, "network": {"node_sockstat_TCP_tw": [8.0, 9.0, 9.0, 9.0, 9.0, 7.0, 7.0, 7.0, 7.0, 7.0, 5.0, 5.0, 6.0, 5.0, 6.0, 5.0, 5.0, 5.0, 3.0, 7.0, 6.0, 6.0, 6.0, 6.0, 6.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 50.666666666666664, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 21.0, 21.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 173.0, 21.0, 21.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 12.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 164.0, 12.0, 12.0]}}}, "15": {"start_time": "1697300059", "end_time": "1697300120", "start_timestamp": "2023-10-15 00:14:19", "end_timestamp": "2023-10-15 00:15:20", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:cpu_usage[ins=] = 0.71 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.71"}, "startsAt": "2023-10-15T13:01:49.467858611Z", "endsAt": "2023-10-15T13:02:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Acpu_usage_1m+%3E+0.7&g0.tab=1", "fingerprint": "b97dc2cd4ed607f5"}], "groupLabels": {"alertname": "NodeCpuHigh"}, "commonLabels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:cpu_usage[ins=] = 0.71 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.71"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeCpuHigh\"}", "truncatedAlerts": 0}, {"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 2.57 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 2.57"}, "startsAt": "2023-10-15T13:00:49.467858611Z", "endsAt": "2023-10-15T13:03:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 2.57 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 2.57"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["POOR JOIN PERFORMANCE", "CPU CONTENTION"], "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION", "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n", "description": "In a large-scale data analytics system, multiple joins are performed between tables with poor join performance. Additionally, there is contention for CPU resources during the join operation, resulting in degraded performance of the system.\n", "workload": "- SQL Query: \"insert into table1 select generate_series(1,3167807), (SELECT substr(md5(random()::text), 1, 70)),..., now();\", Frequency: 1\n", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [2.0, 5.0, 8.0, 5.0, 6.0, 5.0, 8.0, 5.0, 9.0, 8.0, 7.0, 8.0, 5.0, 7.0, 7.0, 7.0, 7.0, 9.0, 5.0, 7.0, 6.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 75826517.33333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_io_time_weighted_seconds_total": [0.0, 3.251333333319053, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333333364377418, 0.0, 0.0, 0.0, 0.0, 0.00033333331036070984, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 3.250999999969887, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"irate(node_disk_write_time_seconds_total": [0.0, 0.00033333331036070984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0033333333364377418, 0.0, 0.0, 0.0, 0.0, 0.0003333333491658171, 0.0, 0.0, 0.0, 0.0, 0.0]}, "network": {"node_sockstat_TCP_tw": [6.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 6.0, 6.0, 6.0, 5.0, 5.0, 5.0, 5.0, 5.0, 4.0, 4.0, 1.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 1.3333333333333333, 2.0, 2.3333333333333335, 1.6666666666666667, 2.0, 2.0, 2.0, 2.0, 1.6666666666666667, 2.0, 2.0, 2.0, 1.6666666666666667, 2.0, 2.0, 2.0, 1.6666666666666667, 2.0, 2.0, 2.0], "node_sockstat_TCP_alloc": [21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0], "node_sockstat_TCP_inuse": [12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0]}}}, "16": {"start_time": "1697303534", "end_time": "1697303594", "start_timestamp": "2023-10-15 01:12:14", "end_timestamp": "2023-10-15 01:13:14", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.55 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.55"}, "startsAt": "2023-10-15T13:50:49.467858611Z", "endsAt": "2023-10-15T13:51:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.55 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.55"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["POOR JOIN PERFORMANCE", "CPU CONTENTION"], "command": "python anomaly_trigger/main.py --anomaly POOR_JOIN_PERFORMANCE,CPU_CONTENTION", "script": "import  os\nimport re\nimport time\n\nimport psycopg2\n\n\nREPEATCOUNT = 1\nTIMELOGPATH = str(int(time.time())) + \"_job_trigger_time_log.txt\"\nTIMELOG = open(TIMELOGPATH, 'w+')\n\n\nclass Database():\n\n    def __init__(self):\n        self.conn = None\n        self.conn = psycopg2.connect(database='imdbload',\n                                     user='xxxx',\n                                     password='xxxx',\n                                     host='xxxx',\n                                     port=5432)\n\n    def execute_sql(self, sql):\n        fail = 1\n        cur = self.conn.cursor()\n        i = 0\n        cnt = 3\n        while fail == 1 and i < cnt:\n            try:\n                fail = 0\n                cur.execute(sql)\n            except BaseException as error:\n                fail = 1\n                print(error)\n            res = []\n            if fail == 0:\n                res = cur.fetchall()\n            i = i + 1\n        if fail == 1:\n            # print(\"SQL Execution Fatal!!\", sql)\n            return 0, ''\n        elif fail == 0:\n            return 1, res\n\n\ndef all_sql_files():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    # all_file_list = list(filter(file_filter, os.listdir(res_path)))\n    # all_file_list = sorted(all_file_list, key=custom_sort)\n    all_file_list = [\n        '1a.sql', '1b.sql', '1c.sql', '1d.sql',\n        '2a.sql', '2b.sql', '2c.sql', '2d.sql',\n        '3a.sql', '3b.sql', '3c.sql',\n        '4a.sql', '4b.sql', '4c.sql',\n        '5a.sql', '5b.sql', '5c.sql',\n        '6a.sql', '6b.sql', '6c.sql', '6d.sql', '6e.sql', '6f.sql',\n        '7a.sql', '7b.sql', '7c.sql',\n        '8a.sql', '8b.sql', '8c.sql', '8d.sql',\n        '9a.sql', '9b.sql', '9c.sql', '9d.sql',\n        '10a.sql', '10b.sql', '10c.sql',\n        '11a.sql', '11b.sql', '11c.sql', '11d.sql',\n        '12a.sql', '12b.sql', '12c.sql',\n        '13a.sql', '13b.sql', '13c.sql', '13d.sql',\n        '14a.sql', '14b.sql', '14c.sql',\n        '15a.sql', '15b.sql', '15c.sql', '15d.sql',\n        '16a.sql', '16b.sql', '16c.sql', '16d.sql',\n        '17a.sql', '17b.sql', '17c.sql', '17d.sql', '17e.sql', '17f.sql',\n        '18a.sql', '18b.sql', '18c.sql',\n        '19a.sql', '19b.sql', '19c.sql', '19d.sql',\n        '20a.sql', '20b.sql', '20c.sql',\n        '21a.sql', '21b.sql', '21c.sql',\n        '22a.sql', '22b.sql', '22c.sql', '22d.sql',\n        '23a.sql', '23b.sql', '23c.sql',\n        '24a.sql', '24b.sql',\n        '25a.sql', '25b.sql', '25c.sql',\n        '26a.sql', '26b.sql', '26c.sql',\n        '27a.sql', '27b.sql', '27c.sql',\n        '28a.sql', '28b.sql', '28c.sql',\n        '29a.sql', '29b.sql', '29c.sql',\n        '30a.sql', '30b.sql', '30c.sql',\n        '31a.sql', '31b.sql', '31c.sql',\n        '32a.sql', '32b.sql',\n        '33a.sql', '33b.sql', '33c.sql']\n\n    print(all_file_list)\n    files_list = []\n    for file in all_file_list:\n        files_list.append(res_path + file)\n    return files_list\n\n\ndef custom_sort(item):\n    # \u63d0\u53d6\u6570\u5b57\u548c\u5b57\u6bcd\u90e8\u5206\n    match = re.match(r'(\\d+)(\\D+)', item)\n    # \u5c06\u6570\u5b57\u90e8\u5206\u8f6c\u6362\u4e3a\u6574\u6570\u4ee5\u8fdb\u884c\u6bd4\u8f83\n    num_part = int(match.group(1))\n    # \u8fd4\u56de\u5143\u7ec4\u4ee5\u6309\u6570\u5b57\u548c\u5b57\u6bcd\u6392\u5e8f\n    return (num_part, match.group(2))\n\n\ndef file_filter(f):\n    if f[-4:] == '.sql' and 'schema' not in f and 'fkindexes' not in f:\n        return True\n    else:\n        return False\n\n\ndef get_sql_from_file(file_name):\n    file = open(file_name)\n    lines = file.readlines().copy()\n    sql = ''\n    for line in lines:\n        sql += line\n    sql = sql.replace('\n', ' ').replace('   ', ' ').replace('  ', ' ')\n    file.close()\n    return sql\n\n\ndef test_hint_from_file(sql_file):\n    db = Database()\n    sql = get_sql_from_file(sql_file)\n    success, result_cont = db.execute_sql(sql)\n    print(success, result_cont)\n\n\ndef test_all():\n    sql_files = all_sql_files()\n\n    for sql_file in list(sql_files)[:-10]:\n        if sql_file:\n            test_hint_from_file(sql_file)\n\n\ndef test_one():\n    res_path = \"{}/join-order-benchmark-master/\".format(\n        os.path.dirname(os.path.abspath(__file__)))\n    test_hint_from_file(res_path + '1a.sql')\n\n\nif __name__ == '__main__':\n    for i in range(0, REPEATCOUNT):\n        TIMELOG.write(str(int(time.time()))+\";\")\n        test_all()\n        TIMELOG.write(str(int(time.time()))+\"\n\")\n        TIMELOG.flush()\n\n    TIMELOG.close()\n\n)\n", "description": "In a data analytics system, multiple users are performing join operations on large datasets. The join performance is poor due to suboptimal join algorithms or inefficient use of indexes. Additionally, there is CPU contention as multiple users are simultaneously executing resource-intensive queries, which leads to slower query execution times.\n", "workload": "SELECT MIN(mc.note) AS production_note, MIN(t.title) AS movie_title, MIN(t.production_year) AS movie_year FROM company_type AS ct, info_type AS it, movie_companies AS mc, movie_info_idx AS mi_idx, title AS t WHERE ct.kind = 'production companies' AND it.info = 'top 250 rank' AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%' AND (mc.note LIKE '%(co-production)%' OR mc.note LIKE '%(presents)%') AND ct.id = mc.company_type_id AND t.id = mc.movie_id AND t.id = mi_idx.movie_id AND mc.movie_id = mi_idx.movie_id AND it.id = mi_idx.info_type_id;", "slow_queries": ["SELECT MIN(mc.note) AS production_note, MIN(t.title) AS movie_title, MIN(t.production_year) AS movie_year FROM company_type AS ct, info_type AS it, movie_companies AS mc, movie_info_idx AS mi_idx, title AS t WHERE ct.kind = 'production companies' AND it.info = 'top 250 rank' AND mc.note NOT LIKE '%(as Metro-Goldwyn-Mayer Pictures)%' AND (mc.note LIKE '%(co-production)%' OR mc.note LIKE '%(presents)%') AND ct.id = mc.company_type_id AND t.id = mc.movie_id AND t.id = mi_idx.movie_id AND mc.movie_id = mi_idx.movie_id AND it.id = mi_idx.info_type_id;"], "exceptions": {"cpu": {"node_procs_running": [1.0, 9.0, 6.0, 10.0, 11.0, 10.0, 8.0, 6.0, 5.0, 5.0, 7.0, 5.0, 8.0, 5.0, 4.0, 7.0, 5.0, 7.0, 6.0, 7.0, 5.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_load1": [7.56, 7.56, 7.43, 7.16, 7.16, 6.91, 6.91, 7.07, 6.91, 6.91, 6.83, 6.83, 6.37, 6.34, 6.34, 6.23, 6.23, 6.05, 6.13, 6.13, 6.04]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 1250.6666666666667, 0.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 75826517.33333333, 0.0, 0.0, 0.0, 16384.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 3.5333333333255723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"irate(node_disk_write_time_seconds_total": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0003333333491658171, 0.0, 0.0, 0.007999999991928538, 0.0, 0.0, 0.0, 0.0, 0.0, 0.006333333323709667, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_memory_Inactive_anon_bytes": [235548672.0, 236871680.0, 238927872.0, 237019136.0, 236695552.0, 236941312.0, 238927872.0, 238931968.0, 236957696.0, 236781568.0, 237002752.0, 238927872.0, 236974080.0, 236978176.0, 237010944.0, 238927872.0, 236957696.0, 236789760.0, 237023232.0, 237977600.0, 238927872.0], "node_memory_Buffers_bytes": [19181568.0, 19210240.0, 19210240.0, 19218432.0, 19218432.0, 19226624.0, 19226624.0, 19234816.0, 19234816.0, 19243008.0, 19243008.0, 19243008.0, 19243008.0, 19243008.0, 19251200.0, 19251200.0, 19259392.0, 19259392.0, 19267584.0, 19267584.0, 19275776.0], "node_memory_Cached_bytes": [5205512192.0, 5434511360.0, 5436698624.0, 5434613760.0, 5434544128.0, 5434662912.0, 5436751872.0, 5436719104.0, 5434613760.0, 5434683392.0, 5434683392.0, 5436751872.0, 5434646528.0, 5434576896.0, 5434638336.0, 5436751872.0, 5434621952.0, 5434494976.0, 5434585088.0, 5436751872.0, 5436751872.0]}, "network": {"node_sockstat_TCP_tw": [8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 6.0, 6.0, 4.0, 5.0, 5.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.6666666666666666, 1.6666666666666667, 2.3333333333333335, 2.0, 2.0, 1.6666666666666667, 2.0, 2.0, 2.0, 2.0, 1.6666666666666667, 2.0, 2.0, 2.0, 1.6666666666666667, 2.0, 2.0, 2.0, 2.0, 2.0], "node_sockstat_TCP_alloc": [22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0], "node_sockstat_TCP_inuse": [13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0]}}}, "17": {"start_time": "1697298558", "end_time": "1697298672", "start_timestamp": "2023-10-14 23:49:18", "end_timestamp": "2023-10-14 23:51:12", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "startsAt": "2023-10-14T23:29:49.467858611Z", "endsAt": "2023-10-15T00:26:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 87\n    \n    # Number of rows to insert\n    num_rows = 831757\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In a financial database with 87 columns and 831,757 rows, each with a column size of 56 characters, a large number of unnecessary indexes are created at the beginning of the query operation for various financial information such as transaction date, account holder, and transaction amount. This can result in additional storage consumption and performance overhead. The query is performed by 5 users simultaneously.\n", "workload": "1. SQL query: \"SELECT indexname FROM pg_indexes WHERE tablename='table1';\", Frequency: 1 thread (as part of `drop_index` function).\n2. SQL query: \"DROP INDEX index_name;\", Frequency: Number of index names returned from the previous query, i.e., dynamic (as part of `drop_index` function).\n3. SQL query: \"CREATE INDEX index_table1_i ON table1(namei);\", Frequency: 6 threads (as part of `build_index` function).\n4. SQL query: \"INSERT INTO table1 SELECT generate_series(1,415253), (SELECT substr(md5(random()::text), 1, 87)), now();\", Frequency: 1 thread (as part of `redundent_index` function).\n5. SQL query: \"CREATE INDEX index_table1_id ON table1(id);\", Frequency: 1 thread (as part of `redundent_index` function).\n6. SQL query: \"UPDATE table1 set namei=(SELECT substr(md5(random()::text), 1, 87)) WHERE id =id;\", Frequency: 5 threads (as part of `lock` function).\n\nNote: In these queries \"i\" and \"id\" are dynamic and vary according to the logic in the code.", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 1.0, 5.0, 3.0, 7.0, 5.0, 5.0, 6.0, 5.0, 7.0, 4.0, 6.0, 9.0, 5.0, 7.0, 4.0, 6.0, 1.0, 7.0, 10.0, 12.0, 15.0, 14.0, 2.0, 16.0, 6.0, 1.0, 15.0, 3.0, 2.0, 12.0, 8.0, 14.0, 15.0, 4.0, 5.0, 6.0, 1.0, 8.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3528.0, 3569.0, 3619.0, 3657.0, 3704.0, 3750.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 0.0, 47.666666666666664, 2.6666666666666665, 5.666666666666667, 3.3333333333333335, 0.3333333333333333, 23.333333333333332, 0.3333333333333333, 18.0, 0.0, 0.0, 1.0, 1.0, 0.3333333333333333, 0.6666666666666666, 88.33333333333333, 1.3333333333333333, 2.0, 1.3333333333333333, 0.0, 0.3333333333333333, 2.3333333333333335, 0.0, 0.6666666666666666, 6.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 1.6666666666666667, 0.3333333333333333, 0.0, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 1.3333333333333333, 1.6666666666666667, 46.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 928426.6666666666, 10922.666666666666, 23210.666666666668, 34133.333333333336, 1365.3333333333333, 95573.33333333333, 2730.6666666666665, 73728.0, 0.0, 0.0, 36864.0, 4096.0, 1365.3333333333333, 2730.6666666666665, 1282048.0, 8192.0, 30037.333333333332, 21845.333333333332, 0.0, 8192.0, 25941.333333333332, 0.0, 2730.6666666666665, 24576.0, 0.0, 2730.6666666666665, 0.0, 1365.3333333333333, 6826.666666666667, 1365.3333333333333, 0.0, 1365.3333333333333, 1365.3333333333333, 1365.3333333333333, 5461.333333333333, 6826.666666666667, 348160.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.012333333337058624, 0.025999999993170302, 0.09566666667039196, 0.006000000013348957, 0.003999999995964269, 0.002333333327745398, 0.0, 0.0016666666682188709, 0.0, 0.0, 0.0, 0.003999999995964269, 0.003999999995964269, 0.004333333345130086, 0.04866666668870797, 0.006999999983236194, 0.0006666666595265269, 0.0006666666595265269, 0.0, 0.0003333333491658171, 0.00033333331036070984, 0.0, 0.010000000009313226, 0.010666666668839753, 0.0, 0.0073333333324020105, 0.0, 0.0016666666682188709, 0.0003333333491658171, 0.0016666666682188709, 0.0, 0.00599999997454385, 0.004666666694295903, 0.004333333306324978, 0.005000000004656613, 0.006000000013348957, 0.05066666666728755]}, "memory": {"node_memory_Inactive_anon_bytes": [2633457664.0, 2633457664.0, 2634362880.0, 2634362880.0, 2634362880.0, 1795219456.0, 1795186688.0, 1795166208.0, 1795133440.0, 1794572288.0, 1794514944.0, 1794473984.0, 1794441216.0, 1794392064.0, 1794355200.0, 1794306048.0, 1794297856.0, 1777143808.0, 1777143808.0, 1777143808.0, 1777176576.0, 1776783360.0, 1776783360.0, 1776807936.0, 1776807936.0, 1776816128.0, 1776816128.0, 1776816128.0, 1776816128.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0, 1773244416.0]}, "network": {"node_sockstat_TCP_tw": [16.0, 16.0, 11.0, 11.0, 12.0, 10.0, 10.0, 7.0, 7.0, 10.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 9.0, 5.0, 5.0, 11.0, 10.0, 11.0, 11.0, 11.0, 11.0, 8.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 9.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 1.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 3.3333333333333335, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333], "node_sockstat_TCP_alloc": [21.0, 21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 21.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 32.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 12.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 23.0]}}}, "18": {"start_time": "1697305593", "end_time": "1697305710", "start_timestamp": "2023-10-15 01:46:33", "end_timestamp": "2023-10-15 01:48:30", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "startsAt": "2023-10-14T23:29:49.467858611Z", "endsAt": "2023-10-15T00:26:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 59\n    \n    # Number of rows to insert\n    num_rows = 459312\n    \n    # Size of each column (in characters)\n    column_size = 87\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In a database used by a financial institution, there are 5 users simultaneously executing queries on a table with 59 columns and 459,312 rows of financial transaction records. Each column has a size of 87 characters. The queries involve redundant indexes that were created at the beginning of the operations and deleted after. This process causes additional storage consumption and performance overhead in the database.\n", "workload": "1. SQL query: \"SELECT indexname FROM pg_indexes WHERE tablename='table1';\", Frequency: 1 thread (as part of `drop_index` function).\n2. SQL query: \"DROP INDEX index_name;\", Frequency: Number of index names returned from the previous query, i.e., dynamic (as part of `drop_index` function).\n3. SQL query: \"CREATE INDEX index_table1_i ON table1(namei);\", Frequency: 6 threads (as part of `build_index` function).\n4. SQL query: \"INSERT INTO table1 SELECT generate_series(1,415253), (SELECT substr(md5(random()::text), 1, 87)), now();\", Frequency: 1 thread (as part of `redundent_index` function).\n5. SQL query: \"CREATE INDEX index_table1_id ON table1(id);\", Frequency: 1 thread (as part of `redundent_index` function).\n6. SQL query: \"UPDATE table1 set namei=(SELECT substr(md5(random()::text), 1, 87)) WHERE id =id;\", Frequency: 5 threads (as part of `lock` function).\n\nNote: In these queries \"i\" and \"id\" are dynamic and vary according to the logic in the code.", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [2.0, 1.0, 5.0, 5.0, 6.0, 8.0, 6.0, 8.0, 10.0, 22.0, 9.0, 2.0, 5.0, 9.0, 5.0, 4.0, 10.0, 9.0, 4.0, 1.0, 4.0, 1.0, 15.0, 1.0, 4.0, 12.0, 2.0, 1.0, 1.0, 6.0, 7.0, 9.0, 14.0, 13.0, 1.0, 11.0, 16.0, 1.0, 17.0, 13.0], "node_procs_blocked": [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 11.0, 2.0, 2.0, 4.0, 1.0, 3.0, 8.0, 2.0, 4.0, 3.0, 2.0, 3.0, 5.0, 2.0, 0.0, 2.0, 2.0, 1.0, 1.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3506.0, 3564.0, 3621.0, 3680.0, 3725.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [1499189248.0, 1499205632.0, 1499205632.0, 216219648.0, 206540800.0, 206532608.0, 206532608.0, 206540800.0, 206520320.0, 206540800.0, 206540800.0, 206540800.0, 206540800.0, 206540800.0, 206540800.0, 206540800.0, 206540800.0, 206520320.0, 206540800.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206319616.0, 206336000.0, 206336000.0, 206336000.0, 206336000.0, 210378752.0, 233701376.0]}, "network": {"node_sockstat_TCP_tw": [10.0, 10.0, 10.0, 10.0, 8.0, 9.0, 9.0, 9.0, 9.0, 8.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 4.0, 8.0, 8.0, 7.0, 8.0, 8.0, 9.0, 9.0, 8.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 10.0, 6.0, 10.0, 10.0, 9.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.6666666666666666, 0.0, 0.3333333333333333, 3.3333333333333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.6666666666666666, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 22.0, 32.0, 33.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0], "node_sockstat_TCP_inuse": [13.0, 13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 13.0, 23.0, 24.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0]}}}, "19": {"start_time": "1697308003", "end_time": "1697308119", "start_timestamp": "2023-10-15 02:26:43", "end_timestamp": "2023-10-15 02:28:39", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "startsAt": "2023-10-14T23:29:49.467858611Z", "endsAt": "2023-10-15T00:26:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.50 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.50"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 9\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 95\n    \n    # Number of rows to insert\n    num_rows = 606760\n    \n    # Size of each column (in characters)\n    column_size = 57\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In a financial institution's database, 9 users perform a query on a table with 95 columns and 606,760 rows. Each column has a size of 57 characters. The query involves redundant indexes on attributes such as customer ID, transaction amount, and transaction date. This can result in additional storage consumption and performance overhead.\n", "workload": "1. SQL query: \"SELECT indexname FROM pg_indexes WHERE tablename='table1';\", Frequency: 1 thread (as part of `drop_index` function).\n2. SQL query: \"DROP INDEX index_name;\", Frequency: Number of index names returned from the previous query, i.e., dynamic (as part of `drop_index` function).\n3. SQL query: \"CREATE INDEX index_table1_i ON table1(namei);\", Frequency: 6 threads (as part of `build_index` function).\n4. SQL query: \"INSERT INTO table1 SELECT generate_series(1,415253), (SELECT substr(md5(random()::text), 1, 87)), now();\", Frequency: 1 thread (as part of `redundent_index` function).\n5. SQL query: \"CREATE INDEX index_table1_id ON table1(id);\", Frequency: 1 thread (as part of `redundent_index` function).\n6. SQL query: \"UPDATE table1 set namei=(SELECT substr(md5(random()::text), 1, 87)) WHERE id =id;\", Frequency: 5 threads (as part of `lock` function).\n\nNote: In these queries \"i\" and \"id\" are dynamic and vary according to the logic in the code.", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 2.0, 3.0, 5.0, 6.0, 7.0, 3.0, 5.0, 6.0, 5.0, 7.0, 5.0, 8.0, 4.0, 6.0, 5.0, 3.0, 1.0, 2.0, 7.0, 2.0, 10.0, 9.0, 5.0, 8.0, 9.0, 2.0, 7.0, 8.0, 4.0, 16.0, 9.0, 4.0, 13.0, 16.0, 10.0, 9.0, 5.0, 8.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 4.0, 2.0, 1.0, 3.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3501.0, 3501.0, 3546.0, 3606.0, 3659.0, 3713.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [192192512.0, 192192512.0, 192192512.0, 192192512.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 192237568.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 191971328.0, 198160384.0, 210939904.0, 215068672.0]}, "network": {"node_sockstat_TCP_tw": [8.0, 8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 6.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 10.0, 10.0, 8.0, 8.0, 9.0, 5.0, 9.0, 7.0, 7.0, 7.0, 6.0, 6.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 3.6666666666666665, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 22.0, 22.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0], "node_sockstat_TCP_inuse": [13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 13.0, 13.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0]}}}, "20": {"start_time": "1697338087", "end_time": "1697338148", "start_timestamp": "2023-10-15 10:48:07", "end_timestamp": "2023-10-15 10:49:08", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.01 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.01"}, "startsAt": "2023-10-14T23:47:49.467858611Z", "endsAt": "2023-10-15T00:15:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.01 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.01"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["highly deletes"], "command": "python anomaly_trigger/main.py --anomaly VACUUM", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 196\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 7\n    \n    # Number of rows to insert\n    num_rows = 3888722\n    \n    # Size of each column (in characters)\n    column_size = 72\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In the database of a social media platform, there are 196 users searching for posts in a table that contains 7 columns, 3,888,722 rows, with each column size being 72 characters. However, after performing a vacuum operation on the database table, an exception occurs when these users simultaneously search for posts.\n", "workload": "1. SQL Query: `insert into table1 select generate_series(1,3637430),(SELECT substr(md5(random()::text), 1, 84)), now();`\n    Frequency Number (Threads): 107\n2. SQL Query: `delete from table1 where id < 2909944;`\n    Frequency Number (Threads): 107\n3. SQL Query: `select * from table1 where id=;`\n    Frequency Number (Threads): 107\n", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 4.0, 6.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 13.0, 5.0, 4.0, 4.0, 3.0, 2.0, 1.0, 9.0, 160.0, 201.0, 204.0, 203.0], "node_procs_blocked": [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0, 3.0, 3.0, 94.0], "node_entropy_available_bits": [3500.0, 3506.0, 3526.0, 3552.0, 3572.0, 3589.0, 3615.0, 3634.0, 3653.0, 3677.0, 3697.0, 3718.0, 3739.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [1640886272.0, 1640890368.0, 1640890368.0, 1724788736.0, 1870258176.0, 2126061568.0, 2387996672.0, 2521866240.0, 2776416256.0, 3027857408.0, 3027603456.0, 3027599360.0, 3027599360.0, 3027599360.0, 3027599360.0, 1634037760.0, 1634037760.0, 1688297472.0, 1708683264.0, 1708691456.0, 1708699648.0]}, "network": {"node_sockstat_TCP_tw": [11.0, 10.0, 11.0, 8.0, 8.0, 12.0, 11.0, 11.0, 9.0, 9.0, 9.0, 10.0, 10.0, 9.0, 9.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 65.33333333333333, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 24.0, 23.0, 23.0, 23.0, 23.0, 23.0, 22.0, 22.0, 218.0, 218.0, 218.0, 218.0], "node_sockstat_TCP_inuse": [13.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 15.0, 14.0, 14.0, 14.0, 14.0, 14.0, 13.0, 13.0, 209.0, 209.0, 209.0, 209.0]}}}, "21": {"start_time": "1697339126", "end_time": "1697339187", "start_timestamp": "2023-10-15 11:05:26", "end_timestamp": "2023-10-15 11:06:27", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:cpu_usage[ins=] = 0.98 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.98"}, "startsAt": "2023-10-15T01:22:49.467858611Z", "endsAt": "2023-10-15T01:23:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Acpu_usage_1m+%3E+0.7&g0.tab=1", "fingerprint": "b97dc2cd4ed607f5"}], "groupLabels": {"alertname": "NodeCpuHigh"}, "commonLabels": {"alertname": "NodeCpuHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:cpu_usage[ins=] = 0.98 > 70%\n", "summary": "WARN NodeCpuHigh @172.27.58.65:9100 0.98"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeCpuHigh\"}", "truncatedAlerts": 0}], "labels": ["highly concurrent updates"], "command": "python anomaly_trigger/main.py --anomaly LOCK_CONTENTION", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef lock_contention(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 73\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 82\n    \n    # Number of rows to insert\n    num_rows = 208\n    \n    # Size of each column (in characters)\n    column_size = 56\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    lock_contention(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In a database for a social media platform, 73 users simultaneously attempt to perform frequent update operations in a database table containing 82 columns and 208 rows of user records, each with a column size of 56 characters. These users compete with each other to lock the database table and perform the update operation. The purpose of this simulation is to observe if any exceptions occur due to lock contention in the database.\n", "workload": "1. SQL Query: 'insert into table1 select generate_series(1,395),(SELECT substr(md5(random()::text), 1, 93))..., now();' - Frequency: 1\n2. SQL Query: 'update table1 set name{i}=(SELECT substr(md5(random()::text), 1, 93)) where id ={j}' (Note: {i} is a random integer between 0 and 81, {j} is a random integer between 1 and 394) -  Frequency: 140", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 3.0, 2.0, 5.0, 7.0, 9.0, 2.0, 7.0, 6.0, 3.0, 7.0, 4.0, 9.0, 2.0, 9.0, 7.0, 7.0, 5.0, 4.0, 7.0, 4.0], "node_procs_blocked": [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "irate(node_disk_reads_completed_total": [0.0, 32.666666666666664, 0.0, 0.0, 29.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.0, 0.0, 0.0, 0.0, 0.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 1118208.0, 0.0, 0.0, 1835008.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1036288.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.1180000000167638, 0.0, 0.0, 0.06533333333209157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08366666664369404, 0.0, 0.0, 0.0, 0.0]}, "memory": {}, "network": {"node_sockstat_TCP_tw": [10.0, 10.0, 10.0, 9.0, 9.0, 9.0, 8.0, 9.0, 8.0, 9.0, 9.0, 8.0, 8.0, 8.0, 9.0, 9.0, 9.0, 9.0, 4.0, 8.0, 8.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0], "node_sockstat_TCP_alloc": [22.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0], "node_sockstat_TCP_inuse": [13.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0]}}}, "22": {"start_time": "1697342786", "end_time": "1697342875", "start_timestamp": "2023-10-15 12:06:26", "end_timestamp": "2023-10-15 12:07:55", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "startsAt": "2023-10-15T01:24:49.467858611Z", "endsAt": "2023-10-15T01:51:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["highly deletes"], "command": "python anomaly_trigger/main.py --anomaly VACUUM", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\n\ndef vacuum(threads,duration,ncolumns,nrows,colsize,table_name='table1'):\n    db=Database(init())\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n\n    # insert some data to be deleted\n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    # delete 80% of the rows\n    delete_nrows=int(nrows*0.8)\n    vacuum=f'delete from {table_name} where id < {delete_nrows};'\n    db.execute_sqls(vacuum)\n\n    # do the select , then the vacuum occurs\n    select='select * from '+table_name+' where id='\n    db.concurrent_execute_sql(threads,duration,select,nrows)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 61\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 5\n    \n    # Number of rows to insert\n    num_rows = 2784402\n    \n    # Size of each column (in characters)\n    column_size = 50\n    \n    # Table name\n    table_name = 'table1'\n    \n    # Call the insert_large_data function\n    vacuum(num_threads, insert_duration, num_columns, num_rows, column_size, table_name)\n", "description": "In the database of a social media platform, if 61 users simultaneously perform a search after a data cleaning operation on a database table containing 5 columns, 2,784,402 rows, each column size of 50 characters, an exception might occur due to the high demand on the database resources.\n", "workload": "1. Query: 'INSERT INTO table1 SELECT generate_series(1,2809399),(SELECT substr(md5(random()::text), 1, 92)), now()', Frequency (threads): 117\n2. Query: 'DELETE FROM table1 WHERE id < 2247519', Frequency (threads): 117\n3. Query: 'SELECT * FROM table1 WHERE id=', Frequency (threads): 117\n", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 4.0, 6.0, 3.0, 2.0, 5.0, 1.0, 1.0, 1.0, 69.0, 71.0, 66.0, 74.0, 73.0, 71.0, 71.0, 70.0, 71.0, 70.0, 73.0, 73.0, 72.0, 71.0, 74.0, 72.0, 71.0, 83.0, 70.0, 75.0, 1.0], "node_procs_blocked": [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0], "node_load15": [22.84, 22.84, 22.72, 22.61, 22.61, 22.49, 22.49, 22.37, 22.25, 22.25, 22.49, 22.49, 22.75, 22.99, 22.99, 23.25, 23.25, 23.5, 23.73, 23.73, 23.97, 23.97, 24.23, 24.48, 24.48, 24.73, 24.73, 24.96, 25.2, 25.2]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_bytes_total": [0.0, 0.0, 117418.66666666667, 2730.6666666666665, 5461.333333333333, 464213.3333333333, 219818.66666666666, 0.0, 0.0, 544768.0, 0.0, 9557.333333333334, 0.0, 1383082.6666666667, 0.0, 58709.333333333336, 1365.3333333333333, 2667861.3333333335, 0.0, 0.0, 0.0, 0.0, 1365.3333333333333, 53248.0, 1365.3333333333333, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_io_time_weighted_seconds_total": [0.0, 0.3940000000099341, 23.59966666654994, 29.38366666669026, 123.98066666675732, 150.6599999999938, 124.75433333326752, 68.25100000000869, 0.002000000017384688, 0.7119999999801317, 0.0, 0.0033333334140479565, 0.00033333323275049526, 0.060000000055879354, 0.0, 0.006333333284904559, 0.05233333337431153, 0.503000000026077, 0.046333333322157465, 0.004333333267519872, 0.002000000017384688, 0.001000000008692344, 0.0003333333879709244, 0.00033333323275049526, 0.0, 0.0, 0.0, 0.008666666690260172, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0, 0.02166666672565043, 0.0, 0.019333333320294816, 0.07666666666045785, 0.12199999997392297, 0.0, 0.0, 0.023000000044703484, 0.0, 0.002333333327745398, 0.0, 0.05966666666790843, 0.0, 0.006333333284904559, 0.0, 0.46866666665300727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003333333879709244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"irate(node_disk_write_time_seconds_total": [0.0, 0.3940000000099341, 23.57799999997951, 29.38366666669026, 123.96133333332061, 150.58333333333334, 124.6323333333324, 68.25100000000869, 0.001999999978579581, 0.6890000000130385, 0.0, 0.001000000008692344, 0.0003333333491658171, 0.00033333331036070984, 0.0, 0.0, 0.05233333333550642, 0.03433333333426466, 0.046333333322157465, 0.004333333345130086, 0.002000000017384688, 0.0009999999698872368, 0.0003333333491658171, 0.0, 0.0, 0.0, 0.0, 0.008666666651455065, 0.0, 0.0]}, "network": {"node_sockstat_TCP_tw": [10.0, 10.0, 11.0, 12.0, 12.0, 12.0, 12.0, 10.0, 11.0, 12.0, 12.0, 12.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 11.0, 8.0, 8.0, 8.0, 8.0, 9.0, 6.0, 6.0, 8.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 20.666666666666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666], "node_sockstat_TCP_alloc": [22.0, 24.0, 23.0, 23.0, 23.0, 23.0, 23.0, 22.0, 22.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 83.0, 22.0], "node_sockstat_TCP_inuse": [13.0, 15.0, 14.0, 14.0, 14.0, 14.0, 14.0, 13.0, 13.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 74.0, 13.0]}}}, "23": {"start_time": "1697348820", "end_time": "1697348935", "start_timestamp": "2023-10-15 13:47:00", "end_timestamp": "2023-10-15 13:48:55", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:stdload1[ins=] = 1.92 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.92"}, "startsAt": "2023-10-15T05:00:49.467858611Z", "endsAt": "2023-10-15T05:18:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Astdload1+%3E+1&g0.tab=1", "fingerprint": "ab4787213c7dd319"}], "groupLabels": {"alertname": "NodeLoadHigh"}, "commonLabels": {"alertname": "NodeLoadHigh", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:stdload1[ins=] = 1.92 > 100%\n", "summary": "WARN NodeLoadHigh @172.27.58.65:9100 1.92"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeLoadHigh\"}", "truncatedAlerts": 0}], "labels": ["too many indexes"], "command": "python anomaly_trigger/main.py --anomaly REDUNDANT_INDEX", "script": "import  psycopg2\nimport sys\nsys.path.append('/root/DB-GPT/')\nimport time\nimport datetime\nimport random\nimport yaml\nfrom multiprocessing.pool import *\n\n\nclass DBArgs(object):\n\n    def __init__(self, dbtype, config, dbname=None):\n        self.dbtype = dbtype\n        if self.dbtype == 'mysql':\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'com.mysql.jdbc.Driver'\n            self.jdbc = 'jdbc:mysql://'\n        else:\n            self.host = config['host']\n            self.port = config['port']\n            self.user = config['user']\n            self.password = config['password']\n            self.dbname = dbname if dbname else config['dbname']\n            self.driver = 'org.postgresql.Driver'\n            self.jdbc = 'jdbc:postgresql://'\n\nclass Database():\n    def __init__(self, args, timeout=-1):\n        self.args = args\n        self.conn = self.resetConn(timeout)\n\n\n        # self.schema = self.compute_table_schema()\n\n    def resetConn(self, timeout=-1):\n        conn = psycopg2.connect(database=self.args.dbname,\n                                            user=self.args.user,\n                                            password=self.args.password,\n                                            host=self.args.host,\n                                            port=self.args.port)\n        return conn\n    \n    def execute_sqls(self,sql):\n        self.conn =self.resetConn(timeout=-1)\n        cur = self.conn.cursor()\n        cur.execute(sql)\n        self.conn.commit()\n        cur.close()\n        self.conn.close()\n\n    def execute_sql_duration(self, duration, sql, max_id=0, commit_interval=500):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        start = time.time()\n        cnt = 0\n        if duration > 0:\n            while (time.time() - start) < duration:\n                if max_id > 0:\n                    id = random.randint(1, max_id - 1)\n                    cursor.execute(sql + str(id) + ';')\n                else:\n                    cursor.execute(sql)\n                cnt += 1\n                if cnt % commit_interval == 0:\n                    self.conn.commit()\n        else:\n            print(\"error, the duration should be larger than 0\")\n        self.conn.commit()\n        cursor.close()\n        self.conn.close()\n        return cnt\n\n    def concurrent_execute_sql(self, threads, duration, sql, max_id=0, commit_interval=500):\n        pool = ThreadPool(threads)\n        results = [pool.apply_async(self.execute_sql_duration, (duration, sql, max_id, commit_interval)) for _ in range(threads)]\n        pool.close()\n        pool.join()\n        return results\n    \n    def build_index(self, table_name, idx_num):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        \n        for i in range(0, idx_num):\n            the_sql = 'CREATE INDEX index_' + table_name + '_' + str(i) + ' ON ' + table_name + '(name' + str(i) + ');'\n            print(the_sql)\n            cursor.execute(the_sql)\n\n        \n        self.conn.commit()\n        self.conn.close()\n        return\n\n\n    \n    def drop_index(self,table_name):\n        self.conn = self.resetConn(timeout=-1)\n        cursor = self.conn.cursor()\n        cursor.execute(\"select indexname from pg_indexes where tablename='\"+table_name+\"';\")\n        idxs = cursor.fetchall()\n        for idx in idxs:\n            the_sql = 'DROP INDEX ' + idx[0] + ';'\n            cursor.execute(the_sql)\n            print(the_sql)\n        self.conn.commit()\n        self.conn.close()\n        return\n\n\ndef init():\n    #add the config\n    config_path = \"/root/DB-GPT/config/tool_config.yaml\"\n    with open(config_path, 'r') as config_file:\n        config = yaml.safe_load(config_file) \n    db_args =DBArgs('pgsql', config)\n    return db_args\n\n\n#create a table\ndef create_table(table_name,colsize, ncolumns):\n    db=Database(init())\n    column_definitions = ', '.join(f'name{i} varchar({colsize})' for i in range(ncolumns))\n    creat_sql = f'CREATE TABLE {table_name} (id int, {column_definitions}, time timestamp);'\n    db.execute_sqls(creat_sql)\n\n#delete the table\ndef delete_table(table_name):\n    db=Database(init())\n    delete_sql=f'DROP TABLE if exists {table_name}'\n    db.execute_sqls(delete_sql)\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\ndef redundent_index(threads,duration,ncolumns,nrows,colsize,nindex,table_name='table1'):\n    #create a new table\n    print_time()\n    delete_table(table_name)\n    create_table(table_name,colsize, ncolumns)\n    db=Database(init())\n    # insert some data to be updated \n    insert_definitions = ', '.join(f'(SELECT substr(md5(random()::text), 1, {colsize}))' for i in range(ncolumns))\n    insert_data=f'insert into {table_name} select generate_series(1,{nrows}),{insert_definitions}, now();' \n    db.execute_sqls(insert_data) \n\n    #initialization of the indexes\n    nindex=int((nindex*ncolumns)/10)\n    db.build_index(table_name,nindex)\n    id_index='CREATE INDEX index_'+table_name+'_id ON '+table_name+'(id);'\n    db.execute_sqls(id_index)\n\n    #lock_contention\n    pool = Pool(threads)\n    for _ in range(threads):\n        pool.apply_async(\n            lock, (table_name, ncolumns, colsize, duration, nrows))\n    pool.close()\n    pool.join()\n\n    #drop the index\n    db.drop_index(table_name)\n\n    #delete the table\n    delete_table(table_name)\n    print_time()\n\n\ndef lock(table_name, ncolumns, colsize, duration, nrows):\n    args=init()\n    start = time.time()\n    #lock_contention\n    while time.time()-start < duration:\n        conn = psycopg2.connect(database=args.dbname, user=args.user, password=args.password,\n                                        host=args.host, port=args.port)\n        cur = conn.cursor()\n        while time.time()-start < duration:\n            col_name = random.randint(0, ncolumns-1)\n            row_name = random.randint(1, nrows-1)\n            lock_contention = f'update {table_name} set name{col_name}=(SELECT substr(md5(random()::text), 1, {colsize})) where id ={row_name}'\n            #db.concurrent_execute_sql(threads,duration,lock_contention,nrows)\n            cur.execute(lock_contention)\n            conn.commit()\n        conn.commit()\n        conn.close()\n\nif __name__ == \"__main__\":\n    # Number of threads to use for concurrent inserts\n    num_threads = 5\n    \n    # Duration for which to run the inserts (in seconds)\n    insert_duration = None\n    \n    # Number of columns in the table\n    num_columns = 73\n    \n    # Number of rows to insert\n    num_rows = 747606\n    \n    # Size of each column (in characters)\n    column_size = 100\n    \n    # Table name\n    table_name = 'table1'\n    \n    nindex=6\n    \n    # Call the insert_large_data function\n    redundent_index(num_threads, insert_duration, num_columns, num_rows, column_size, nindex,table_name)\n", "description": "In a database for a social media platform, when optimizing the search function, a large number of indexes are created for user profiles, posts, and comments. However, some of these indexes are redundant and do not significantly improve the search performance. This can lead to unnecessary storage consumption and a potential decrease in query performance.\n", "workload": "According to the provided code, excluding CREATE TABLE and DROP TABLE queries and parameters with None values, following SQL queries are being repeatedly utilized along with their corresponding frequency (threads):\n\n1. Query: \"select indexname from pg_indexes where tablename='table1';\" - Frequency: 1\n\nExecution of this query is not inside a loop, so it should be executed only once per calling the `drop_index` method.\n\n2. Query: \"CREATE INDEX index_table1_i ON table1(namei);\" - Frequency: 18\n\nThis `CREATE INDEX` query is created inside a for loop that ranges from 0 to `idx_num` - 1 (idx_num is `nindex` that is calculated as 10% of the number of columns). This query essentially creates an index on a specific column in the table. For num_columns=18, the number of threads could be estimated as 18.\n\n3. Query: \"insert into table1 select generate_series(1,3167807),(SELECT substr(md5(random()::text), 1, 70)), now();\" - Frequency: 1\n\nThis insert query is used to insert data into the table. As it's not within a loop, this query should run once.\n\n4. Query: \"update table1 set namei=(SELECT substr(md5(random()::text), 1, 70)) where id =j\" - Frequency: 176\n\nIn `lock` method this query runs in a while loop until a condition (time-based) is met. The total threads involved in these queries will be the number of threads to be utilized for concurrent execution, which is given as 176.\n\n5. Query: \"CREATE INDEX index_table1_id ON table1(id);\" - Frequency: 1\n\nIt is not inside a loop and thus executed only once.\n\n6. Query: \"DROP INDEX idx0;\" - Frequency: Depends on result of the select indexname query\n\nThis query is inside a loop where it runs for each index fetched from the \"select indexname from pg_indexes where tablename='table1';\" query. Its frequency entirely depends on the result of that select query.", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 2.0, 2.0, 4.0, 8.0, 7.0, 6.0, 8.0, 3.0, 5.0, 6.0, 3.0, 5.0, 4.0, 4.0, 5.0, 6.0, 5.0, 1.0, 10.0, 12.0, 2.0, 12.0, 9.0, 8.0, 1.0, 14.0, 16.0, 2.0, 1.0, 2.0, 1.0, 7.0, 1.0, 7.0, 1.0, 4.0, 12.0, 10.0], "node_procs_blocked": [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 9.0, 3.0, 2.0, 3.0, 3.0, 1.0, 4.0, 4.0, 1.0, 4.0, 4.0, 2.0, 3.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0], "node_entropy_available_bits": [3512.0, 3513.0, 3523.0, 3538.0, 3553.0, 3557.0, 3562.0, 3565.0, 3571.0, 3577.0, 3584.0, 3589.0, 3596.0, 3601.0, 3607.0, 3611.0, 3615.0, 3618.0, 3619.0, 3624.0, 3710.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_disk_read_time_seconds_total": [0.0, 0.0003333333879709244, 0.007666666681567828, 0.0, 0.2729999999670933, 0.0033333333364377418, 0.00033333331036070984, 0.4443333333668609, 0.0, 0.0, 0.0, 0.5253333332948387, 0.18699999999565384, 0.018666666699573398, 0.0, 0.5123333333370587, 0.2739999999757856, 0.0006666666983316342, 0.014333333276833097, 0.5026666667157164, 21.761000000018004, 5.054999999934807, 18.752000000017386, 10.44833333332402, 7.145666666716958, 15.185999999986961, 20.839666666695848, 12.750666666620722, 18.027666666700195, 17.610666666645557, 7.083333333333333, 6.773999999975786, 7.3930000000012415, 5.045000000003104, 1.2890000000285606, 5.612333333352581, 1.4179999999857198, 0.4599999999627471, 3.8843333333885917]}, "memory": {"node_memory_Inactive_anon_bytes": [1659064320.0, 1659064320.0, 1659068416.0, 1659068416.0, 1656721408.0, 1656692736.0, 1656713216.0, 1656713216.0, 1656713216.0, 1656713216.0, 1656713216.0, 1656713216.0, 1656709120.0, 1656709120.0, 1656709120.0, 1656709120.0, 1656709120.0, 1656709120.0, 1656496128.0, 1656496128.0, 1656496128.0, 1656496128.0, 1656496128.0, 1656496128.0, 1656496128.0, 1656496128.0, 1656512512.0, 1656512512.0, 1656512512.0, 1656512512.0, 1656512512.0, 1656512512.0, 1656512512.0, 1656508416.0, 1656508416.0, 1656508416.0, 1656508416.0, 1656524800.0, 1656524800.0]}, "network": {"node_sockstat_TCP_tw": [11.0, 10.0, 10.0, 10.0, 10.0, 11.0, 6.0, 6.0, 11.0, 11.0, 11.0, 9.0, 10.0, 9.0, 9.0, 9.0, 8.0, 8.0, 9.0, 9.0, 9.0, 8.0, 8.0, 7.0, 8.0, 8.0, 3.0, 3.0, 7.0, 7.0, 7.0, 6.0, 6.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 0.3333333333333333, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 3.3333333333333335, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 21.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 21.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0, 31.0], "node_sockstat_TCP_inuse": [12.0, 12.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 13.0, 12.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0, 22.0]}}}, "24": {"start_time": "1697297563", "end_time": "1697297653", "start_timestamp": "2023-10-14 23:32:43", "end_timestamp": "2023-10-14 23:34:13", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "startsAt": "2023-10-15T09:55:49.467858611Z", "endsAt": "2023-10-15T14:22:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["INSERT_LARGE_DATA", "IO_CONTENTION"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION", "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n", "description": "In a file sharing system where multiple users are uploading, downloading, or editing files at the same time, the system is experiencing contention for input/output (I/O) resources. This results in a slowdown in file transfers.\n", "workload": "- SQL Query: \"SELECT o_id, o_entry_d, o_carrier_id FROM order WHERE o_w_id = 1 AND o_d_id = 1 AND o_c_id = 1 AND o_id = (SELECT MAX(o_id) FROM order WHERE o_w_id = 1 AND o_d_id = 1 AND o_c_id = 1)\", Frequency: 3\n- SQL Query: \"UPDATE customer SET c_balance = c_balance - :1 WHERE c_w_id = 1 AND c_d_id = 1 AND c_id = 1\", Frequency: 2\n- SQL Query: \"insert into table1 select generate_series(1,3167807), (SELECT substr(md5(random()::text), 1, 70)),..., now();\", Frequency: 1\n- SQL Query: \"delete from table1 where id < 2534245;\", Frequency: 1", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [1.0, 34.0, 37.0, 29.0, 29.0, 26.0, 29.0, 33.0, 25.0, 32.0, 27.0, 24.0, 31.0, 31.0, 26.0, 29.0, 28.0, 29.0, 22.0, 27.0, 28.0, 33.0, 29.0, 21.0, 35.0, 28.0, 26.0, 30.0, 29.0, 30.0, 26.0], "node_procs_blocked": [0.0, 7.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [204587008.0, 204619776.0, 204619776.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204636160.0, 204632064.0, 204632064.0, 204632064.0, 204632064.0, 204632064.0, 204595200.0, 204595200.0, 204595200.0, 204595200.0, 204599296.0, 204599296.0, 204599296.0, 204599296.0, 204562432.0, 204562432.0, 204562432.0, 204546048.0, 204546048.0]}, "network": {"node_sockstat_TCP_tw": [9.0, 9.0, 10.0, 10.0, 10.0, 9.0, 9.0, 9.0, 8.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 6.0, 7.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 5.0, 7.0, 7.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 16.666666666666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6666666666666666, 0.0], "node_sockstat_TCP_alloc": [21.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0], "node_sockstat_TCP_inuse": [12.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0]}}}, "25": {"start_time": "1697301082", "end_time": "1697301173", "start_timestamp": "2023-10-15 00:31:22", "end_timestamp": "2023-10-15 00:32:53", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "startsAt": "2023-10-15T09:55:49.467858611Z", "endsAt": "2023-10-15T14:22:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["INSERT_LARGE_DATA", "IO_CONTENTION"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION", "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n", "description": "In a file sharing system, multiple users are simultaneously uploading, downloading, or editing files, causing competition for input/output operations. This results in slower file transfer speeds.\n", "workload": "- SQL Query: \"SELECT o_id, o_entry_d, o_carrier_id FROM order WHERE o_w_id = 1 AND o_d_id = 1 AND o_c_id = 1 AND o_id = (SELECT MAX(o_id) FROM order WHERE o_w_id = 1 AND o_d_id = 1 AND o_c_id = 1)\", Frequency: 3\n- SQL Query: \"UPDATE customer SET c_balance = c_balance - :1 WHERE c_w_id = 1 AND c_d_id = 1 AND c_id = 1\", Frequency: 2\n- SQL Query: \"insert into table1 select generate_series(1,3167807), (SELECT substr(md5(random()::text), 1, 70)),..., now();\", Frequency: 1\n- SQL Query: \"delete from table1 where id < 2534245;\", Frequency: 1", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [2.0, 36.0, 34.0, 38.0, 31.0, 32.0, 19.0, 31.0, 32.0, 22.0, 23.0, 31.0, 25.0, 31.0, 31.0, 32.0, 31.0, 36.0, 26.0, 26.0, 32.0, 28.0, 29.0, 27.0, 24.0, 26.0, 28.0, 27.0, 34.0, 27.0, 27.0], "node_procs_blocked": [0.0, 12.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_entropy_available_bits": [3499.0, 3540.0, 3640.0, 3728.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [510971904.0, 511037440.0, 511053824.0, 511078400.0, 511086592.0, 511094784.0, 511102976.0, 511098880.0, 511094784.0, 511111168.0, 511111168.0, 511111168.0, 511111168.0, 511111168.0, 511102976.0, 511102976.0, 511102976.0, 511102976.0, 511102976.0, 511102976.0, 511102976.0, 510996480.0, 511004672.0, 511012864.0, 511021056.0, 511029248.0, 511053824.0, 511053824.0, 511012864.0, 511021056.0, 511029248.0]}, "network": {"node_sockstat_TCP_tw": [8.0, 8.0, 8.0, 7.0, 7.0, 7.0, 7.0, 7.0, 6.0, 6.0, 6.0, 7.0, 7.0, 5.0, 5.0, 6.0, 7.0, 7.0, 4.0, 4.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 7.0, 6.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.3333333333333333, 16.666666666666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0], "node_sockstat_TCP_alloc": [21.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 72.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0, 71.0], "node_sockstat_TCP_inuse": [12.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 63.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0, 62.0]}}}, "26": {"start_time": "1697302227", "end_time": "1697302318", "start_timestamp": "2023-10-15 00:50:27", "end_timestamp": "2023-10-15 00:51:58", "alerts": [{"receiver": "db-gpt", "status": "resolved", "alerts": [{"status": "resolved", "labels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "annotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "startsAt": "2023-10-15T09:55:49.467858611Z", "endsAt": "2023-10-15T14:22:49.467858611Z", "generatorURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9090/graph?g0.expr=node%3Ains%3Amem_avail+%3C+0.4&g0.tab=1", "fingerprint": "a36b530e24996667"}], "groupLabels": {"alertname": "NodeOutOfMem"}, "commonLabels": {"alertname": "NodeOutOfMem", "category": "node", "instance": "172.27.58.65:9100", "job": "node", "level": "1", "severity": "WARN"}, "commonAnnotations": {"description": "node:ins:mem_avail[ins=] = 0.05 < 10%\n", "summary": "WARN NodeOutOfMem @172.27.58.65:9100 0.05"}, "externalURL": "http://iZ2ze0ree1kf7ccu4p1vcyZ:9093", "version": "4", "groupKey": "{}:{alertname=\"NodeOutOfMem\"}", "truncatedAlerts": 0}], "labels": ["INSERT_LARGE_DATA", "IO_CONTENTION"], "command": "python anomaly_trigger/main.py --anomaly INSERT_LARGE_DATA,IO_CONTENTION", "script": "import  os\nimport datetime\n\n#print the current time\ndef print_time():\n    current_time = datetime.datetime.now()\n    formatted_time = current_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n    print(formatted_time)\n\nif __name__ == \"__main__\":\n    print_time()\n    command = (\n    \"su - root -c 'cd /sysbench-tpcc-master; \"\n    \"./tpcc.lua --db-driver=pgsql --tables=2 --scale=3 --threads=50 --events=0 \"\n    \"--pgsql-host=xxxx --pgsql-user=xxxx --pgsql-password=xxxx \"\n    \"--pgsql-port=5432 --pgsql-db=tpcc --time=90 --rand-type=uniform --report-interval=10 run'\"\n    )\n\n    os.system(command)\n    print_time()\n", "description": "In a file sharing system where multiple users are sharing files, if there is a simultaneous upload, download, or editing of files, it creates I/O contention. This contention leads to a slowdown in file transfer and affects the overall performance of the system.\n", "workload": "- SQL Query: \"SELECT o_id, o_entry_d, o_carrier_id FROM order WHERE o_w_id = 1 AND o_d_id = 1 AND o_c_id = 1 AND o_id = (SELECT MAX(o_id) FROM order WHERE o_w_id = 1 AND o_d_id = 1 AND o_c_id = 1)\", Frequency: 3\n- SQL Query: \"UPDATE customer SET c_balance = c_balance - :1 WHERE c_w_id = 1 AND c_d_id = 1 AND c_id = 1\", Frequency: 2\n- SQL Query: \"insert into table1 select generate_series(1,3167807), (SELECT substr(md5(random()::text), 1, 70)),..., now();\", Frequency: 1\n- SQL Query: \"delete from table1 where id < 2534245;\", Frequency: 1", "slow_queries": [], "exceptions": {"cpu": {"node_procs_running": [5.0, 1.0, 22.0, 35.0, 27.0, 32.0, 28.0, 35.0, 35.0, 27.0, 25.0, 27.0, 27.0, 29.0, 33.0, 34.0, 27.0, 25.0, 25.0, 30.0, 27.0, 30.0, 29.0, 26.0, 35.0, 28.0, 24.0, 33.0, 35.0, 23.0, 33.0], "node_procs_blocked": [0.0, 0.0, 5.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_entropy_available_bits": [3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3569.0, 3654.0, 3738.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0, 3754.0]}, "io": {"node_filesystem_size_bytes": [212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0, 212300566528.0], "node_disk_io_now": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}, "memory": {"node_memory_Inactive_anon_bytes": [316387328.0, 311500800.0, 311599104.0, 311615488.0, 311615488.0, 311615488.0, 311615488.0, 311656448.0, 311664640.0, 311664640.0, 311066624.0, 311083008.0, 311074816.0, 311074816.0, 311074816.0, 311062528.0, 311062528.0, 311062528.0, 311062528.0, 311070720.0, 311078912.0, 311087104.0, 310972416.0, 310980608.0, 311005184.0, 311013376.0, 311021568.0, 311029760.0, 311037952.0, 311037952.0, 311046144.0]}, "network": {"node_sockstat_TCP_tw": [8.0, 8.0, 9.0, 9.0, 9.0, 8.0, 8.0, 8.0, 8.0, 8.0, 5.0, 5.0, 6.0, 5.0, 6.0, 5.0, 5.0, 5.0, 3.0, 5.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 4.0, 3.0], "node_sockstat_TCP_orphan": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "irate(node_netstat_Tcp_PassiveOpens": [0.0, 0.0, 16.666666666666668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "node_sockstat_TCP_alloc": [22.0, 22.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0, 72.0], "node_sockstat_TCP_inuse": [13.0, 13.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0, 63.0]}}}}